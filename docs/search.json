[
  {
    "objectID": "lectures/lecture9.html#title-slide",
    "href": "lectures/lecture9.html#title-slide",
    "title": "EGH445 Modern Control",
    "section": "Title Slide",
    "text": "Title Slide\n\n\n\n\n\nDiscrete-Time Control Design 3\n\n\nOptimal Control\n\n\n\nDr Guilherme Froes Silva School of Electrical Engineering & Robotics Queensland University of Technology\n\n\nEGH445 - Modern Control\n\n\n\nConsultation: GP-S1111 Email: g.froessilva@qut.edu.au"
  },
  {
    "objectID": "lectures/lecture9.html#quick-review-of-some-of-the-content-so-far",
    "href": "lectures/lecture9.html#quick-review-of-some-of-the-content-so-far",
    "title": "EGH445 Modern Control",
    "section": "Quick review of (some of) the content so far",
    "text": "Quick review of (some of) the content so far\n\nContinuous-time system: \\begin{align*}\n\\dot{x}(t) &= f(x, u), \\\\\ny(t) &= g(x, u)\n\\end{align*}\n\n\nLinearised system: \\begin{align*}\n\\delta\\dot{x}(t) &= A\\delta x(t) + B \\delta u(t),  \\\\\n\\delta y(t) &= C\\delta x(t) + D\\delta u(t),\n\\end{align*}\nwhere \\delta x = x - \\bar x, \\delta u = u - \\bar u, \\delta y = y - \\bar y, and the A, B, C, D are the matrices\n\\begin{align*}\nA = \\frac{\\partial f}{\\partial x} \\bigg|_{\\bar x, \\bar u}\nB = \\frac{\\partial f}{\\partial u} \\bigg|_{\\bar x, \\bar u}\nC = \\frac{\\partial g}{\\partial x} \\bigg|_{\\bar x, \\bar u}\nD = \\frac{\\partial g}{\\partial u} \\bigg|_{\\bar x, \\bar u}\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture9.html#section",
    "href": "lectures/lecture9.html#section",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Discrete-time system:\n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\\\ y(kT) &= Cx(kT) + Du(kT), \\end{align*}\nwhere\n\\begin{align*}\nG = e^{AT}, \\quad H = \\left[\\int_0^T e^{A\\tau} d\\tau\\right] B \\quad \\left(\\text{or, if $A$ is invertible, } H = A^{-1} (G-I)B\\right)\n\\end{align*}\n\nState-feedback controller: \\;u(kT) = -Kx(kT),\nwhere K is the feedback gain matrix that is designed to arbitrarily move the poles of the closed-loop system \\begin{align*}\nx(kT+T) = (G-HK)x(kT)\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture9.html#section-1",
    "href": "lectures/lecture9.html#section-1",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "This is achieved, for example, by equating the characteristic polynomial of the closed-loop system to a desired polynomial,\n\\begin{align*}\n\\det(zI - (G-HK)) = (z - z_1)(z - z_2) \\cdots (z - z_n)\n\\end{align*}\n\n\n\n\n\n\n\nImportant\n\n\nThe solution exists if the system is controllable, i.e. if the controllability matrix \\;\\mathcal{C} = \\left[H,\\; GH,\\; G^2H,\\; \\ldots,\\; G^{n-1}H\\right] has full rank (\\text{rank}(\\mathcal{C})=n).\n\n\n\n\n\nHow do you choose the poles?\n\n\n\nt_s - settling time\nt_r - rise time\n\\%OS - percent overshoot\n\\omega_n - natural frequency\n\\zeta - damping ratio (or through percent overshoot)\n\n\n\n\\zeta = \\dfrac{\\ln(\\%OS/100)}{\\sqrt{\\pi^2 + \\ln^2(\\%OS/100)}}\n\\omega_n = \\frac{4}{\\zeta t_s}\ns_{1,2} = -\\zeta \\omega_n \\pm j \\omega_n \\sqrt{1-\\zeta^2}\nz_{1,2} = e^{s_{1,2}T}"
  },
  {
    "objectID": "lectures/lecture9.html#section-2",
    "href": "lectures/lecture9.html#section-2",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "We also saw the Internal Model Principle (e.g. Integral Action) to reject disturbances (or follow references) with a known model (e.g. a step input, a ramp input, a sinusoidal input, etc.). Those still relied on the pole placement approach."
  },
  {
    "objectID": "lectures/lecture9.html#higher-order-systems",
    "href": "lectures/lecture9.html#higher-order-systems",
    "title": "EGH445 Modern Control",
    "section": "Higher-Order Systems",
    "text": "Higher-Order Systems\n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\\\ y(kT) &= Cx(kT) + Du(kT), \\end{align*}\nwhere \\quad x(kT) \\in \\mathbb{R}^n, \\quad u(kT) \\in \\mathbb{R}, \\quad y(kT) \\in \\mathbb{R}, \\quad n &gt; 3\n\n\n\n\n\n\n\nImportant\n\n\nThe link between pole locations and desired time-domain response becomes less clear. Arbitrary choices can lead to poor performance or excessive control effort.\nHow do you choose the best pole locations for a 5th, 10th, or higher-order system?"
  },
  {
    "objectID": "lectures/lecture9.html#mimo-systems",
    "href": "lectures/lecture9.html#mimo-systems",
    "title": "EGH445 Modern Control",
    "section": "MIMO Systems",
    "text": "MIMO Systems\nLet’s start with an example.\nConsider a simple MIMO system with two states and two inputs:\n\\begin{align*} x(kT+T) &= \\begin{bmatrix} 1 & 0.1 \\\\ 0 & 0.8 \\end{bmatrix} x(kT) + \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} u(kT)\n\\end{align*}\nWe want to place the poles at 0.4 and 0.6, by using u(kT) = -Kx(kT), where K = \\begin{bmatrix} k_1 & k_2 \\\\ k_3 & k_4\\end{bmatrix}.\n\nThe characteristic polynomial is given by:\n\\begin{align*}\n&\\det(zI - (G-HK)) = \\det\\left(\\begin{bmatrix} z-1+k_1 & k_2-0.1 \\\\ k_3 & z-0.8+k_4\\end{bmatrix}\\right) \\\\\n&= z^2 + (k_1 + k_4 - 1.8)z + (0.1k_3 - k_4 - 0.8k_1 + k_1k_4 - k_2k_3 + 0.8)\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture9.html#section-3",
    "href": "lectures/lecture9.html#section-3",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "The desired characteristic polynomial is given by:\n\\begin{align*}\n(z-0.4)(z-0.6) = z^2 - 1z + 0.24\n\\end{align*}\nWhen we equate the coefficients, we get:\n\\begin{align*}\n\\begin{cases}\n(k_1 + k_4 - 1.8) = -1 \\\\\n0.1k_3 - k_4 - 0.8k_1 + k_1k_4 - k_2k_3 + 0.8 = 0.24\n\\end{cases}\n\\end{align*}\n\n\n\n\n\n\n\nImportant\n\n\nNote that we have two equations and four unknowns. This means that we have degrees of freedom in the design. We can choose two of the four variables arbitrarily, and then solve for the other two.\n\n\n\n\nThis seems like a good idea, but it is not. The problem is that different choices of K, even if they lead to the same eigenvalues, can lead to different eigenvectors. The closed-loop system’s eigenvectors directly affect the response of the system."
  },
  {
    "objectID": "lectures/lecture9.html#section-4",
    "href": "lectures/lecture9.html#section-4",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Let k_2 = k_3 = 0 and consider the following two cases of gains matrices K_1 and K_2, both of which lead assign the closed-loop eigenvalues to the desired values: \nK_1 = \\begin{bmatrix} 0.6 & 0 \\\\ 0 & 0.2 \\end{bmatrix}, \\quad K_2 = \\begin{bmatrix} 0.4 & 0 \\\\ 0 & 0.4 \\end{bmatrix}\n\nBoth of which lead to the same eigenvalues we wanted. But let’s simulate the system with both K_1 and K_2.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport control as ctrl\n# Create the system matrices  \nG = np.array([[1, 0.1], [0, 0.8]])\nH = np.array([[1, 0], [0, 1]])\nC = np.array([[1, 0], [0, 1]]) # Outputs are the states\nD = np.array([[0, 0], [0, 0]])\nK1 = np.array([[0.6, 0], [0, 0.2]]) \nK2 = np.array([[0.4, 0], [0, 0.4]]) \nGcl1 = G - H @ K1 \nGcl2 = G - H @ K2\n\nTs = 0.1 # Sampling time (seconds)\n# Create the closed-loop LTI systems\nsys1 = ctrl.ss(Gcl1, H, C, D, Ts) \nsys2 = ctrl.ss(Gcl2, H, C, D, Ts) \n\n# Simulate for initial condition x0 = [1, 0]\nx0 = np.array([1, 0])\nt_end = 1.5 \nt = np.arange(0, t_end, Ts) \nt1, y1 = ctrl.initial_response(sys1, T=t, X0=x0)\nt2, y2 = ctrl.initial_response(sys2, T=t, X0=x0)\n\nplt.figure(figsize=(10, 6))\nplt.plot(t1, y1[0, :], label='K1 - x1', linewidth=3)\nplt.plot(t1, y1[1, :], label='K1 - x2', linewidth=3)\nplt.plot(t2, y2[0, :], label='K2 - x1', linestyle='--', \n    linewidth=3) \nplt.plot(t2, y2[1, :], label='K2 - x2', linestyle='--', \n    linewidth=3) \nplt.title('Initial Condition Response ($x_0=[1, 0]^T$) \\\n     with Different K Matrices', fontsize=16) \nplt.xlabel('Time (steps * dt)', fontsize=14) \nplt.ylabel('State Values', fontsize=14) \nplt.legend(fontsize=12)\nplt.grid(True) \nplt.show()"
  },
  {
    "objectID": "lectures/lecture9.html#mimo-systems-1",
    "href": "lectures/lecture9.html#mimo-systems-1",
    "title": "EGH445 Modern Control",
    "section": "MIMO Systems",
    "text": "MIMO Systems\n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\\\ y(kT) &= Cx(kT) + Du(kT), \\end{align*}\nx(kT) \\in \\mathbb{R}^n, \\quad u(kT) \\in \\mathbb{R}^m, \\quad y(kT) \\in \\mathbb{R}^p, \\quad n, m, p &gt; 1\n\n\n\n\n\n\n\nImportant\n\n\nFor MIMO systems, pole placement is significantly more complex. Specifying only the eigenvalues leaves degrees of freedom in the eigenvectors, which also affect the response. The design process becomes non-unique and less intuitive.\nHow do you systematically handle interactions between different inputs and outputs?"
  },
  {
    "objectID": "lectures/lecture9.html#performance-trade-off",
    "href": "lectures/lecture9.html#performance-trade-off",
    "title": "EGH445 Modern Control",
    "section": "Performance trade-off",
    "text": "Performance trade-off\nFinally, pole placement does not consider the control effort, not addressing the trade-off between:\n\nregulating the state, by making x(kT), or \\delta x(kT)1, small\n\n1 For linearised systems, we defined \\delta x = x - \\bar x.\n\nregulating required control effort u(kT).\n\n\n\n\n\n\n\n\nImportant\n\n\nYou might achieve desired poles but with impractically large control signals.\nWhich, like in the following example, can lead to instability."
  },
  {
    "objectID": "lectures/lecture9.html#linear-controller-failure-for-a-nonlinear-system",
    "href": "lectures/lecture9.html#linear-controller-failure-for-a-nonlinear-system",
    "title": "EGH445 Modern Control",
    "section": "Linear Controller Failure for a Nonlinear System",
    "text": "Linear Controller Failure for a Nonlinear System\nConsider a scalar system with cubic nonlinearity, \\; \\dot{x}(t) = -x(t) + x(t)^3 + u(t).\n\n\n-x represents a stabilizing linear dynamic.\n\n\n\n\nx^3 is a destabilizing nonlinearity that becomes significant for larger values of |x|.\n\n\n\nWe want to design a controller u(kT) to stabilize the system at \\bar x = 0 (requiring \\bar u = 0).\n\n\n1. Linearise the system around the equilibrium point (\\bar x=0, \\bar u=0):\n\n\n\nA = \\frac{\\partial}{\\partial x}(-x + x^3 + u) \\Big|_{x=0, u=0} = (-1 + 3x^2)\\Big|_{x=0} = -1\n\n\n\n\nB = \\frac{\\partial}{\\partial u}(-x + x^3 + u) \\Big|_{x=0, u=0} = 1\n\n\n\nThe linearised continuous-time system is \\;\\boxed{\\delta\\dot{x}(t) = -\\delta x(t) + \\delta u(t)}."
  },
  {
    "objectID": "lectures/lecture9.html#section-5",
    "href": "lectures/lecture9.html#section-5",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "2. Discretise the system with a sampling time T=0.1s:\n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\quad y(kT) = x(kT) \\end{align*}\nwhere G = e^{-AT} = 0.905, and H = A^{-1}(G-I)B = 0.095.\nThe discrete-time (linearised) system is \\boxed{x(kT+T) = 0.905x(kT) + 0.095u(kT)}.\n\n3. Design u(kT) = -K x(kT) to place the closed-loop pole of this linearised system at p = 0.5.\n3.1 Design through pole placement. The closed-loop is x(kT+T) = (G - H K) x(kT).\nWe want the closed-loop pole to be equal to p=0.5:\n\n\n\nTry to do this through equating the characteristic polynomials: \\det(zI - (G-HK)) = (z-p)."
  },
  {
    "objectID": "lectures/lecture9.html#section-6",
    "href": "lectures/lecture9.html#section-6",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "\\begin{align*} 0.905 - 0.095 K &= 0.5 \\\\ 0.095 K &= 0.905 - 0.5 = 0.405 \\\\ K &= \\frac{0.405}{0.095} = 4.263 \\end{align*}\nSo, the linear controller designed for the linearized system is \\boxed{u(kT) = -4.263 x(kT)}.\n\n4. Test the controller on the original nonlinear system.\nSimulate the continuous nonlinear dynamics, but apply the control input u(kT) = -K x(kT) constant over the period [kT, kT + T) (Zero-Order Hold).\n\n\n\n\n\n\nImportant\n\n\nNote that the controller was derived for the linearised system, which ignores the x^3 term. The linear model is only accurate for small x (close to the equilibrium x=0). When |x| becomes large, the x^3 term can dominate, and the linear controller may fail."
  },
  {
    "objectID": "lectures/lecture9.html#section-7",
    "href": "lectures/lecture9.html#section-7",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "import numpy as np\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\n# Set default font size for plots\nplt.rcParams['font.size'] = 14 \n\n# 1. Nonlinear System Dynamics\ndef nonlinear_system(x, u):\n    # Scalar system dynamics: dx/dt = -x + x^3 + u\n    return -x + x**3 + u\n\n# 2. Linear Controller Parameters (derived above)\nAd = 0.905\nBd = 0.095\nK = 4.263\nTs = 0.1\nx_eq = 0.0 # Equilibrium state\n\n# 3. Simulation Setup\nt_start = 0\nt_end = 1.0 # Shorter simulation time might be enough\nn_steps = int(t_end / Ts)\n\n# Initial Conditions to compare\nx0_small = 2\nx0_large = 2.4\n\n# Function to run the simulation for a given x0\ndef run_simulation_smooth(x0_val, points_per_interval=10):\n    # Store results for smooth plotting\n    t_history_smooth = [t_start]\n    x_history_smooth = [x0_val]\n    # Store results at discrete intervals for markers and control calc\n    t_history_discrete = [t_start]\n    x_history_discrete = [x0_val]\n    u_history = [] # Control applied over the interval starting at t_history_discrete\n\n    current_t = t_start\n    current_x_start_of_interval = np.array([x0_val]) # State at beginning of interval\n\n    # print(f\"\\nRunning smooth simulation for x(0) = {x0_val}...\")\n    for k in range(n_steps):\n        # Calculate discrete control based on state at START of interval\n        x_deviation = current_x_start_of_interval[0] - x_eq\n        u_k = -K * x_deviation\n        # Optional: Limit control effort\n        # u_k = np.clip(u_k, -u_max, u_max)\n        u_history.append(u_k)\n\n        # Define ODE function for this interval (constant u_k)\n        def ode_interval(t, y): # y is a 1-element array\n            return np.array([nonlinear_system(y[0], u_k)])\n\n        # Simulate one interval Ts, evaluating at multiple points\n        t_eval_interval = np.linspace(current_t, current_t + Ts, points_per_interval, endpoint=True)\n        sol_interval = solve_ivp(\n            ode_interval,\n            (current_t, current_t + Ts), # t_span for the interval\n            current_x_start_of_interval, # Initial state for interval\n            method='RK45',\n            t_eval=t_eval_interval # Evaluate at these points\n        )\n\n        if sol_interval.status != 0:\n            print(f\"Simulation failed at step {k} (t={current_t:.2f}) for x(0)={x0_val}\")\n            # Pad history if needed\n            # ... (padding logic can be added if needed) ...\n            break\n\n        # Update state for next interval START\n        current_x_start_of_interval = sol_interval.y[:, -1] # State at the end\n        current_t += Ts\n\n        # Store history\n        # Append points *after* the first one to avoid duplication\n        t_history_smooth.extend(sol_interval.t[1:])\n        x_history_smooth.extend(sol_interval.y[0, 1:])\n        # Store discrete points\n        t_history_discrete.append(current_t)\n        x_history_discrete.append(current_x_start_of_interval[0])\n\n\n        # Stop if state diverges excessively\n        if abs(current_x_start_of_interval[0]) &gt; 10:\n            print(f\"State diverging at step {k} (t={current_t:.2f}) for x(0)={x0_val}. Stopping.\")\n            break\n\n    return (np.array(t_history_smooth), np.array(x_history_smooth), # Smooth results\n            np.array(t_history_discrete), np.array(x_history_discrete), # Discrete results\n            np.array(u_history)) # Control history\n\n# --- Run both simulations ---\n(t_small_smooth, x_small_smooth,\n t_small_discrete, x_small_discrete, u_small) = run_simulation_smooth(x0_small)\n\n(t_large_smooth, x_large_smooth,\n t_large_discrete, x_large_discrete, u_large) = run_simulation_smooth(x0_large)\n\n\n# --- MODIFIED Plotting Section ---\nplt.figure(figsize=(10, 8))\n\n# --- Plot states (Top Plot) ---\nplt.subplot(2, 1, 1)\n# Plot smooth trajectories\nplt.plot(t_small_smooth, x_small_smooth,\n         label=f'State x(t) (x0={x0_small})', linewidth=2)\nplt.plot(t_large_smooth, x_large_smooth,\n         label=f'State x(t) (x0={x0_large})', linewidth=2, linestyle='--')\n\nplt.axhline(0, color='black', linewidth=0.5, linestyle=':')\nplt.ylabel('State x')\nplt.title('Scalar Nonlinear System with Linear Controller (Pole @ 0.5)')\nplt.grid(True)\nplt.legend()\n# Adjust ylim automatically or set manually if needed\nylim_min = min(np.nanmin(x_small_smooth), np.nanmin(x_large_smooth), -1) - 0.5\nylim_max = max(np.nanmax(x_small_smooth), np.nanmax(x_large_smooth), 1) + 0.5\nplt.ylim(ylim_min, ylim_max)\n\n# --- Plot control inputs (Bottom Plot - using stairs) ---\nplt.subplot(2, 1, 2)\n# Ensure simulation ran successfully and generated history\nfinal_idx_small = len(u_small)\nif final_idx_small &gt; 0:\n    plt.stairs(u_small, t_small_discrete, label=f'Control u[k] (x0={x0_small})',\n               baseline=None, linewidth=2, linestyle='--')\n\nfinal_idx_large = len(u_large)\nif final_idx_large &gt; 0:\n    plt.stairs(u_large[:final_idx_large-1], t_large_discrete, label=f'Control u[k] (x0={x0_large})',\n               baseline=None, linewidth=2, linestyle='--')\n\n\nplt.ylabel('Control Input u')\nplt.xlabel('Time (s)')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nSimulation failed at step 2 (t=0.20) for x(0)=2.4"
  },
  {
    "objectID": "lectures/lecture9.html#the-discrete-time-optimal-control-problem",
    "href": "lectures/lecture9.html#the-discrete-time-optimal-control-problem",
    "title": "EGH445 Modern Control",
    "section": "The Discrete-Time Optimal Control Problem",
    "text": "The Discrete-Time Optimal Control Problem\nConsider the System Dynamics1, \\; x(kT+T) = Gx(kT) + Hu(kT), \\quad x(0) = x_0.\n\n1The general concept extends to nonlinear systems, but through methods that are beyond the scope of this course.\n\n\nWe want to find u(kT) = -Kx(kT) that minimizes the Cost Function / Performance Index2:\n\nJ = \\sum_{k=0}^{\\infty} \\left( x(kT)^T Q x(kT) + u(kT)^T R u(kT) \\right), \\quad Q\\succeq 0, \\quad R\\succ 0\n\n\n2 This is an infinite horizon cost function, where the state penalty matrix Q and the control penalty matrix R define the cost.\n\n\n\n\nThe solution to this problem is the Linear Quadratic Regulator (LQR).\n\n\n\n\n\n\n\nAdditional Reading\n\n\nFor more information about the general optimal control problem, see the additional readings on Canvas.\n\n\n\n\n\n\nA\\succeq 0 means that A is positive semi-definite, i.e. x^T A x \\geq 0 for all x \\in \\mathbb{R}^n.\nA\\succ 0 means that A is positive definite, i.e. x^T A x &gt; 0 for all x \\in \\mathbb{R}^n and x \\neq 0."
  },
  {
    "objectID": "lectures/lecture9.html#the-discrete-linear-quadratic-regulator-dlqr",
    "href": "lectures/lecture9.html#the-discrete-linear-quadratic-regulator-dlqr",
    "title": "EGH445 Modern Control",
    "section": "The Discrete Linear Quadratic Regulator (DLQR)",
    "text": "The Discrete Linear Quadratic Regulator (DLQR)\nThe gain K that minimizes the cost function1, subject to the system dynamics2 is given by:\n\nK = (R + H^\\intercal P H)^{-1} H^\\intercal P G,\n\n\nwhich depends on the unique, symmetric, positive semi-definite solution P of the Discrete Algebraic Riccati Equation (DARE): \nP = G^\\intercal PG - (G^\\intercal PH)(R+H^\\intercal PH)^{-1}(H^\\intercal PG) + Q,\n\n\nRemember that P must be unique, symmetric, positive semi-definite, that is P=P^\\intercal\\succeq 0.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe full derivation of the optimal control gain K formula is beyond the scope of the course. If you’re curious, start by investigating the Bellman’s Principle of Optimality (Dynamic Programming).\n\n\n\n\nJ = \\sum_{k=0}^{\\infty} \\left( x(kT)^T Q x(kT) + u(kT)^T R u(kT) \\right), \\quad Q\\succeq 0, \\quad R\\succ 0x(kT+T) = Gx(kT) + Hu(kT), \\quad x(0) = x_0"
  },
  {
    "objectID": "lectures/lecture9.html#design-of-dlqr-controllers",
    "href": "lectures/lecture9.html#design-of-dlqr-controllers",
    "title": "EGH445 Modern Control",
    "section": "Design of DLQR Controllers",
    "text": "Design of DLQR Controllers\nThe design of DLQR controllers is based on the following steps:\n\n\nCheck controllability of x(KT+T) = Gx(kT) + Hu(kT).\nSelect the cost matrices Q\\succeq 0 and R\\succ 0.\n\nJ = \\sum_{k=0}^{\\infty} \\left( x(kT)^T Q x(kT) + u(kT)^T R u(kT) \\right)\n\nSolve the DARE to find the positive semi-definite symmetric P.\nCompute the gain matrix K = (R + H^\\intercal P H)^{-1} H^\\intercal P G.\nImplement the controller u(kT) = -Kx(kT).\nTest the controller performance and adjust Q and R as necessary."
  },
  {
    "objectID": "lectures/lecture9.html#existence-and-stability-of-the-solution",
    "href": "lectures/lecture9.html#existence-and-stability-of-the-solution",
    "title": "EGH445 Modern Control",
    "section": "Existence and Stability of the Solution",
    "text": "Existence and Stability of the Solution\nWe need to check if the solution to the DARE exists and is stable.\n\n\n\n\n\n\n\nQuestions\n\n\n\nDoes a unique solution to the DARE always exist?\nDoes the resulting controller guarantee stability?\n\n\n\n\n\n\nTurns out that we need the following conditions.\nConditions for Stabilizing DLQR Solution:\n\nControllability: Can the controller move all unstable poles of the system?\nObservability: Can all unstable models be observed by the cost function?"
  },
  {
    "objectID": "lectures/lecture9.html#the-dlqr-stability-theorem",
    "href": "lectures/lecture9.html#the-dlqr-stability-theorem",
    "title": "EGH445 Modern Control",
    "section": "The DLQR Stability Theorem",
    "text": "The DLQR Stability Theorem\n\n\n\n\n\n\nTheorem: DLQR Stability Theorem\n\n\nAssume R \\succ 0 and Q \\succeq 0. Let Q=V^TV (where V might not be unique).\nIf the following conditions hold:\n\n\nThe pair (G,H) is controllable.\nThe pair (G,V) is observable1.\n\n1Note that this is not the same as the observability of the system, as what’s important is observability of states by the cost function.\n\n\n\nThen:\n\nA unique solution P=P^\\intercal \\succeq 0 to the DARE exists.\nThe resulting controller closed-loop system is stable."
  },
  {
    "objectID": "lectures/lecture9.html#simple-example-of-dlqr-design",
    "href": "lectures/lecture9.html#simple-example-of-dlqr-design",
    "title": "EGH445 Modern Control",
    "section": "Simple Example of DLQR Design",
    "text": "Simple Example of DLQR Design\nLet’s consider a simple example of a DLQR design and solve it “by hand”. Consider again the scalar system with cubic nonlinearity and its discrete linearised version,\n\n\\dot{x}(t) = -x(t) + x(t)^3 + u(t) \\quad \\rightarrow \\quad x(kT+T) = 0.905x(kT) + 0.095u(kT)\n\n\nControllability: \\mathcal{C} = [H,\\; GH] = [0.095,\\; 0.086475] has full rank (\\text{rank}(\\mathcal{C})=1).\nCost Matrices: Select Q = 10.0 and R = 0.1.\nObservability: \\mathcal{O} = [V,\\; VG]^\\intercal, where V = \\sqrt{Q}. Then \\mathcal{O} = [3.162,\\; 2.857]^\\intercal, which has full rank.\nSolution P=P^\\intercal\\succeq 0 of the DARE: P - G^\\intercal P G + (G^\\intercal P H)(R + H^\\intercal P H)^{-1}(H^\\intercal P G) = Q"
  },
  {
    "objectID": "lectures/lecture9.html#section-8",
    "href": "lectures/lecture9.html#section-8",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "\\begin{align*}\nP - G^2 P + \\frac{(G H P)^2}{R + H^2 P} &= Q \\\\\n(P - G^2 P)(R + H^2 P) + G^2 H^2 P^2 &= Q (R + H^2 P) \\\\\nP R + H^2 P^2 - G^2 P R - Q R - Q H^2 P &= 0 \\\\\n(H^2) P^2 + (R - G^2 R - Q H^2) P + (- Q R) &= 0 \\\\\n0.009 P^2 -0.0722 P - 1 &= 0 \\\\\nP = 15.2571 \\text{ or } P = -7.2624\n\\end{align*}\n\nPick the positive semi-definite solution: P = 15.2571.\nCompute the gain K = (R + H^\\intercal P H)^{-1} H^\\intercal P G = 5.519."
  },
  {
    "objectID": "lectures/lecture9.html#section-9",
    "href": "lectures/lecture9.html#section-9",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "import numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are # Import DARE solver\nimport matplotlib.pyplot as plt\n\n# Set default font size for plots\nplt.rcParams['font.size'] = 14 \n\n# 1. Nonlinear System Dynamics\ndef nonlinear_system(x, u):\n    # Scalar system dynamics: dx/dt = -x + x^3 + u\n    return -x + x**3 + u\n\n# 2. Linear Controller Parameters (derived above)\nAd = 0.905\nBd = 0.095\nK = 4.263\nTs = 0.1\nx_eq = 0.0 # Equilibrium state\n\n# 3. Simulation Setup\nt_start = 0\nt_end = 1.0 # Shorter simulation time might be enough\nn_steps = int(t_end / Ts)\n\n# Initial Conditions to compare\nx0_small = 2\nx0_large = 2.4\n\n# --- DLQR Controller Design ---\nQd = 10.0  # State weight\nRd = 0.1  # Control weight\n\n# Solve Discrete Algebraic Riccati Equation (DARE)\n# Need to pass arguments as 2D arrays for the solver\nA_dare = np.array([[Ad]])\nB_dare = np.array([[Bd]])\nQ_dare = np.array([[Qd]])\nR_dare = np.array([[Rd]])\n\nS = solve_discrete_are(A_dare, B_dare, Q_dare, R_dare)\nS_scalar = S[0, 0] # Extract scalar value\n\n# Calculate DLQR gain Kd\n# Kd = (R + B'SB)^-1 B'SA\nKd_term1 = R_dare + B_dare.T @ S @ B_dare\nKd_term2 = B_dare.T @ S @ A_dare\nKd = np.linalg.inv(Kd_term1) @ Kd_term2\nK_lqr = Kd[0, 0] # Extract scalar gain\n\n# Check closed-loop stability for the linear system\nAcl_lqr = Ad - Bd * K_lqr\nprint(f\"DARE Solution S = {S_scalar:.3f} (for Q={Qd}, R={Rd})\")\nprint(f\"DLQR Gain K = {K_lqr:.3f}\")\nif abs(Acl_lqr) &lt; 1:\n    print(f\"Linear Closed-Loop Pole = {Acl_lqr:.3f}. (STABLE)\")\nelse:\n    print(f\"Linear Closed-Loop Pole = {Acl_lqr:.3f}. (UNSTABLE)\")\n# --- End DLQR Design ---\n\n# Function to run the simulation (same as before, just uses K_lqr)\ndef run_simulation(x0_val, K_gain):\n    t_history = [t_start]\n    x_history = [x0_val]\n    u_history = []\n    current_t = t_start\n    current_x = np.array([x0_val]) # State needs to be array\n\n    for k in range(n_steps):\n        x_deviation = current_x[0] - x_eq\n        u_k = -K_gain * x_deviation\n        u_history.append(u_k)\n\n        def ode_interval(t, y):\n            return np.array([nonlinear_system(y[0], u_k)])\n\n        t_interval = (current_t, current_t + Ts)\n        sol_interval = solve_ivp(\n            ode_interval, t_interval, current_x, method='RK45',\n            t_eval=[current_t + Ts]\n        )\n\n        if sol_interval.status != 0:\n            print(f\"Simulation failed at step {k} (t={current_t:.2f}) for x(0)={x0_val}\")\n            # Pad history...\n            failed_steps = n_steps - k\n            t_history.extend(np.linspace(current_t + Ts, t_end, failed_steps))\n            x_history.extend([np.nan] * failed_steps)\n            u_history.extend([np.nan] * failed_steps)\n            break\n\n        current_t += Ts\n        current_x = sol_interval.y[:, -1]\n        t_history.append(current_t)\n        x_history.append(current_x[0])\n\n        if abs(current_x[0]) &gt; 10: # Stop if diverging\n            print(f\"State diverging at step {k} (t={current_t:.2f}) for x(0)={x0_val}. Stopping.\")\n            failed_steps = n_steps - k -1\n            if failed_steps &gt; 0:\n                 t_history.extend(np.linspace(current_t + Ts, t_end, failed_steps))\n                 x_history.extend([np.nan] * failed_steps)\n            break\n\n    return np.array(t_history), np.array(x_history), np.array(u_history)\n\n# Run both simulations with DLQR gain\nt_small, x_small, u_small = run_simulation(x0_small, K_lqr)\nt_large, x_large, u_large = run_simulation(x0_large, K_lqr)\n\n# 4. Plotting Results\nplt.figure(figsize=(10, 8))\nplt.subplot(2, 1, 1)\nplt.plot(t_small, x_small, label=f'State x(t) (x0={x0_small})', linewidth=2)\nplt.plot(t_large, x_large, label=f'State x(t) (x0={x0_large})', linewidth=2, linestyle='--')\nplt.axhline(0, color='black', linewidth=0.5, linestyle=':')\nplt.ylabel('State x')\nplt.title(f'Scalar Nonlinear System with DLQR Controller (Q={Qd}, R={Rd})')\nplt.grid(True)\nplt.legend()\nplt.ylim(min(np.nanmin(x_small), np.nanmin(x_large), -1) - 0.5, max(np.nanmax(x_small), np.nanmax(x_large), 1) + 0.5)\n\nplt.subplot(2, 1, 2)\nfinal_idx_small = len(u_small)\nif final_idx_small &gt; 0:\n    # Use step plot for control signal visualization\n    plt.step(t_small[:final_idx_small], u_small[:final_idx_small], where='post',\n             label=f'Control u[k] (x0={x0_small})', linewidth=2)\n\nfinal_idx_large = len(u_large)\nif final_idx_large &gt; 0:\n    plt.step(t_large[:final_idx_large], u_large[:final_idx_large], where='post',\n             label=f'Control u[k] (x0={x0_large})', linewidth=2, linestyle='--')\n\nplt.ylabel('Control Input u')\nplt.xlabel('Time (s)')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nDARE Solution S = 15.257 (for Q=10.0, R=0.1)\nDLQR Gain K = 5.519\nLinear Closed-Loop Pole = 0.381. (STABLE)"
  },
  {
    "objectID": "lectures/lecture9.html#mimo-example-of-dlqr-design",
    "href": "lectures/lecture9.html#mimo-example-of-dlqr-design",
    "title": "EGH445 Modern Control",
    "section": "MIMO Example of DLQR Design",
    "text": "MIMO Example of DLQR Design\nConsider the MIMO system with two states and two inputs:\n\\begin{align*} x(kT+T) &= \\begin{bmatrix} 1 & 0.1 \\\\ 0 & 0.8 \\end{bmatrix} x(kT) + \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} u(kT) \\\\ y(kT) &= Cx(kT) + Du(kT) \\end{align*}\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.linalg as linalg\nimport control as ctrl\n# Create the system matrices\nG = np.array([[1, 0.1], [0, 0.8]])\nH = np.array([[1, 0], [0, 1]])\nC = np.array([[1, 0], [0, 1]]) # Outputs are the states\nD = np.array([[0, 0], [0, 0]])\n\n# Pole placement for MIMO system\nK1 = np.array([[0.6, 0], [0, 0.2]]) \nK2 = np.array([[0.4, 0], [0, 0.4]]) \n\n# QLQR design for MIMO system\nQ = np.array([[10, 0], [0, 10]]) # State weight matrix\nR = np.array([[0.1, 0], [0, 0.1]]) # Control weight matrix\n\n# Solve the Discrete Algebraic Riccati Equation (DARE)\n# S is the solution to the DARE\nS = linalg.solve_discrete_are(G, H, Q, R) \n# Calculate DLQR gain K\nK3 = np.linalg.inv(R + H.T @ S @ H) @ (H.T @ S @ G)\nprint(f\"K3 = [{K3[0, 0]:.3f}, {K3[0, 1]:.3f}; \\n\"\n    f\" {K3[1, 0]:.3f}, {K3[1, 1]:.3f}], \"\n    f\" eig(G - H @ K3) = {np.linalg.eigvals(G - H @ K3)}\")\n\n# Compute the closed-loop system matrices\nGcl1 = G - H @ K1 \nGcl2 = G - H @ K2\nGcl3 = G - H @ K3\n\nTs = 0.1 # Sampling time (seconds)\n# Create the closed-loop LTI systems\nsys1 = ctrl.ss(Gcl1, H, C, D, Ts) \nsys2 = ctrl.ss(Gcl2, H, C, D, Ts) \nsys3 = ctrl.ss(Gcl3, H, C, D, Ts)\n\n# Simulate for initial condition x0 = [1, 0]\nx0 = np.array([1.5, 0.5])\nt_end = 1.5 \nt = np.arange(0, t_end, Ts) \nt1, y1 = ctrl.initial_response(sys1, T=t, X0=x0)\nt2, y2 = ctrl.initial_response(sys2, T=t, X0=x0)\nt3, y3 = ctrl.initial_response(sys3, T=t, X0=x0)\n\nplt.figure(figsize=(10, 4))\nplt.plot(t1, y1[0, :], label='K1 - x1', linewidth=3)\nplt.plot(t1, y1[1, :], label='K1 - x2', linewidth=3)\nplt.plot(t2, y2[0, :], label='K2 - x1', linestyle='--', \n    linewidth=3) \nplt.plot(t2, y2[1, :], label='K2 - x2', linestyle='--', \n    linewidth=3) \nplt.plot(t3, y3[0, :], label='K3 - x1', linestyle=':', \n    linewidth=3) \nplt.plot(t3, y3[1, :], label='K3 - x2', linestyle=':', \n    linewidth=3) \nplt.title('Initial Condition Response ($x_0=[1, 0]^T$)' \n    'with Different K Matrices', fontsize=16) \nplt.xlabel('Time (steps * dt)', fontsize=14) \nplt.ylabel('State Values', fontsize=14) \nplt.legend(fontsize=12)\nplt.grid(True) \nplt.show()\n\n\nK3 = [0.990, 0.099; \n 0.000, 0.792],  eig(G - H @ K3) = [0.00980006 0.0078745 ]"
  },
  {
    "objectID": "lectures/lecture9.html#tuning-q-and-r-the-trade-off",
    "href": "lectures/lecture9.html#tuning-q-and-r-the-trade-off",
    "title": "EGH445 Modern Control",
    "section": "Tuning Q and R: The Trade-off",
    "text": "Tuning Q and R: The Trade-off\n\nThe relative size (Q/R ratio) affects the trade-off between state regulation and control effort.\nIncreasing Q relative to R \\rightarrow increases the penalty on state deviation\n\nfaster state convergence,\nhigher control effort.\n\nIncreasing R relative to Q \\rightarrow increases the penalty on control effort\n\nslower state convergence,\nlower control effort."
  },
  {
    "objectID": "lectures/lecture9.html#quick-example-of-dlqr-tuning",
    "href": "lectures/lecture9.html#quick-example-of-dlqr-tuning",
    "title": "EGH445 Modern Control",
    "section": "Quick Example of DLQR Tuning",
    "text": "Quick Example of DLQR Tuning\n\n\nCode\n# Iterate over different Q values and simulate the system\nQ_values = [1, 10, 50, 100]  # Different state penalty values\nR = 0.1  # Fixed control penalty\nAd = 0.905\nBd = 0.095\nTs = 0.1\nx0 = 2  # Initial condition\n\n# Store results for plotting\nresults = []\n\nfor Q in Q_values:\n  # Solve DARE for each Q\n  S = solve_discrete_are(np.array([[Ad]]), np.array([[Bd]]), np.array([[Q]]), np.array([[R]]))\n  K = np.linalg.inv(R + Bd**2 * S) @ (Bd * S * Ad)\n  K = K[0, 0]  # Extract scalar gain\n\n  # Simulate the system\n  t_history = [0]\n  x_history = [x0]\n  current_x = x0\n  for k in range(50):  # Simulate for 50 steps\n    u = -K * current_x\n    current_x = Ad * current_x + Bd * u\n    t_history.append((k + 1) * Ts)\n    x_history.append(current_x)\n\n  results.append((Q, t_history, x_history))\n\n# Plot results\nplt.figure(figsize=(10, 6))\nfor Q, t_history, x_history in results:\n  plt.plot(t_history, x_history, label=f\"Q={Q}\", linewidth=2)\n\nplt.title(\"DLQR State Response for Different Q Values\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"State x\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/lecture9.html#limitations-of-lqr-control-saturation",
    "href": "lectures/lecture9.html#limitations-of-lqr-control-saturation",
    "title": "EGH445 Modern Control",
    "section": "Limitations of LQR: Control Saturation",
    "text": "Limitations of LQR: Control Saturation\nThe Linear Quadratic Regulator may not be suitable for all systems, however.\nConsider the system \\; \\ddot x = x - c\\dot x + u, where u_{\\min} \\le u(kT) \\le u_{\\max}.\nLet’s compare the simulations:\n\nUnconstrained LQR: No control saturation applied.\nSaturated LQR: Control effort limited to u_{\\min} \\le u(kT) \\le u_{\\max}.\nModel Predictive Control (MPC): Control effort limited to u_{\\min} \\le u(kT) \\le u_{\\max}, but with a prediction horizon."
  },
  {
    "objectID": "lectures/lecture9.html#unsaturated-lqr",
    "href": "lectures/lecture9.html#unsaturated-lqr",
    "title": "EGH445 Modern Control",
    "section": "Unsaturated LQR",
    "text": "Unsaturated LQR\n\n\n# Scenario 1: Unconstrained LQR for Nonzero Regulation (Damped 2nd Order System)\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are, expm\nfrom scipy.signal import cont2discrete\nimport matplotlib.pyplot as plt\nimport time\n\n# print(\"--- Running Scenario 1: Unconstrained LQR (Nonzero Setpoint, Damped System) ---\")\n\n# --- 1. System Definition ---\n# Continuous system: dx/dt = Ac*x + Bc*u\n# x = [y, y_dot]^T = [x1, x2]^T\n# d(x1)/dt = x2\n# d(x2)/dt = x1 - c*x2 + u\nc_damping = 0.5\nAc = np.array([[0., 1.], [1., -c_damping]])\nBc = np.array([[0.], [1.]])\nn_states = Ac.shape[0]\nm_inputs = Bc.shape[1]\n\ndef system_ode(t, x, u, Ac_mat, Bc_mat):\n    u_val = u if np.isscalar(u) else u[0]\n    dxdt = Ac_mat @ x + Bc_mat @ np.array([u_val])\n    return dxdt.flatten()\n\n# --- 2. Discretization ---\nTs = 0.1 # Sampling time\nAd, Bd, _, _, _ = cont2discrete((Ac, Bc, np.eye(n_states), np.zeros((n_states, m_inputs))), Ts, method='zoh')\n# print(\"--- Discrete Model ---\")\n# print(f\"Ad = \\n{Ad}\")\n# print(f\"Bd = \\n{Bd}\")\n\n# --- 3. Setpoint ---\nx_ref = np.array([1.0, 0.0]) # Target state y=1, y_dot=0\n# Calculate corresponding u_ref for discrete system\n# x_ref = Ad*x_ref + Bd*u_ref =&gt; Bd*u_ref = (I-Ad)*x_ref\n# Use pseudo-inverse if Bd is not square: u_ref = pinv(Bd) @ (I-Ad) @ x_ref\ntry:\n    u_ref_vec = np.linalg.pinv(Bd) @ (np.eye(n_states) - Ad) @ x_ref\n    u_ref = u_ref_vec[0] # Extract scalar\n    # Verify continuous u_ref = -x1_ref = -1\n    # print(f\"Setpoint: x_ref = {x_ref.T}, requires discrete u_ref = {u_ref:.3f} (cont. u_ref=-1.0)\")\nexcept Exception as e:\n    print(f\"Could not calculate discrete u_ref: {e}. Using continuous u_ref.\")\n    u_ref = -x_ref[0] # Use continuous calculation\n\n# --- 4. DLQR Design (for error regulation) ---\n# Weights for error cost: sum(xtilde' Q xtilde + utilde' R utilde)\nQd = np.diag([1.0, 0.1]) # Penalize position x1 more\nRd = np.array([[0.01]])   # Aggressive: Low penalty on control effort\n\ntry:\n    S = solve_discrete_are(Ad, Bd, Qd, Rd)\n    Kd_term1 = Rd + Bd.T @ S @ Bd\n    Kd_term2 = Bd.T @ S @ Ad\n    Kd = np.linalg.inv(Kd_term1) @ Kd_term2 # Gain is 1x2\n    print(f\"DLQR Gain for error K = {Kd}\")\nexcept Exception as e:\n    print(f\"DLQR Design failed: {e}\"); exit()\n\n# --- 5. Simulation Setup ---\nt_start = 0\nt_end = 6.0\nn_steps = int(t_end / Ts)\nx0 = np.array([0.0, 0.0]) # Initial condition (start from origin)\n\n# --- Helper function for interval simulation ---\ndef simulate_interval(ode_func, t_start, x_start, duration, control_val):\n    t_eval_interval = [t_start + duration]\n    sol_interval = solve_ivp(\n        ode_func, (t_start, t_start + duration), x_start, method='RK45',\n        t_eval=t_eval_interval, args=(control_val,)\n    )\n    if sol_interval.status != 0: return None, False\n    return sol_interval.y[:, -1], True\n\n# --- 6. Simulation Loop ---\nt_history = [t_start]\nx_history = [x0]\nu_history = []\ncurrent_t = t_start\ncurrent_x = x0.copy()\nsuccess_run = True\n\nstart_sim_time = time.time()\nprint(\"Running simulation...\")\nfor k in range(n_steps):\n    x_error = current_x - x_ref\n    # Control Law: u[k] = u_ref - K*(x[k]-x_ref)\n    u_k_vec = u_ref - Kd @ x_error\n    u_k = u_k_vec[0] # Extract scalar control\n    # NO SATURATION APPLIED HERE\n    u_history.append(u_k)\n\n    def ode_lqr_interval(t, y, u_val):\n        return system_ode(t, y, u_val, Ac, Bc)\n\n    next_x, success = simulate_interval(ode_lqr_interval, current_t, current_x, Ts, u_k)\n\n    if not success:\n        print(f\"ODE solver failed at t={current_t:.2f}\")\n        success_run = False; break\n\n    current_t += Ts\n    current_x = next_x\n    t_history.append(current_t)\n    x_history.append(current_x.copy())\n\n    if np.max(np.abs(current_x)) &gt; 100: # Divergence check\n        print(f\"State diverging excessively at t={current_t:.2f}. Stopping.\")\n        success_run = False; break\n\nprint(f\"Simulation loop time: {time.time() - start_sim_time:.2f} s\")\n\n# --- 7. Plotting ---\nif success_run:\n    t_history = np.array(t_history)\n    x_history = np.array(x_history).T # Shape is (n_states, n_points)\n    u_history = np.array(u_history)\n    n_plot = len(u_history)\n\n    # Define consistent plot limits (adjust if needed)\n    xlims = (t_start, t_end)\n    ylims_x1 = (-0.5, 1.5)\n    ylims_x2 = (-0.5, 1.5)\n    ylims_u = (-4, 1) # Adjusted for u_ref=-1 and expected transient\n\n    plt.figure(\"Unconstrained LQR (Nonzero Setpoint, Damped)\", figsize=(10, 8))\n    plt.rcParams.update({'font.size': 12})\n\n    plt.subplot(3, 1, 1)\n    plt.plot(t_history[:n_plot+1], x_history[0,:n_plot+1], label='$x_1$ (y)', lw=2)\n    plt.axhline(x_ref[0], color='k', linestyle=':', label=f'Setpoint $x_1={x_ref[0]}$')\n    plt.title(f'Unconstrained LQR (Setpoint x=[{x_ref[0]},{x_ref[1]}]^T)')\n    plt.ylabel('Position $x_1$')\n    plt.ylim(ylims_x1); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 2)\n    plt.plot(t_history[:n_plot+1], x_history[1,:n_plot+1], label='$x_2$ (dy/dt)', lw=2)\n    plt.axhline(x_ref[1], color='k', linestyle=':', label=f'Setpoint $x_2={x_ref[1]}$')\n    plt.ylabel('Velocity $x_2$')\n    plt.ylim(ylims_x2); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 3)\n    plt.step(t_history[:n_plot], u_history[:n_plot], where='post', label='Control u[k]', lw=2)\n    plt.axhline(u_ref, color='k', linestyle=':', label=f'Steady-State u={u_ref:.2f}')\n    plt.ylabel('Control Input u')\n    plt.xlabel('Time (s)')\n    plt.ylim(ylims_u); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Simulation failed, not plotting.\")\n\n\nDLQR Gain for error K = [[8.76224795 4.36193866]]\nRunning simulation...\nSimulation loop time: 0.02 s"
  },
  {
    "objectID": "lectures/lecture9.html#saturated-lqr",
    "href": "lectures/lecture9.html#saturated-lqr",
    "title": "EGH445 Modern Control",
    "section": "Saturated LQR",
    "text": "Saturated LQR\n\n\n# Scenario 2: Saturated LQR for Nonzero Regulation (Damped 2nd Order System)\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are, expm\nfrom scipy.signal import cont2discrete\nimport matplotlib.pyplot as plt\nimport time\n\n# print(\"--- Running Scenario 2: Saturated LQR (Nonzero Setpoint, Damped System) ---\")\n\n# --- 1. System Definition ---\nc_damping = 0.5\nAc = np.array([[0., 1.], [1., -c_damping]])\nBc = np.array([[0.], [1.]])\nn_states = Ac.shape[0]\nm_inputs = Bc.shape[1]\ndef system_ode(t, x, u, Ac_mat, Bc_mat):\n    u_val = u if np.isscalar(u) else u[0]\n    dxdt = Ac_mat @ x + Bc_mat @ np.array([u_val])\n    return dxdt.flatten()\n\n# --- 2. Discretization ---\nTs = 0.1\nAd, Bd, _, _, _ = cont2discrete((Ac, Bc, np.eye(n_states), np.zeros((n_states, m_inputs))), Ts, method='zoh')\n\n# --- 3. Setpoint ---\nx_ref = np.array([1.0, 0.0])\ntry:\n    u_ref_vec = np.linalg.pinv(Bd) @ (np.eye(n_states) - Ad) @ x_ref\n    u_ref = u_ref_vec[0]\n    # print(f\"Setpoint: x_ref = {x_ref.T}, requires discrete u_ref = {u_ref:.3f} (cont. u_ref=-1.0)\")\nexcept Exception as e:\n    print(f\"Could not calculate discrete u_ref: {e}. Using continuous u_ref.\")\n    u_ref = -x_ref[0]\n\n# --- 4. DLQR Design (for error regulation) ---\nQd = np.diag([1.0, 0.1])\nRd = np.array([[0.01]])\ntry:\n    S = solve_discrete_are(Ad, Bd, Qd, Rd)\n    Kd_term1 = Rd + Bd.T @ S @ Bd\n    Kd_term2 = Bd.T @ S @ Ad\n    Kd = np.linalg.inv(Kd_term1) @ Kd_term2\n    print(f\"DLQR Gain for error K = {Kd}\")\nexcept Exception as e:\n    print(f\"DLQR Design failed: {e}\"); exit()\n\n# --- 5. Simulation Setup ---\nt_start = 0\nt_end = 6.0\nn_steps = int(t_end / Ts)\nx0 = np.array([0.0, 0.0])\nx_eq_ref = np.array([x_ref]) # Use as reference, not equilibrium for deviation calc\nu_min = -2.5 # Saturation Limits (Allow u_ref=-1)\nu_max = 0.5\nprint(f\"Control Limits: [{u_min}, {u_max}]\")\n\n# --- Helper function for interval simulation ---\ndef simulate_interval(ode_func, t_start, x_start, duration, control_val):\n    t_eval_interval = [t_start + duration]\n    sol_interval = solve_ivp(\n        ode_func, (t_start, t_start + duration), x_start, method='RK45',\n        t_eval=t_eval_interval, args=(control_val,)\n    )\n    if sol_interval.status != 0: return None, False\n    return sol_interval.y[:, -1], True\n\n# --- 6. Simulation Loop ---\nt_history = [t_start]\nx_history = [x0]\nu_lqr_desired_hist = []\nu_sat_applied_hist = []\ncurrent_t = t_start\ncurrent_x = x0.copy()\nsuccess_run = True\n\nstart_sim_time = time.time()\n# print(\"Running simulation...\")\nfor k in range(n_steps):\n    x_error = current_x - x_ref\n    # Calculate desired LQR control\n    u_lqr_k_vec = u_ref - Kd @ x_error\n    u_lqr_k = u_lqr_k_vec[0]\n    u_lqr_desired_hist.append(u_lqr_k)\n\n    # Apply saturation\n    u_sat_k = np.clip(u_lqr_k, u_min, u_max)\n    u_sat_applied_hist.append(u_sat_k)\n\n    def ode_satlqr_interval(t, y, u_val):\n        return system_ode(t, y, u_val, Ac, Bc)\n\n    next_x, success = simulate_interval(ode_satlqr_interval, current_t, current_x, Ts, u_sat_k)\n\n    if not success:\n        print(f\"ODE solver failed at t={current_t:.2f}\")\n        success_run = False; break\n\n    current_t += Ts\n    current_x = next_x\n    t_history.append(current_t)\n    x_history.append(current_x.copy())\n\n    if np.max(np.abs(current_x)) &gt; 100: # Divergence check\n        print(f\"State diverging excessively at t={current_t:.2f}. Stopping.\")\n        success_run = False; break\n\nprint(f\"Simulation loop time: {time.time() - start_sim_time:.2f} s\")\n\n# --- 7. Plotting ---\nif success_run:\n    t_history = np.array(t_history)\n    x_history = np.array(x_history).T\n    u_lqr_desired_hist = np.array(u_lqr_desired_hist)\n    u_sat_applied_hist = np.array(u_sat_applied_hist)\n    n_plot = len(u_sat_applied_hist)\n\n    # Use consistent plot limits from Block 1 (or adjust if needed)\n    xlims = (t_start, t_end)\n    ylims_x1 = (-0.5, 1.5)\n    ylims_x2 = (-0.5, 1.5)\n    ylims_u = (-4, 1) # Keep wide limits to see desired vs saturated\n\n    plt.figure(\"Saturated LQR (Nonzero Setpoint, Damped)\", figsize=(10, 8))\n    plt.rcParams.update({'font.size': 12})\n\n    plt.subplot(3, 1, 1)\n    plt.plot(t_history[:n_plot+1], x_history[0,:n_plot+1], label='$x_1$ (y)', lw=2)\n    plt.axhline(x_ref[0], color='k', linestyle=':', label=f'Setpoint $x_1={x_ref[0]}$')\n    plt.title(f'Saturated LQR (Setpoint x=[{x_ref[0]},{x_ref[1]}]^T, u limits=[{u_min},{u_max}])')\n    plt.ylabel('Position $x_1$')\n    plt.ylim(ylims_x1); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 2)\n    plt.plot(t_history[:n_plot+1], x_history[1,:n_plot+1], label='$x_2$ (dy/dt)', lw=2)\n    plt.axhline(x_ref[1], color='k', linestyle=':', label=f'Setpoint $x_2={x_ref[1]}$')\n    plt.ylabel('Velocity $x_2$')\n    plt.ylim(ylims_x2); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 3)\n    plt.step(t_history[:n_plot], u_lqr_desired_hist[:n_plot], where='post', label='Desired LQR u[k]', lw=1.5, ls=':')\n    plt.step(t_history[:n_plot], u_sat_applied_hist[:n_plot], where='post', label='Applied Saturated u[k]', lw=2)\n    plt.axhline(u_ref, color='k', linestyle=':', label=f'Steady-State u={u_ref:.2f}')\n    plt.axhline(u_max, color='r', linestyle=':', label='Limit')\n    plt.axhline(u_min, color='r', linestyle=':')\n    plt.ylabel('Control Input u')\n    plt.xlabel('Time (s)')\n    plt.ylim(ylims_u); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Simulation failed, not plotting.\")\n\n\nDLQR Gain for error K = [[8.76224795 4.36193866]]\nControl Limits: [-2.5, 0.5]\nSimulation loop time: 0.01 s"
  },
  {
    "objectID": "lectures/lecture9.html#mpc",
    "href": "lectures/lecture9.html#mpc",
    "title": "EGH445 Modern Control",
    "section": "MPC",
    "text": "MPC\n\n\n# Scenario 3: MPC for Nonzero Regulation (Damped 2nd Order System)\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are, expm\nfrom scipy.signal import cont2discrete\nimport cvxpy as cp # Requires cvxpy and a solver like OSQP\nimport matplotlib.pyplot as plt\nimport time\n\n# print(\"--- Running Scenario 3: MPC (Nonzero Setpoint, Damped System) ---\")\n\n# --- 1. System Definition ---\nc_damping = 0.5\nAc = np.array([[0., 1.], [1., -c_damping]])\nBc = np.array([[0.], [1.]])\nn_states = Ac.shape[0]\nm_inputs = Bc.shape[1]\ndef system_ode(t, x, u, Ac_mat, Bc_mat):\n    u_val = u if np.isscalar(u) else u[0]\n    dxdt = Ac_mat @ x + Bc_mat @ np.array([u_val])\n    return dxdt.flatten()\n\n# --- 2. Discretization ---\nTs = 0.1\nAd, Bd, _, _, _ = cont2discrete((Ac, Bc, np.eye(n_states), np.zeros((n_states, m_inputs))), Ts, method='zoh')\n# print(\"--- Discrete Model ---\")\n# print(f\"Ad = \\n{Ad}\")\n# print(f\"Bd = \\n{Bd}\")\n\n# --- 3. Setpoint ---\nx_ref = np.array([[1.0], [0.0]]) # Target state y=1, y_dot=0 (use column vector shape)\ntry:\n    u_ref_vec = np.linalg.pinv(Bd) @ (np.eye(n_states) - Ad) @ x_ref\n    u_ref = u_ref_vec[0,0] # Extract scalar\n    # print(f\"Setpoint: x_ref = {x_ref.T}, requires discrete u_ref = {u_ref:.3f} (cont. u_ref=-1.0)\")\nexcept Exception as e:\n    print(f\"Could not calculate discrete u_ref: {e}. Using continuous u_ref.\")\n    u_ref = -x_ref[0,0] # Use continuous calculation\n\n# --- 4. MPC Setup ---\nP = 30 # Prediction Horizon\n# Use same base weights as LQR example, but consider tuning R_mpc\nQ_mpc = np.diag([1.0, 0.1])\n# R_mpc = np.array([[0.01]]) # Original aggressive LQR R\nR_mpc = np.array([[0.1]]) # Use less aggressive R for MPC? Try this.\n# Calculate terminal weight Qf from DARE solution S using MPC weights\ntry:\n    S = solve_discrete_are(Ad, Bd, Q_mpc, R_mpc)\n    Qf_mpc = S\n    # print(f\"Using terminal weight Qf=S based on MPC Q/R: \\n{Qf_mpc}\")\nexcept Exception as e:\n    print(f\"DARE solve failed ({e}), using Qf=Q\"); Qf_mpc = Q_mpc\n\n# Constraints\nu_min = -2.5\nu_max = 0.5\nprint(f\"Control Limits: [{u_min}, {u_max}]\")\n\n# --- 5. CVXPY Optimization Problem Setup ---\nU = cp.Variable((m_inputs, P), name='U')\nx_k_param = cp.Parameter(n_states, name='x_k_param') # Current state parameter\ncost = 0.0\nconstraints = []\nx_pred = x_k_param # Use parameter for current state\n\nfor t in range(P):\n    # Penalize deviation from reference state & absolute control input\n    cost += cp.quad_form(x_pred - x_ref.flatten(), Q_mpc) + cp.quad_form(U[:, t], R_mpc)\n    x_next = Ad @ x_pred + Bd @ U[:, t]\n    constraints += [U[:, t] &gt;= u_min, U[:, t] &lt;= u_max]\n    x_pred = x_next\n# Terminal cost penalizes deviation from x_ref\ncost += cp.quad_form(x_pred - x_ref.flatten(), Qf_mpc)\n\nobjective = cp.Minimize(cost)\nproblem = cp.Problem(objective, constraints)\nprint(\"MPC CVXPY Problem defined.\")\n\n# --- 6. Simulation Setup ---\nt_start = 0\nt_end = 6.0\nn_steps = int(t_end / Ts)\nx0 = np.array([0.0, 0.0]) # Initial condition\n\n# --- Helper function for interval simulation ---\ndef simulate_interval(ode_func, t_start, x_start, duration, control_val):\n    t_eval_interval = [t_start + duration]\n    sol_interval = solve_ivp(\n        ode_func, (t_start, t_start + duration), x_start, method='RK45',\n        t_eval=t_eval_interval, args=(control_val,)\n    )\n    if sol_interval.status != 0: return None, False\n    return sol_interval.y[:, -1], True\n\n# --- 7. Simulation Loop ---\nt_history = [t_start]\nx_history = [x0]\nu_history = [] # Applied MPC control\ncurrent_t = t_start\ncurrent_x = x0.copy()\nsuccess_run = True\ninfeasibility_count = 0\n\nstart_sim_time = time.time()\n# print(\"Running simulation...\")\nfor k in range(n_steps):\n    x_k_param.value = current_x # Update parameter\n    u_k = u_ref # Default if solver fails\n    try:\n        problem.solve(solver=cp.OSQP, warm_start=True, verbose=False)\n        if problem.status == cp.OPTIMAL or problem.status == cp.OPTIMAL_INACCURATE:\n            u_k = U.value[0, 0]\n        else:\n            print(f\"MPC Warning: Solver status {problem.status} at k={k}\")\n            infeasibility_count += 1\n            if U.value is not None: u_k = U.value[0, 0]\n            if problem.status == cp.INFEASIBLE:\n                 print(\"MPC Problem is infeasible. Stopping.\"); success_run = False; break\n    except Exception as e:\n        print(f\"MPC Solver failed at k={k}: {e}\"); infeasibility_count += 1\n\n    # Apply saturation as safety/clip numerical tolerances\n    u_k_sat = np.clip(u_k, u_min, u_max)\n    u_history.append(u_k_sat)\n\n    def ode_mpc_interval(t, y, u_val):\n        return system_ode(t, y, u_val, Ac, Bc)\n\n    next_x, success = simulate_interval(ode_mpc_interval, current_t, current_x, Ts, u_k_sat)\n\n    if not success:\n        print(f\"ODE solver failed at t={current_t:.2f}\")\n        success_run = False; break\n\n    current_t += Ts\n    current_x = next_x\n    t_history.append(current_t)\n    x_history.append(current_x.copy())\n\n    if np.max(np.abs(current_x)) &gt; 100: # Divergence check\n        print(f\"State diverging excessively at t={current_t:.2f}. Stopping.\")\n        success_run = False; break\n\nprint(f\"MPC Simulation loop time: {time.time() - start_sim_time:.2f} s\")\nif infeasibility_count &gt; 0: print(f\"Warning: MPC Solver non-optimal status {infeasibility_count} times.\")\n\n# --- 8. Plotting ---\nif success_run:\n    t_history = np.array(t_history)\n    x_history = np.array(x_history).T\n    u_history = np.array(u_history)\n    n_plot = len(u_history)\n\n    # Use consistent plot limits from Block 1 & 2 (or adjust)\n    xlims = (t_start, t_end)\n    ylims_x1 = (-0.5, 1.5)\n    ylims_x2 = (-0.5, 1.5)\n    ylims_u = (-4, 1) # Use same limits\n\n    plt.figure(\"MPC (Nonzero Setpoint, Damped)\", figsize=(10, 8))\n    plt.rcParams.update({'font.size': 12})\n\n    plt.subplot(3, 1, 1)\n    plt.plot(t_history[:n_plot+1], x_history[0,:n_plot+1], label='$x_1$ (y)', lw=2)\n    plt.axhline(x_ref[0], color='k', linestyle=':', label=f'Setpoint $x_1={x_ref[0,0]}$')\n    plt.title(f'MPC Response (Setpoint x=[{x_ref[0,0]},{x_ref[1,0]}]^T, u limits=[{u_min},{u_max}], P={P})')\n    plt.ylabel('Position $x_1$')\n    plt.ylim(ylims_x1); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 2)\n    plt.plot(t_history[:n_plot+1], x_history[1,:n_plot+1], label='$x_2$ (dy/dt)', lw=2)\n    plt.axhline(x_ref[1], color='k', linestyle=':', label=f'Setpoint $x_2={x_ref[1,0]}$')\n    plt.ylabel('Velocity $x_2$')\n    plt.ylim(ylims_x2); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 3)\n    plt.step(t_history[:n_plot], u_history[:n_plot], where='post', label='Applied MPC u[k]', lw=2)\n    plt.axhline(u_ref, color='k', linestyle=':', label=f'Steady-State u={u_ref:.2f}')\n    plt.axhline(u_max, color='r', linestyle=':', label='Limit')\n    plt.axhline(u_min, color='r', linestyle=':')\n    plt.ylabel('Control Input u')\n    plt.xlabel('Time (s)')\n    plt.ylim(ylims_u); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Simulation failed, not plotting.\")\n\n\nControl Limits: [-2.5, 0.5]\nMPC CVXPY Problem defined.\n\n\nMPC Simulation loop time: 8.20 s"
  },
  {
    "objectID": "lectures/lecture9.html#how-mpc-works",
    "href": "lectures/lecture9.html#how-mpc-works",
    "title": "EGH445 Modern Control",
    "section": "How MPC Works",
    "text": "How MPC Works\n\nFinds an optimal control sequence for a finite-horizon problem.\nThe first control input of the optimal sequence is applied to the system.\nRepeats at the next time step, using the current state as the new initial condition.\n\n\nMPC vs LQR:\n\nMPC handles constraints (saturation, state limits, rates of change, etc).\nMPC is more complex and computationally intensive than LQR.\nMPC’s performance depends on the accuracy of the system model."
  },
  {
    "objectID": "lectures/lecture9.html#quadratic-forms-and-positiveness",
    "href": "lectures/lecture9.html#quadratic-forms-and-positiveness",
    "title": "EGH445 Modern Control",
    "section": "Quadratic Forms and Positiveness",
    "text": "Quadratic Forms and Positiveness\nFor a square matrix P and a vector x, the product x^\\intercal P x is a scalar called a quadratic form.\nFor example, \\; [x_1, x_2] \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = 1x_1^2 + 2x_1x_2 + 3x_1x_2 + 4x_2^2.\nP\\succeq 0 means that P is positive semi-definite, i.e. x^T P x \\geq 0 for all x \\in \\mathbb{R}^n.\nP\\succ 0 means that P is positive definite, i.e. x^T P x &gt; 0 for all x \\in \\mathbb{R}^n and x \\neq 0.\n\n\nA symmetric matrix P is positive definite if (and only if)\n\nall its eigenvalues are positive.\nall its principal minors are positive. \nP = \\begin{bmatrix} p_{1,1} & p_{1,2} & p_{1,3} \\\\\n    p_{2,1} & p_{2,2} & p_{2,3} \\\\ p_{3,1} & p_{3,2} & p_{3,3} \\end{bmatrix} \\succ 0 \\iff p_{1,1} &gt; 0, \\; det\\begin{bmatrix} p_{1,1} & p_{1,2} \\\\ p_{2,1} & p_{2,2} \\end{bmatrix} &gt; 0, \\; det(P) &gt; 0"
  },
  {
    "objectID": "lectures/lecture9.html#lqr-cost-function-and-tuning",
    "href": "lectures/lecture9.html#lqr-cost-function-and-tuning",
    "title": "EGH445 Modern Control",
    "section": "LQR Cost Function and Tuning",
    "text": "LQR Cost Function and Tuning\nNote that the cost function is a quadratic form in the state and control variables.\n\n\nJ = \\sum_{k=0}^{\\infty} (x(k)^\\intercal Q x(k) + u(k)^\\intercal R u(k))\n\n\n\nConsider x(k) = [x_1(k), x_2(k)]^\\intercal and u(k) = [u_1(k), u_2(k)]^\\intercal. The cost function can be expressed as:\n\n\n\\begin{align*}\nJ &= \\sum_{k=0}^{\\infty} (q_{1,1}x_1(k)^2 + q_{1,2}x_1(k)x_2(k) + q_{2,1}x_2(k)x_1(k) + q_{2,2}x_2(k)^2 \\\\ &+ r_{1,1}u_1(k)^2 + r_{1,2}u_1(k)u_2(k) + r_{2,1}u_2(k)u_1(k) + r_{2,2}u_2(k)^2)\n\\end{align*}\n\n\nNotice that you have freedom to tune the individual weights q_{i,j} and r_{i,j} in the cost function."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "2025/week9_workshop.html#title-slide",
    "href": "2025/week9_workshop.html#title-slide",
    "title": "EGH445 Modern Control",
    "section": "Title Slide",
    "text": "Title Slide\n\n\n\n\n\nModern Control\n\n\nWeek 9 Workshop\n\n\n\nDr Guilherme Froes Silva School of Electrical Engineering & Robotics Queensland University of Technology\n\n\nEGH445 - Modern Control\n\n\n\nConsultation: GP-S1111 Email: g.froessilva@qut.edu.au"
  },
  {
    "objectID": "2025/week9_workshop.html#diversity-in-stem",
    "href": "2025/week9_workshop.html#diversity-in-stem",
    "title": "EGH445 Modern Control",
    "section": "Diversity in STEM",
    "text": "Diversity in STEM"
  },
  {
    "objectID": "2025/week9_workshop.html#discrete-time-control-design-3-lecture-content",
    "href": "2025/week9_workshop.html#discrete-time-control-design-3-lecture-content",
    "title": "EGH445 Modern Control",
    "section": "Discrete-Time Control Design 3: Lecture Content",
    "text": "Discrete-Time Control Design 3: Lecture Content\n\n\n\n\nOverview\n\nSome content review\nLimitations of pole placement\nOptimal Control\n\nLinear Quadratic Regulator (LQR)\nModel Predictive Control (MPC)"
  },
  {
    "objectID": "2025/week9_workshop.html#why-move-beyond-pole-placement",
    "href": "2025/week9_workshop.html#why-move-beyond-pole-placement",
    "title": "EGH445 Modern Control",
    "section": "Why Move Beyond Pole Placement?",
    "text": "Why Move Beyond Pole Placement?\n\n\nTheoretical\n\n\nWorks well:\n\nSISO, low-order systems\n\nDoes not work well:\n\nHigher-order systems\nMIMO systems\nHighly nonlinear systems\n\n\n\nPractical Implications\n\n\nComplexity\n\nTuning difficult for complex systems.\n\nResource Constraints\n\nDoes not limit control effort.\n\nTrade-offs\n\nNo systematic way to manage."
  },
  {
    "objectID": "2025/week9_workshop.html#linear-quadratic-regulator-lqr",
    "href": "2025/week9_workshop.html#linear-quadratic-regulator-lqr",
    "title": "EGH445 Modern Control",
    "section": "Linear Quadratic Regulator (LQR)",
    "text": "Linear Quadratic Regulator (LQR)\n\n\nTheoretical\n\n\nSeeks the best controller that minimizes J1.\nCost function J quantifies performance goals.\n\nx^\\intercal Q x penalizes state deviations.\nu^\\intercal R u penalizes control effort.\n\nSolution needs the unique, positive semi-definite solution of the Riccati equation.\nControl law K = (R+H^\\intercal PH)^{-1}H^\\intercal P G.\n\n\nPractical Implications\n\n\nLQR forces explicit definition of performance by selection of Q and R.\nSystematic way to manage trade-offs.\nComputational tools are necessary to solve the Riccati equation.\n\n\nJ=\\sum_{t=0}^{\\infty} (x^\\intercal Q x + u^\\intercal R u)"
  },
  {
    "objectID": "2025/week9_workshop.html#lqr-design",
    "href": "2025/week9_workshop.html#lqr-design",
    "title": "EGH445 Modern Control",
    "section": "LQR Design",
    "text": "LQR Design\n\n\nTheoretical\n\n\nControllability of (G,H) is necessary.\nObservability1 of (G,V) is necessary.\n\nQ = V^\\intercal V. This captures which states are  seen by the cost function.\n\nThe ratio of Q and R determines the trade-off.\nTuning involves adjusting Q and R and simulating.\n\n\nPractical Implications\n\n\nNeed accurate linear model.\nConsideration of which states need to converge faster (or cannot deviate much).\nConsideration of which control effort is acceptable.\nThe weights might be physically meaningful.\n\nR – energy cost of actuators.\nQ – square error penalties of states in metres.\n\n\n\nCost-related observability."
  },
  {
    "objectID": "2025/week9_workshop.html#beyond-lqr-model-predictive-control-mpc",
    "href": "2025/week9_workshop.html#beyond-lqr-model-predictive-control-mpc",
    "title": "EGH445 Modern Control",
    "section": "Beyond LQR: Model Predictive Control (MPC)",
    "text": "Beyond LQR: Model Predictive Control (MPC)\n\n\nTheoretical\n\n\nLQR is a static controller.\n\nUnconstrained optmimisation problem.\nInfinite-horizon cost function.\n\nMPC is a dynamic controller.\n\nExplcitly considers constraints.\nFinite-horizon cost function.\n\nCalculates optimal control sequence.\nApplies only the first control action.\nRepeats (receding horizon).\n\n\n\n\nPractical Implications\n\n\nReal systems have physical limitations.\nStates might have bounds (e.g., safety zones).\nMPC is significantly more complex and computationally expensive.\nMPC also relies on accurate models (nonlinear extensions do exist)."
  },
  {
    "objectID": "2025/week9.html#diversity-in-stem",
    "href": "2025/week9.html#diversity-in-stem",
    "title": "EGH445 Modern Control",
    "section": "Diversity in STEM",
    "text": "Diversity in STEM"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lectures/lecture10.html#title-slide",
    "href": "lectures/lecture10.html#title-slide",
    "title": "EGH445 Modern Control",
    "section": "Title Slide",
    "text": "Title Slide\n\n\n\n\n\nState Estimation\n\n\nObservers and Output Feedback\n\n\n\nDr Guilherme Froes Silva School of Electrical Engineering & Robotics Queensland University of Technology\n\n\nEGH445 - Modern Control\n\n\n\nConsultation: GP-S1111 Email: g.froessilva@qut.edu.au"
  },
  {
    "objectID": "lectures/lecture10.html#state-availability-problem",
    "href": "lectures/lecture10.html#state-availability-problem",
    "title": "EGH445 Modern Control",
    "section": "State Availability Problem",
    "text": "State Availability Problem\n\n\n\n\n\n\nImportant\n\n\nHow can we implement state-feedback, u(kT) = -Kx(kT), when x(kT) is not directly measured?\n\n\n\n\nCan we calculate the state from the output?\nGiven the output equation for a discrete-time system: \ny(kT) = Cx(kT) + Du(kT)\n\nAssuming D=0 for simplicity, could we  simply invert C?\n\n\\hat{x}(kT) \\overset{?}{=} C^{-1}y(kT)"
  },
  {
    "objectID": "lectures/lecture10.html#can-we-calculate-the-state-from-the-output",
    "href": "lectures/lecture10.html#can-we-calculate-the-state-from-the-output",
    "title": "EGH445 Modern Control",
    "section": "Can we calculate the state from the output?",
    "text": "Can we calculate the state from the output?\n\nThis is not possible in general!\nAs the matrix C\\in\\mathbb{R}^{p \\times n} is often not square.\n\n\nConsider a system with two states (n=2) and one output (p=1): \n\\begin{align*}\n\\underbrace{\\begin{bmatrix}x_1(t) \\\\ x_2(t)\\end{bmatrix}}_{x(t)\\in\\mathbb{R}^n} &= \\begin{bmatrix} 0 & 1 \\\\ -0.1 & -0.2 \\end{bmatrix} \\begin{bmatrix}x_1(t) \\\\ x_2(t)\\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u(t) \\\\\n\\underbrace{y(t)}_{y(t)\\in\\mathbb{R}^p} &= \\begin{bmatrix} 1 & 0 \\end{bmatrix}_{\\textcolor{red}{p\\times n}} \\begin{bmatrix}x_1(t) \\\\ x_2(t)\\end{bmatrix}\n\\end{align*}"
  },
  {
    "objectID": "lectures/lecture10.html#can-we-calculate-the-state-from-the-input",
    "href": "lectures/lecture10.html#can-we-calculate-the-state-from-the-input",
    "title": "EGH445 Modern Control",
    "section": "Can we calculate the state from the input?",
    "text": "Can we calculate the state from the input?\nThat is, can we simulate the system and use the states?\n\n\n\nConsider the discrete-time system: \n\\textcolor{blue}{x(kT+T) = Gx(kT) + Hu(kT)}\n Let’s build a “copy” of the system to estimate the state \\hat{x}(kT): \n\\textcolor{green}{\\hat{x}(kT+T) = G\\hat{x}(kT) + Hu(kT)}\n\n\n\n\nThis is an open-loop observer or predictor."
  },
  {
    "objectID": "lectures/lecture10.html#open-loop-estimation-error",
    "href": "lectures/lecture10.html#open-loop-estimation-error",
    "title": "EGH445 Modern Control",
    "section": "Open-loop estimation error",
    "text": "Open-loop estimation error\nDefine the estimation error e(kT) = \\textcolor{blue}{x(kT)} - \\textcolor{green}{\\hat{x}(kT)}.\nNow, let’s find the dynamics of the estimation error: \\begin{align*}\ne(kT+T) &= \\textcolor{blue}{x(kT+T)} - \\textcolor{green}{\\hat{x}(kT+T)} \\\\\n&= \\textcolor{blue}{(Gx(kT) + Hu(kT))} - \\textcolor{green}{(G\\hat{x}(kT) + Hu(kT))} \\\\\n&= G\\left(x(kT) - \\hat{x}(kT)\\right) \\\\\n&= G e(kT)\n\\end{align*}\nProblem: The error dynamics are given by the open-loop system matrix G.\n\nIf G has eigenvalues outside the unit circle (unstable system), the estimation error e(kT) will diverge, regardless of the initial estimate \\hat{x}(0).\nEven if G is stable, the error might converge too slowly for effective control."
  },
  {
    "objectID": "lectures/lecture10.html#discrete-time-luenberger-observer",
    "href": "lectures/lecture10.html#discrete-time-luenberger-observer",
    "title": "EGH445 Modern Control",
    "section": "Discrete-Time Luenberger Observer",
    "text": "Discrete-Time Luenberger Observer\n\n\nConsider the system: \\begin{align*}\n\\textcolor{blue}{x(kT+T)} &= \\textcolor{blue}{Gx(kT) + Hu(kT)} \\\\\n\\textcolor{blue}{y(kT)} &= \\textcolor{blue}{Cx(kT)}\n\\end{align*} (Assuming D=0 for simplicity, can be added back later). Luenberger Observer:  \\color{green}\n\\hat{x}(kT+T) = G\\hat{x}(kT) + Hu(kT) + \\textcolor{red}{L \\big(y(kT) - \\hat{y}(kT)\\big)}\n\n\n\n\n\n\n\n\\hat{x}(kT) is the state estimate at time kT.\nL is the observer gain matrix (to be designed).\n\n\n\ny(kT) is the actual measured output.\n\\hat{y}(kT) = C\\hat{x}(kT) is the estimated output."
  },
  {
    "objectID": "lectures/lecture10.html#section",
    "href": "lectures/lecture10.html#section",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Observer Error Dynamics:\nThe observer equation is: \n\\begin{align*}\n\\hat{x}(kT+T) &= G\\hat{x}(kT) + Hu(kT) + L (Cx(kT) - C\\hat{x}(kT)) \\\\\n\\color{green}\n\\hat{x}(kT+T) &= \\color{green}(G - L C)\\hat{x}(kT) + Hu(kT) + L C x(kT)\n\\end{align*}\n\nNow, let’s find the dynamics of the estimation error e(kT) = x(kT) - \\hat{x}(kT): \\begin{align*}\ne(kT+T) &= x(kT+T) - \\hat{x}(kT+T) \\\\\n&= \\textcolor{blue}{(Gx(kT) + Hu(kT))} - \\textcolor{green}{((G - L C)\\hat{x}(kT) + Hu(kT) + L C x(kT))} \\\\\n&= G(x(kT) - \\hat{x}(kT)) - L C (x(kT) - \\hat{x}(kT)) \\\\\n&= \\textcolor{red}{(G - L C)} e(kT)\n\\end{align*}\n\n\n\n\n\n\nImportant\n\n\nThe observer error dynamics are governed by the matrix (G - L C). We want to choose the observer gain L such that the error e(kT) converges to zero quickly, i.e., the eigenvalues of (G - L C) are stable (magnitude less than 1) and have desired dynamics."
  },
  {
    "objectID": "lectures/lecture10.html#revisiting-observability",
    "href": "lectures/lecture10.html#revisiting-observability",
    "title": "EGH445 Modern Control",
    "section": "Revisiting Observability",
    "text": "Revisiting Observability\n\n\n\n\n\n\nAnalogy\n\n\nFor state feedback u = -Kx, we could arbitrarily place the eigenvalues of (G - HK) if and only if the system (G, H) was controllable.\nFor observer design, the ability to place the eigenvalues of (G - L C) depends on the system (G,C) being observable.\n\n\n\n\n\n\n\n\n\n\nObservability\n\n\nA discrete-time system, \\,x(kT+T) = Gx(kT) + Hu(kT), \\quad y(kT) = Cx(kT), is completely observable if, for any initial time k_0 T, the initial state x(k_0 T) can be uniquely determined from the knowledge of the input sequence u(kT) and the output sequence y(kT) for a finite number of steps."
  },
  {
    "objectID": "lectures/lecture10.html#determining-observability",
    "href": "lectures/lecture10.html#determining-observability",
    "title": "EGH445 Modern Control",
    "section": "Determining Observability",
    "text": "Determining Observability\n\n\n\n\n\n\nTheorem\n\n\nThe pair (G, C) is completely observable if and only if the observability matrix \\mathcal{O} has rank n, where n is the dimension of the state vector x.\n\n\n\n\nHow to check:\n\nConstruct the matrix \\mathcal{O} = \\begin{bmatrix} C \\;\\; CG \\;\\; CG^2 \\; \\dots \\; CG^{n-1} \\end{bmatrix}^\\intercal.\nCalculate its rank\n\nnumpy.linalg.matrix_rank(ObsvMatrix) (Python)\nrank(ObsvMatrix) (MATLAB).\n\nIf \\mathcal{O} is square, it is full rank if and only if \\det(\\mathcal{O}) \\neq 0.\nIf \\text{rank}(\\mathcal{O}) &lt; n, the system is not completely observable."
  },
  {
    "objectID": "lectures/lecture10.html#example-observability-check",
    "href": "lectures/lecture10.html#example-observability-check",
    "title": "EGH445 Modern Control",
    "section": "Example: Observability Check",
    "text": "Example: Observability Check\nConsider a discrete-time model for a vehicle, with m=1, b=0.5.\nAssume a sampling time T=0.1.\nContinuous: A = \\begin{bmatrix} 0 & 1 \\\\ 0 & -b/m \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 0 & -0.5 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 0 \\\\ 1/m \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\nDiscretised (using c2d(...) in MATLAB): G = \\begin{bmatrix} 1 & 0 \\\\ 0.0975 & 0.9512 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 0.0049 \\\\ 0.0975 \\end{bmatrix}."
  },
  {
    "objectID": "lectures/lecture10.html#section-1",
    "href": "lectures/lecture10.html#section-1",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Case 1: Position Measurement\nLet C_1 = \\begin{bmatrix} 1 & 0 \\end{bmatrix}.\n\\mathcal{O}_1 = \\begin{bmatrix} C_1 \\\\ C_1 G \\end{bmatrix} =\n\\begin{bmatrix}\n1.0000    &     0 \\\\\n1.0000  &  0.0975\n\\end{bmatrix}.\n\\det(\\mathcal{O}_1) = 1 \\times 0.0975 - 0 \\times 1 = 0.0975 \\neq 0.\nRank is 2. System is observable when measuring position.\n\nCase 2: Velocity Measurement\nC_2 = \\begin{bmatrix} 0 & 1 \\end{bmatrix}.\n\\mathcal{O}_2 = \\begin{bmatrix} C_2 \\\\ C_2 G \\end{bmatrix} =\n\\begin{bmatrix}\n  0  &  1.0000 \\\\\n  0  &  0.9512\n\\end{bmatrix}.\n\\det(\\mathcal{O}_2) = 0 \\times 0.9512 - 0 \\times 1 = 0.\nRank is 1. System is not observable when measuring velocity."
  },
  {
    "objectID": "lectures/lecture10.html#luenberger-observer-design-steps",
    "href": "lectures/lecture10.html#luenberger-observer-design-steps",
    "title": "EGH445 Modern Control",
    "section": "Luenberger Observer: Design Steps",
    "text": "Luenberger Observer: Design Steps\n\n\nCheck Observability: Verify that \\text{rank}(\\mathcal{O}) = n.\nChoose Desired Observer Poles:\n\nSelect stable pole locations z_{1}, z_{2}, \\dots, z_{n} within the unit circle, |z_{i}| &lt; 1.\nThese poles dictate the speed of convergence of the estimation error e(kT).\nTypically chosen 5-10 times faster (closer to the origin) than the desired controller poles.\n\nForm Desired Characteristic Polynomial: (z - z_{1})(z - z_{2}) \\cdots (z - z_{n}).\nDetermine Observer Gain L:\n\nFind L such that the characteristic polynomial of (G - L C) matches the desired characteristic polynomial. (place in MATLAB and control.place in Python)."
  },
  {
    "objectID": "lectures/lecture10.html#duality-in-design",
    "href": "lectures/lecture10.html#duality-in-design",
    "title": "EGH445 Modern Control",
    "section": "Duality in Design",
    "text": "Duality in Design\n\n\nState-feedback control problem\nFind K to place eigenvalues of (G - HK).\nThis requires (G, H) to be controllable.\n\nObserver design problem\nFind L to place eigenvalues of (G - L C).\nThis requires (G, C) to be observable.\n\n\n\n\n\n\n\nDuality Property\n\n\nThe eigenvalues of a matrix are the same as the eigenvalues of its transpose. \\quad \\text{eig}(G - L C) = \\text{eig}((G - L C)^\\intercal) = \\text{eig}(G^\\intercal - C^\\intercal L^\\intercal)\n\n\n\n\nThis means that designing a gain L to achieve desired poles for (G - L C) is dual to designing a gain K_{obs} to achieve the same desired poles for (G^\\intercal - C^\\intercal K_{obs}^\\intercal).\n\n\n\n\n\n\nTip\n\n\n\n\nControl Design:\nK = place(G, H, p)\n\nObserver Design:\nL = place(G', C', p)'"
  },
  {
    "objectID": "lectures/lecture10.html#closed-loop-dynamics",
    "href": "lectures/lecture10.html#closed-loop-dynamics",
    "title": "EGH445 Modern Control",
    "section": "Closed-Loop Dynamics",
    "text": "Closed-Loop Dynamics\n\nThe state variables are the actual system state x(kT) and the estimation error e(kT) = x(kT) - \\hat{x}(kT).\n\n\nx(kT+T) = Gx(kT) + Hu(kT)\ne(kT+T) = (G - L C)e(kT)\n\n\n\\begin{align*}\nu(kT) &= -K\\hat{x}(kT) \\\\&= -K(\\textcolor{blue}{x(kT) - e(kT)})\n\\end{align*}\n\n\n\n\n\nSubstitute u(kT) above:\n\n\\begin{align*}\nx(kT+T) &= Gx(kT) + H(-K(x(kT) - e(kT))) \\\\\n&= Gx(kT) - HKx(kT) + HKe(kT) \\\\\n&= \\textcolor{blue}{(G - HK)}x(kT) + HKe(kT)\n\\end{align*}\n\n\n\nCombining the dynamics for x(kT) and e(kT) in matrix form: \n\\begin{bmatrix} x(kT+T) \\\\ e(kT+T) \\end{bmatrix} = \\begin{bmatrix} \\textcolor{blue}{G-HK} & HK \\\\ 0 & \\textcolor{blue}{G-LC}  \\end{bmatrix} \\begin{bmatrix} x(kT) \\\\ e(kT) \\end{bmatrix}"
  },
  {
    "objectID": "lectures/lecture10.html#separation-principle",
    "href": "lectures/lecture10.html#separation-principle",
    "title": "EGH445 Modern Control",
    "section": "Separation Principle",
    "text": "Separation Principle\n\n\\begin{bmatrix} x(kT+T) \\\\ e(kT+T) \\end{bmatrix} = \\begin{bmatrix} \\textcolor{blue}{G-HK} & HK \\\\ 0 & \\textcolor{blue}{G-LC}  \\end{bmatrix} \\begin{bmatrix} x(kT) \\\\ e(kT) \\end{bmatrix}\n The closed-loop system matrix is block triangular. Therefore, the eigenvalues of the overall closed-loop system are the union of:\n\nThe eigenvalues of the state-feedback controller (eigenvalues of G-HK).\nThe eigenvalues of the observer error dynamics (eigenvalues of G-L C).\n\n\n\n\n\n\n\n\nCertainty Equivalence Principle\n\n\nThis means we can design the controller gain K and the observer gain L independently. The controller design determines the system response poles, and the observer design determines the error convergence poles."
  },
  {
    "objectID": "lectures/lecture10.html#example-observer-based-control",
    "href": "lectures/lecture10.html#example-observer-based-control",
    "title": "EGH445 Modern Control",
    "section": "Example: Observer-based Control",
    "text": "Example: Observer-based Control\nSystem: Vehicle from previous example. G = \\begin{bmatrix}\n    1.0000  &  0.0975 \\\\\n         0  &  0.9512\n\\end{bmatrix}, \\quad H = \\begin{bmatrix} 0.0049 \\\\ 0.0975 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}.\n\n\n\nController Design (Pole Placement), desired controller poles at \\{0.8, 0.7\\}:\n\nK = place(G, H, [0.8, 0.7]) \\rightarrow K = \\begin{bmatrix} 6.1512 & 4.3159 \\end{bmatrix}\n\n\n\nObserver Design (Pole Placement), desired observer poles (faster than controller): \\{0.2, 0.3\\}.\n\nL = place(G', C', [0.2, 0.3])' \\rightarrow L = \\begin{bmatrix}1.4418 & 4.7007\\end{bmatrix}^\\intercal"
  },
  {
    "objectID": "lectures/lecture10.html#section-2",
    "href": "lectures/lecture10.html#section-2",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Closed-Loop Simulation:\n\nImplement the observer and controller u(kT) = -K\\hat{x}(kT). The overall system state includes x and e.\nG_{cl} = \\begin{bmatrix} G-HK & HK \\\\ {0} & G-L C \\end{bmatrix}, \\quad x(0) = [1, 0]^T.\nInitial estimate error: e(0) = x(0) - \\hat{x}(0) = [1, 0]^T - [0, 0]^T = [1, 0]^T (assume observer starts at 0).\n\n\n\n% MATLAB code\nG = [1 0.0975; 0 0.9512]; % System matrix\nH = [0.0049; 0.0975]; % Input matrix\nC = [1 0]; % Output matrix\n\nK = place(G, H, [0.8, 0.7]); % Controller gain\nL = place(G', C', [0.2, 0.3])'; % Observer gain\n\nGcl = [G - H*K, H*K; zeros(2), G - L*C]; % Closed-loop system matrix\nx0 = [1; 0]; % Initial state\ne0 = [1; 0]; % Initial estimation error\n\nTs = 0.1; % Sampling time\nSys = ss(Gcl, zeros(4,1), eye(4), zeros(4,1), Ts); % State-space system\n\nt = 0:Ts:3; % Time vector\n[y, t] = initial(Sys, [x0; e0], t); % Simulate system response\n\n% Plot position state and estimation\nplot(t, y(:,1), t, y(:,1)-y(:,3), '--', LineWidth=4); \nxlabel('Time (s)');\nylabel('Position (m)');\nlegend(\"True\", 'Estimate');"
  },
  {
    "objectID": "lectures/lecture10.html#observer-based-control-with-reference-tracking",
    "href": "lectures/lecture10.html#observer-based-control-with-reference-tracking",
    "title": "EGH445 Modern Control",
    "section": "Observer-Based Control with Reference Tracking",
    "text": "Observer-Based Control with Reference Tracking\nFor non-zero setpoint tracking, y(kT) \\to r(kT), we can adapt the controller structure.\nUsing the certainty equivalence, we replace x with \\hat{x} in the setpoint regulation schemes discussed previously.\nUsing feedforward gain K_{r}: u(kT) = -K\\hat{x}(kT) + K_{r}r(kT)\nWhere K_{r} is chosen to ensure steady-state output matches r(kT).\n\n\n\n\n\n\n\nImportant\n\n\nFinding K_{r} now requires considering the combined controller-observer dynamics. Often, integral action is preferred for robustness."
  },
  {
    "objectID": "lectures/lecture10.html#integral-action-with-observers",
    "href": "lectures/lecture10.html#integral-action-with-observers",
    "title": "EGH445 Modern Control",
    "section": "Integral Action with Observers",
    "text": "Integral Action with Observers\nObservers can be used to estimate disturbances, which can then be used to effectively implement integral action.\n\nThe idea is to augment the observer with a state that estimates the disturbance, \\hat z(kT) = \\begin{bmatrix} \\hat x(kT), \\hat w(kT)\\end{bmatrix}^\\intercal.\n\n\n\n\n\n\\begin{align*}\n\\hat{z}(kT+T) &=\n\\begin{bmatrix}\nG & I \\\\ 0 & 0\n\\end{bmatrix} \\hat{z}(kT) +\n\\begin{bmatrix}\nH \\\\ 0 \\end{bmatrix} u(kT), \\\\\n\\hat y(kT) &= \\begin{bmatrix} C & 0 \\end{bmatrix} \\hat{z}(kT)\n\\end{align*}\n\n\n\n\n\n\n\n\nTip\n\n\nAlternatively, you can simply use \\hat x(kT) and dynamic extension (like before) to achieve integral action."
  },
  {
    "objectID": "lectures/lecture10.html#the-kalman-filter-kf",
    "href": "lectures/lecture10.html#the-kalman-filter-kf",
    "title": "EGH445 Modern Control",
    "section": "The Kalman Filter (KF)",
    "text": "The Kalman Filter (KF)\nAn optimal estimator for linear systems subject to Gaussian noise, which explicitly models:\n\nProcess Noise \\,w(kT): Uncertainty in the system dynamics model. Characterised by covariance Q.\nMeasurement Noise \\,v(kT): Uncertainty in the sensor measurements. Characterised by covariance R.\n\n\n\n\\begin{align*}\n{x}(kT+T) &= G{x}(kT) + H{u}(kT) + {w}(kT) \\\\\n{y}(kT) &= C{x}(kT) + {v}(kT)\n\\end{align*}\n\n\n\n\n\n\n\n\n\nKey Ideas\n\n\n\nRecursive: Uses previous estimate and current measurement.\nPredictor-Corrector Structure:\n\nPredict: Project state estimate and error covariance forward in time based on the model (G, H, Q).\nCorrect: Update the predicted estimate using the actual measurement y(kT), weighted by the Kalman Gain (K_k).\n\nOptimal Gain: K_k is calculated at each step to minimise the estimated error covariance P_k, considering the relative uncertainties (Q vs R). It optimally balances trusting the model prediction vs. trusting the noisy measurement."
  },
  {
    "objectID": "lectures/lecture10.html#how-the-kalman-filter-works-predict-correct",
    "href": "lectures/lecture10.html#how-the-kalman-filter-works-predict-correct",
    "title": "EGH445 Modern Control",
    "section": "How the Kalman Filter Works: Predict & Correct",
    "text": "How the Kalman Filter Works: Predict & Correct\n\n\n\n\nPrediction (Time Update):\nUses the system (G, H) and process noise covariance (Q).\nGoal: Predict \\hat{{x}}(kT) and error covariance P(kT) at kT based on previous estimate.\n\n\n\n\n\n\n\nPredict\n\n\n\n\\begin{align*}\n\\hat{{x}}^-(kT) &= G\\hat{{x}}(kT-T) + H{u}(kT-T) \\quad &\\text{(Predicted State)} \\\\\nP^-(kT) &= G P(kT-T) G^\\intercal + Q \\quad &\\text{(Predicted Covariance)}\n\\end{align*}\n\n\n\n\n\n\n\n\n\nUpdate (Measurement Update / Correction):\nUses the measurement model (C) and measurement noise covariance (R).\nGoal: Correct the predicted state and error covariance using the actual measurement {y}(kT).\n\n\n\n\n\n\n\nUpdate\n\n\n\n\\begin{align*}\nK_k &= P^-(kT) C^T (C P^-(kT) C^T + R)^{-1} \\quad &\\text{(Kalman Gain)} \\\\\n\\hat{{x}}(kT) &= \\hat{{x}}^-(kT) + K_k \\big({y}(kT) - C\\hat{{x}}(kT)^-\\big) \\quad &\\text{(Updated State)} \\\\\nP(kT) &= (I - K_k C) P^-(kT) \\quad &\\text{(Updated Covariance)}\n\\end{align*}\n\n\n\n\n\n\n\n\nCycle: The updated state \\hat{{x}}(kT) and covariance P(kT) become the inputs for the prediction step at the next time instant kT+T.\nKF finds the optimal gain based on noise statistics (Q, R), while Luenberger uses pole placement."
  },
  {
    "objectID": "lectures/lecture10.html#handling-nonlinearity-extended-kalman-filter-ekf",
    "href": "lectures/lecture10.html#handling-nonlinearity-extended-kalman-filter-ekf",
    "title": "EGH445 Modern Control",
    "section": "Handling Nonlinearity: Extended Kalman Filter (EKF)",
    "text": "Handling Nonlinearity: Extended Kalman Filter (EKF)\nWhat if the system dynamics or measurement model are nonlinear? \n\\begin{align*}\n{x}(kT+T) &= f({x}(kT), {u}(kT)) + {w}(kT) \\\\\n{y}(kT) &= h({x}(kT)) + {v}(kT)\n\\end{align*}\n \n\nApproach: Linearise the nonlinear functions f and h around the state estimate \\hat{{x}}(kT) at each time step. \n\\begin{align*}\nG_k = \\frac{\\partial f}{\\partial {x}} \\bigg|_{\\hat{{x}}(kT), {u}(kT)} \\qquad\nH_k = \\frac{\\partial h}{\\partial {x}} \\bigg|_{\\hat{{x}}(kT)}\n\\end{align*}\n\nApply the standard KF equations using these time-varying Jacobians (G_k, H_k).\n\n\n\n\n\n\n\n\nTip\n\n\n\nEKF is widely used for nonlinear estimation (e.g., navigation, target tracking).\nEKF does not guarantee optimality like the linear KF. The linearization introduces approximations.\nEKF can diverge if nonlinearities are severe or initial estimates are poor.\nEKF is more computationally intensive due to Jacobian calculations at each step.\nOther nonlinear filters exist (e.g., Unscented Kalman Filter - UKF, Particle Filter) that avoid direct linearization but are computationally more complex. EKF is often a good starting point for mildly nonlinear systems."
  },
  {
    "objectID": "2025/week10_workshop.html#title-slide",
    "href": "2025/week10_workshop.html#title-slide",
    "title": "EGH445 Modern Control",
    "section": "Title Slide",
    "text": "Title Slide\n\n\n\n\n\nModern Control\n\n\nWeek 10 Workshop\n\n\n\nDr Guilherme Froes Silva School of Electrical Engineering & Robotics Queensland University of Technology\n\n\nEGH445 - Modern Control\n\n\n\nConsultation: GP-S1111 Email: g.froessilva@qut.edu.au"
  },
  {
    "objectID": "2025/week10_workshop.html#discrete-time-control-design-3-lecture-content",
    "href": "2025/week10_workshop.html#discrete-time-control-design-3-lecture-content",
    "title": "EGH445 Modern Control",
    "section": "Discrete-Time Control Design 3: Lecture Content",
    "text": "Discrete-Time Control Design 3: Lecture Content\n\n\n\n\nOverview\n\nWhy we need state estimation.\nThe Luenberger Observer.\nObservability.\nObserver Design via Pole Placement.\nOutput Feedback.\nOptimal Estimation (Kalman Filter)."
  },
  {
    "objectID": "2025/week10_workshop.html#the-need-for-state-estimation",
    "href": "2025/week10_workshop.html#the-need-for-state-estimation",
    "title": "EGH445 Modern Control",
    "section": "The Need for State Estimation",
    "text": "The Need for State Estimation\nWhy can’t we just use the states x(kT) if our controllers u(kT) = -Kx(kT) depend on them?\n\n\nTheoretical Background\n\n\nFull State Unavailability:\n\nLack of sensors for certain states.\nCost or physical constraints of sensors.\n\nOutput Equation: We only have access to outputs: \ny(kT) = Cx(kT) + Du(kT)\n\nInverting C is Not a General Solution:\n\nC is often not square (more states than outputs).\n\n\n\nPractical Implications\n\n\nSensor Cost & Complexity: expensive or technically impossible.\n\nExample: Measuring the internal temperature of a large chemical reactor at multiple points.\n\nNoise: Measurements are often noisy; estimation can help filter the noise."
  },
  {
    "objectID": "2025/week10_workshop.html#how-to-estimate-states",
    "href": "2025/week10_workshop.html#how-to-estimate-states",
    "title": "EGH445 Modern Control",
    "section": "How to Estimate States?",
    "text": "How to Estimate States?\n\n\nTheoretical Background\nOpen-Loop Observer Issues:\n\nSimulating the system:\n\\hat{x}(kT+T) = G\\hat{x}(kT) + Hu(kT).\nError dynamics e(kT+T) = Ge(kT).\nIf G is unstable, error diverges.\nIf G is stable but slow, error converges too slowly.\n\n\nPractical Implications\n\nOpen-loop estimators are unreliable:\n\nSensitive to initial conditions \\hat{x}(0) \\neq x(0).\nSensitive to model inaccuracies.\nCannot correct for disturbances."
  },
  {
    "objectID": "2025/week10_workshop.html#observer-error-dynamics-practical-design",
    "href": "2025/week10_workshop.html#observer-error-dynamics-practical-design",
    "title": "EGH445 Modern Control",
    "section": "Observer: Error Dynamics & Practical Design",
    "text": "Observer: Error Dynamics & Practical Design\nWith the Luenberger observer, how do we ensure \\hat{x}(kT) is useful?\n\n\nError Dynamics Recap\n\ne(kT) = x(kT) - \\hat{x}(kT).\nError dynamics: \ne(kT+T) = (G - LC)e(kT)\n\n\nDesign L so (G - LC):\n\nIs stable (poles inside unit circle).\nConverges fast.\n\n\n\nPractical Tips for Observer Poles (L Design)\n\n\nConvergence Speed:\n\nObserver dynamics faster than controller.\nWhy? Controller needs accurate estimates.\n\nNoise Sensitivity:\n\nFast convergence needs large L.\nL(y - C\\hat{x}) uses measured y.\nIf y is noisy, large L amplifies noise in \\hat{x}."
  },
  {
    "objectID": "2025/week10_workshop.html#output-feedback-control-using-the-estimate",
    "href": "2025/week10_workshop.html#output-feedback-control-using-the-estimate",
    "title": "EGH445 Modern Control",
    "section": "Output Feedback Control: Using the Estimate",
    "text": "Output Feedback Control: Using the Estimate\nWe’ve designed a controller K (assuming x(kT) was known) and an observer gain L (to get \\hat{x}(kT)).\n\n\nLuenberger State Observer: \n\\hat{x}(kT+T) = (G - LC)\\hat{x}(kT) + Hu(kT) + L y(kT)\n\nControl Law (Output Feedback): \nu(kT) = -K\\hat{x}(kT)\n\n\nKey Points:\n\nGain K designed first as if x(kT) was available.\nGain L designed so \\hat{x}(kT) converges to x(kT) quickly and robustly."
  },
  {
    "objectID": "lectures/lecture11.html#title-slide",
    "href": "lectures/lecture11.html#title-slide",
    "title": "EGH445 Modern Control",
    "section": "Title Slide",
    "text": "Title Slide\n\n\n\n\n\nPractical Considerations\n\n\nObservers and Output Feedback\n\n\n\nDr Guilherme Froes Silva School of Electrical Engineering & Robotics Queensland University of Technology\n\n\nEGH445 - Modern Control\n\n\n\nConsultation: GP-S1111 Email: g.froessilva@qut.edu.au"
  },
  {
    "objectID": "lectures/lecture11.html#state-vs-output-feedback-revisited",
    "href": "lectures/lecture11.html#state-vs-output-feedback-revisited",
    "title": "EGH445 Modern Control",
    "section": "State vs Output Feedback Revisited",
    "text": "State vs Output Feedback Revisited\nWhen and why? Consider the system x(kT+T) = Gx(kT) + Hu(kT), y(kT) = Cx(kT).\nSystem Matrix G = e^{AT} represents the internal dynamics of the system.\n\nIt describes how the state x(kT) evolves over time. Usually inherent to the system.\n\nInput Matrix H = \\int_0^T e^{A\\tau}B d\\tau represents how the input u(kT) affects the state x(kT).\n\nRelates to the actuator(s) chosen (e.g. types of motors, heaters, valves, etc.).\n\nOutput Matrix C represents how the states are measured as outputs y(kT).\n\nRelates to the sensor(s) chosen (e.g. types and placement of sensors, cameras, etc.)."
  },
  {
    "objectID": "lectures/lecture11.html#when-and-why",
    "href": "lectures/lecture11.html#when-and-why",
    "title": "EGH445 Modern Control",
    "section": "When and why?",
    "text": "When and why?\n\n\n\nState feedback: u(kT) = -Kx(kT)\n\nAssumes all states are available.\n\n\n\n\nOutput feedback: u(kT) = -K\\hat{x}(kT)\n\nUsed when only output y(kT) = Cx(kT) is available.\nRequires an observer to estimate the state \\hat{x}(kT) from y(kT) and u(kT)."
  },
  {
    "objectID": "lectures/lecture11.html#section",
    "href": "lectures/lecture11.html#section",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "State feedback: u(kT) = -Kx(kT)\n\n\nSystem (G,H) must be controllable.\nClosed-loop dynamics, G_{cl} = G - HK.\nAssumes all states are available.\n\nPole placement: K gain.\nLQR: K gain and cost function Q,R.\nIntegral action: Dynamic Extension and K_I gain.\n\n\n\nOutput feedback: u(kT) = -K\\hat{x}(kT)\n\n\nSystem (G,C) must be observable.\nObserver dynamics, G_{obs} = G - LC.\nUsed when only y(kT) = Cx(kT) is available.\n\ny(kT) is what is actually being measured.\n\nNeeds estimating \\hat{x}(kT) from y(kT) and u(kT).\n\nLuenberger Observer - L gain.\nKalman Filter - K_k gain and noise covariances.\n\n\n\n\nPractical Question: When is it feasible/necessary to measure all states versus estimate them?\n\nSensor cost and complexity (relates to choice of C).\nSensor availability (some states might be internal/unmeasurable)."
  },
  {
    "objectID": "lectures/lecture11.html#practical-trade-offs",
    "href": "lectures/lecture11.html#practical-trade-offs",
    "title": "EGH445 Modern Control",
    "section": "Practical Trade-offs",
    "text": "Practical Trade-offs\n\nObserver Performance:\n\nLuenberger poles determine error convergence speed. Faster poles \\rightarrow potentially higher gain L.\nKalman Filter dynamic gain K_k minimises the estimation error covariance iteratively, balancing model trust vs measurement trust based on noise characteristics (Q, R).\n\nNoise Amplification:\n\nLarge observer gains (L or K_k) can amplify measurement noise (v(kT)), negatively impacting the control input u(kT) = -K\\hat{x}(kT). There’s a trade-off: fast estimation vs noise sensitivity.\n\nComputational Load:\n\nObservers add computational cost (matrix multiplications, additions).\nIncrease in computational cost: Luenberger observers \\rightarrow Kalman filters \\rightarrow Extended Kalman filters (EKF).\nNeeds to be feasible on the target hardware, considering the selected/needed sampling time T."
  },
  {
    "objectID": "lectures/lecture11.html#section-1",
    "href": "lectures/lecture11.html#section-1",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Certainty Equivalence Principle:\n\nRecall: Controller (K) and Observer (L) can be designed independently. \nThis holds perfectly if the model is perfect.\nModel mismatch can affect performance;\n\nSometimes detuning the observer (slower poles) can improve robustness in practice.\n\n\nDiscussion Point: Do the benefits of state estimation outweigh the costs?\n\n\n\nBenefits of State Estimation:\n\n\nHandling unmeasured states\nBetter performance via full state feedback\n\n\nCosts of State Estimation:\n\n\nAdded complexity (and computational load)\nNeed of tuning\nSensitivity to noise\nModel mismatch\n\n\nDepends on the application!"
  },
  {
    "objectID": "lectures/lecture11.html#recap-lqr-fundamentals",
    "href": "lectures/lecture11.html#recap-lqr-fundamentals",
    "title": "EGH445 Modern Control",
    "section": "Recap: LQR Fundamentals",
    "text": "Recap: LQR Fundamentals\n\nGoal: Find u(kT) = -Kx(kT) that minimises a quadratic cost function over an infinite horizon.\nCost Function (J): Balances state regulation (Q, state weighting) and control effort (R, control weighting). \n  J = \\sum_{k=0}^{\\infty} \\left( x(kT)^T Q x(kT) + u(kT)^T R u(kT) \\right), \\quad Q \\succeq 0, \\quad R \\succ 0\n  \nSolution: Solve the Discrete Algebraic Riccati Equation (DARE) for a unique, positive semi-definite matrix P: \n  P = G^T P G - (G^T P H)(R + H^T P H)^{-1}(H^T P G) + Q\n   Then the gain is \\; K = (R + H^T P H)^{-1} H^T P G.\nStability: The closed-loop system x(kT+T) = (G-HK)x(kT) is guaranteed to be stable if (G, H) is controllable and (G, V) is observable, where Q=V^T V."
  },
  {
    "objectID": "lectures/lecture11.html#important-remark-on-gv-observability",
    "href": "lectures/lecture11.html#important-remark-on-gv-observability",
    "title": "EGH445 Modern Control",
    "section": "Important remark on (G,V) observability",
    "text": "Important remark on (G,V) observability\n\nThe matrix V (from Q=V^T V) defines which states or state combinations are penalised in the cost J.\n(G, V) observability ensures that any unstable behaviour in the system dynamics (G) will eventually affect the states penalised by Q, forcing the controller to act.\nAnalogy: C relates to what states are measured (physical sensors), while V relates to what states are penalised (design choice in Q). They don’t have to be the same."
  },
  {
    "objectID": "lectures/lecture11.html#important-remark-on-g-v-observability",
    "href": "lectures/lecture11.html#important-remark-on-g-v-observability",
    "title": "EGH445 Modern Control",
    "section": "Important remark on (G, V) observability",
    "text": "Important remark on (G, V) observability\n\nThe matrix V (from Q=V^T V) defines which states or state combinations are penalised in the cost J.\n(G, V) observability ensures that any unstable behaviour will eventually affect the states penalised by Q, forcing the controller to act.\n\n\n\n\n\n\n\nAnalogy\n\n\nC describes the outputs, i.e. what states are measured (through physical sensors).\nV describes what states are penalised (design choice in Q).\nAnalogously to designing controllers assuming full state availability, V and C do not need to be the same."
  },
  {
    "objectID": "lectures/lecture11.html#tuning-q-and-r-in-practice",
    "href": "lectures/lecture11.html#tuning-q-and-r-in-practice",
    "title": "EGH445 Modern Control",
    "section": "Tuning Q and R in practice",
    "text": "Tuning Q and R in practice\n\nStarting Point: Diagonal Matrices\n\nOften start with diagonal Q and R.\nQ = \\text{diag}(q_1, q_2, \\dots, q_n): Penalises squared state deviations q_i x_i^2.\nR = \\text{diag}(r_1, r_2, \\dots, r_m): Penalises squared control inputs r_j u_j^2.\n\nRelative Weights Matter:\n\nThe absolute values of Q and R don’t matter as much as their ratio. Scaling both Q and R by the same factor results in the same gain K.\nCommon practice: Fix one element (e.g., R=1 or R=I) and tune the elements of Q.\n\nInterpreting Weights:\n\nLarger q_i means state x_i is more important to regulate quickly/keep small.\nLarger r_j means control input u_j is more “expensive” (e.g., energy consumption, saturation limits)."
  },
  {
    "objectID": "lectures/lecture11.html#section-2",
    "href": "lectures/lecture11.html#section-2",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Tuning Strategies:\n\nBryson’s Rule (Rule of Thumb): Scale weights based on maximum acceptable deviations/inputs:\n\nq_{ii} \\approx \\frac{1}{(\\text{max acceptable } x_i)^2}, \\quad r_{jj} \\approx \\frac{1}{(\\text{max acceptable } u_j)^2}\n\nTrial and Error (Simulation/Experiment):\n\nSimulate/run the system.\nObserve states x(kT) and inputs u(kT).\nIf states converge too slowly \\rightarrow Increase elements in Q relative to R.\nIf control effort is too high/saturating \\rightarrow Increase elements in R relative to Q.\n\n\nNon-Diagonal Weights:\n\nOff-diagonal terms in Q can penalise relationships between states (e.g., x_1 x_2).\nOff-diagonal terms in R can penalise simultaneous use of multiple actuators.\nUse only if there’s a clear physical reason; increases tuning complexity."
  },
  {
    "objectID": "lectures/lecture11.html#lqr-limitations-revisited",
    "href": "lectures/lecture11.html#lqr-limitations-revisited",
    "title": "EGH445 Modern Control",
    "section": "LQR Limitations Revisited",
    "text": "LQR Limitations Revisited\n\nModel Accuracy:\n\nLQR performance relies on an accurate linear model (G, H).\nNonlinearities or parameter uncertainties can degrade performance or lead to instability.\n\nConstraint Handling:\n\nStandard LQR formulation does not explicitly handle constraints on states (x) or inputs (u).\nExample: Actuator saturation (u_{min} \\le u(kT) \\le u_{max}).\nCalculating gain K and then saturating the control value can lead to poor performance or instability. \n\nImplicit Pole Placement:\n\nLQR does place closed-loop poles, but indirectly through the choice of Q and R.\nIt’s hard to directly relate Q/R choices to transient response characteristics (like %OS, t_s).\n\nAddressing Limitations: \n\nFor constraints: Model Predictive Control (MPC) explicitly incorporates constraints in its optimization."
  },
  {
    "objectID": "lectures/lecture11.html#recap-kf-fundamentals",
    "href": "lectures/lecture11.html#recap-kf-fundamentals",
    "title": "EGH445 Modern Control",
    "section": "Recap: KF Fundamentals",
    "text": "Recap: KF Fundamentals\nOptimal state estimator for linear systems with Gaussian noise.\n\n\n\nSystem Model with Noise: \n  \\begin{align*}\n  {x}(kT+T) &= G{x}(kT) + H{u}(kT) + {w}(kT) \\\\\n  {y}(kT) &= C{x}(kT) + {v}(kT)\n  \\end{align*}\n  \n\n{w}(kT): Process noise, Q = E[{w}{w}^T]\n{v}(kT): Measurement noise, R = E[{v}{v}^T]\n\n\n\n\nRecursive Structure (Predict-Correct):\n\nPredict: Project state estimate \\hat{{x}} and error covariance P forward using the model (G, H, Q).\n\n\\hat{{x}}^-(kT) = G\\hat{{x}}(kT-T) + H{u}(kT-T)\nP^-(kT) = G P(kT-T) G^T + Q_{k-1}\n\nCorrect (Update): Adjust prediction using the current measurement {y}(kT) and the calculated Kalman Gain K_k.\n\nK_k = P^-(kT) C^T (C P^-(kT) C^T + R)^{-1}\n\\hat{{x}}(kT) = \\hat{{x}}^-(kT) + K_k ({y}(kT) - C\\hat{{x}}^-(kT))\nP(kT) = (I - K_k C) P^-(kT)\n\n\n\n\n\n\n\n\n\n\nKey Idea (watch Workshop 10 for a practical example)\n\n\nK_k optimally balances trusting the model prediction vs. trusting the noisy measurement based on the relative sizes of P^-(kT), Q, and R."
  },
  {
    "objectID": "lectures/lecture11.html#the-tuning-challenge",
    "href": "lectures/lecture11.html#the-tuning-challenge",
    "title": "EGH445 Modern Control",
    "section": "The Tuning Challenge",
    "text": "The Tuning Challenge\n\n“Optimal” performance relies on having the correct noise covariance matrices Q and R.\n\nR – sensor measurement uncertainty (variance).\nQ – dynamics model uncertainty (variance) introduced by, unmodelled forces, discretisation errors, etc.\n\nThe Problem: In most real-world applications, the true values of Q and R are unknown.\nConsequence: Choosing incorrect Q and R leads to suboptimal state estimates.\n\nIf the ratio Q/R is too small, the filter will trust the model too much and ignore useful measurements.\nEstimates converge slowly and may drift.\nIf the ratio Q/R is too large, the filter will trust the measurements too much and ignore the models.\nEstimates become noisy.\n\n\nTherefore: Q and R often become tuning parameters adjusted based on observed filter performance, rather than precisely known quantities."
  },
  {
    "objectID": "lectures/lecture11.html#tuning-strategies",
    "href": "lectures/lecture11.html#tuning-strategies",
    "title": "EGH445 Modern Control",
    "section": "Tuning Strategies",
    "text": "Tuning Strategies\n\n\nChoosing R:\n\nR \\in \\mathbb{R}^{p \\times p}, where p is the number of outputs.\nOften diagonal, when the sensor noise is uncorrelated.\n \\scriptsize\nR_k = \\begin{bmatrix} \\sigma_{v_1}^2 & 0 & \\dots & 0 \\\\ 0 & \\sigma_{v_2}^2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\sigma_{v_p}^2 \\end{bmatrix}\n\nDetermining values:\n\nSensor datasheets: specified noise characteristics.\nExperimental data: calculated variance from samples.\nCross-correlation: covariance between sensor signals.\n\n\nChoosing Q:\n\nQ \\in \\mathbb{R}^{n \\times n}, where n is the number of states.\nOften diagonal and treated as a tuning parameter.  \\scriptsize\nQ_k = \\begin{bmatrix} \\sigma_{w_1}^2 & 0 & \\dots & 0 \\\\ 0 & \\sigma_{w_2}^2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\sigma_{w_n}^2 \\end{bmatrix}\n\nDetermining values:\n\n\\sigma_{w_i}^2 reflects how much x_i is expected to deviate.\nPhysical insight: deviation due real dynamics.\nIterative tuning: observe residual e(kT)."
  },
  {
    "objectID": "lectures/lecture11.html#kalman-filter-tuning-residual-analysis",
    "href": "lectures/lecture11.html#kalman-filter-tuning-residual-analysis",
    "title": "EGH445 Modern Control",
    "section": "Kalman Filter Tuning: Residual Analysis",
    "text": "Kalman Filter Tuning: Residual Analysis\nThe residual is the difference between the actual measurement and the predicted measurement at each step: \n{e}(kT) = {y}(kT) - \\hat{{y}}^-(kT) = {y}(kT) - C\\hat{{x}}^-(kT)\n\n\nTheoretical Property: For a perfectly modelled and optimally tuned Kalman filter:\n\nZero-mean: E[{e}(kT)] = 0.\nWhite noise: Uncorrelated over time, E[{e}(kT){e}(jT)^T] = 0 for k \\neq j.\nKnown Covariance: E[{e}(kT){e}(kT)^T] = C P^-(kT) C^T + R_k.\n\nPractical Use for Tuning:\n\nCollect Residuals: Record {e}(kT) over a representative run.\nCheck Mean: If it’s significantly non-zero: bias in sensors or in the system model.\nCheck Whiteness (Autocorrelation): Plot the autocorrelation function (ACF) of the residuals.\n\n\nAdjust Q and R iteratively until the residuals “look like” zero-mean white noise with the expected covariance."
  },
  {
    "objectID": "lectures/lecture11.html#kalman-filter-tuning-residual-analysis-.smaller",
    "href": "lectures/lecture11.html#kalman-filter-tuning-residual-analysis-.smaller",
    "title": "EGH445 Modern Control",
    "section": "*Kalman Filter Tuning: Residual Analysis {.smaller}",
    "text": "*Kalman Filter Tuning: Residual Analysis {.smaller}\nThe residual is the difference between the actual measurement and the predicted measurement at each step: \n{e}(kT) = {y}(kT) - \\hat{{y}}^-(kT) = {y}(kT) - C\\hat{{x}}^-(kT)\n\n\nTheoretical Property: For a perfectly modelled and optimally tuned Kalman filter:\n\nZero-mean: E[{e}(kT)] = 0.\nWhite noise: Uncorrelated over time, E[{e}(kT){e}(jT)^T] = 0 for k \\neq j.\nKnown Covariance: E[{e}(kT){e}(kT)^T] = C P^-(kT) C^T + R_k.\n\nPractical Use for Tuning: Analyzing the actual residuals obtained from simulation or experiments is a powerful tuning diagnostic:\n\nCollect Residuals: Record {e}(kT) over a representative run.\nCheck Mean: Calculate the mean of the residuals. If it’s significantly non-zero, it might indicate:\n\nBias in sensors (fixable?).\nIncorrect system model (bias terms missing?).\nIncorrect Q_k or R_k.\n\nCheck Whiteness (Autocorrelation): Plot the autocorrelation function (ACF) of the residuals.\n\nIf residuals are correlated (ACF non-zero for lags &gt; 0), the filter is suboptimal.\nOften points to incorrect Q_k (filter dynamics don’t match real process noise) or R_k. Try adjusting Q_k or R_k.\n\nCheck Covariance: Compare the sample covariance of the residuals to the theoretical covariance C P^-(kT) C^T + R_k. Discrepancies suggest tuning issues. (Can be complex to use directly).\n\nTuning Goal: Adjust Q_k and R_k iteratively until the residuals “look like” zero-mean white noise with the expected covariance."
  },
  {
    "objectID": "lectures/lecture11.html#emulation-design-discretising-continuous-controllers",
    "href": "lectures/lecture11.html#emulation-design-discretising-continuous-controllers",
    "title": "EGH445 Modern Control",
    "section": "Emulation Design: Discretising Continuous Controllers",
    "text": "Emulation Design: Discretising Continuous Controllers"
  },
  {
    "objectID": "lectures/lecture11.html#emulation-discretising-continuous-controllers",
    "href": "lectures/lecture11.html#emulation-discretising-continuous-controllers",
    "title": "EGH445 Modern Control",
    "section": "Emulation: Discretising Continuous Controllers",
    "text": "Emulation: Discretising Continuous Controllers\nDesign a digital controller G_d(z) that mimics the behaviour of a pre-designed continuous controller G_c(s).\n\n\n\n\nWhy Emulate?\n\nLeverage existing continuous controller designs.\nControl design is sometimes preferred in s-domain.\nImplement advanced continuous controllers digitally.\n\n\n\n\n\n\n\n\nEmulation vs. Direct Design\n\n\n\nDirect design: Discretise the plant, then design the controller directly (e.g., using DLQR)\nEmulation: Design the controller in the continuous domain (e.g., PID, Lead-Lag, LQR) and then discretise it."
  },
  {
    "objectID": "lectures/lecture11.html#discretisation-methods",
    "href": "lectures/lecture11.html#discretisation-methods",
    "title": "EGH445 Modern Control",
    "section": "Discretisation Methods",
    "text": "Discretisation Methods\nHow do we convert a continuous controller G_c(s) into a discrete controller G_d(z)?\nThere are three main methods.\n1. Tustin (Bilinear) Transformation\nUses a trapezoidal integration to get to a bilinear relation between s and z (opposed to s=\\frac 1 T \\ln{(z)}).\n2. Zero-Order Hold (ZOH) Equivalence\nAssumes the input to the controller e(t) is held constant between samples by a ZOH \ne(kT+\\tau) = e(kT) \\text{ for } 0 \\le \\tau &lt; T\n\n3. Matched Pole-Zero (MPZ) Method\nMaps poles and zeros of G_c(s), in the s-plane, to those of G_d(z), in the z-plane, using z = e^{sT}."
  },
  {
    "objectID": "lectures/lecture11.html#method-1-tustin-bilinear-transform",
    "href": "lectures/lecture11.html#method-1-tustin-bilinear-transform",
    "title": "EGH445 Modern Control",
    "section": "Method 1: Tustin (Bilinear) Transform",
    "text": "Method 1: Tustin (Bilinear) Transform\nApproximates the continuous-time integration operator 1/s with a discrete-time equivalent derived from the trapezoidal rule for numerical integration.\nu(kT) = \\int e(t) dt \\quad \\rightarrow \\quad u(kT) \\approx u(kT-T) + \\frac{T}{2}(e(kT) + e(kT-T)).\nTaking the z-transform of both sides gives: \n\\begin{align*}\nU(z) &= z^{-1}U(z) + \\frac{T}{2} \\left( E(z) + z^{-1} E(z) \\right) \\\\\n\\frac{U(z)}{E(z)} &= \\frac{T}{2} \\frac{\\left( 1 + z^{-1} \\right)}{\\left(1 - z^{-1}\\right)} \\\\\n\\end{align*}\n\n\nTransformation Rule: To convert G_c(s) to G_d(z), substitute: \n  s \\leftarrow \\frac{2}{T} \\frac{z-1}{z+1}\n   into G_c(s).\nMapping Properties:\n\nMaps the left-half of the s-plane entirely into the unit circle in the z-plane.\nMaps the j\\omega axis of the s-plane onto the unit circle |z|=1 in the z-plane.\nThis preserves stability: a stable G_c(s) will result in a stable G_D(z).\n\nFrequency Warping:\n\nThere is a nonlinear relationship between the continuous frequency \\omega and the discrete frequency \\Omega T: \\omega_d T = 2 \\arctan(\\frac{\\omega T}{2}).\nThis means the frequency response of G_D(z) can be a “warped” version of G_c(s)’s frequency response, especially at higher frequencies relative to the sampling rate.\nPre-warping: If matching the frequency response at a critical frequency \\omega_c is important, the s-domain controller can be “pre-warped” before Tustin transformation. (Usually not detailed in an introductory context unless critical).\n\nMATLAB function: c2d(Gc, T, 'tustin') or c2d(Gc, T, 'bilinear')."
  },
  {
    "objectID": "lectures/lecture11.html#tustin-bilinear-transform",
    "href": "lectures/lecture11.html#tustin-bilinear-transform",
    "title": "EGH445 Modern Control",
    "section": "Tustin (Bilinear) Transform",
    "text": "Tustin (Bilinear) Transform\nApproximates the continuous-time integration operator 1/s with a discrete-time equivalent derived from the trapezoidal rule for numerical integration.\nu(kT) = \\int e(t) dt \\quad \\rightarrow \\quad u(kT) \\approx u(kT-T) + \\frac{T}{2}(e(kT) + e(kT-T)).\nTaking the z-transform of both sides gives: \n\\begin{align*}\nU(z) &= z^{-1}U(z) + \\frac{T}{2} \\left( E(z) + z^{-1} E(z) \\right) \\\\\n\\frac{U(z)}{E(z)} &= \\frac{T}{2} \\frac{\\left( 1 + z^{-1} \\right)}{\\left(1 - z^{-1}\\right)} \\\\\n\\end{align*}\n\n\n\n\n\n\n\nTransformation Rule\n\n\nTo convert G_c(s) to G_d(z), substitute s in G_c(s) with: \n    s \\leftarrow \\frac{2}{T} \\frac{z-1}{z+1}, \\quad \\text{that is, } \\quad G_d(z) = G_c\\left(\\frac{2}{T} \\frac{z-1}{z+1}\\right)."
  },
  {
    "objectID": "lectures/lecture11.html#tustin-bilinear-transform-properties",
    "href": "lectures/lecture11.html#tustin-bilinear-transform-properties",
    "title": "EGH445 Modern Control",
    "section": "Tustin (Bilinear) Transform Properties",
    "text": "Tustin (Bilinear) Transform Properties\n\nMaps the left-half of the s-plane entirely into the unit circle in the z-plane.\nMaps the j\\omega axis of the s-plane onto the unit circle |z|=1 in the z-plane.\nThis preserves stability: a stable G_c(s) will result in a stable G_d(z).\n\n\nMATLAB: c2d(Gc, T, 'tustin').\nPython: scipy.signal.cont2discrete(Gc, dt=T, method='bilinear')."
  },
  {
    "objectID": "lectures/lecture11.html#zero-order-hold-zoh-equivalence-1",
    "href": "lectures/lecture11.html#zero-order-hold-zoh-equivalence-1",
    "title": "EGH445 Modern Control",
    "section": "Zero-Order Hold (ZOH) Equivalence",
    "text": "Zero-Order Hold (ZOH) Equivalence\nAssumes the input e(t) to the controller is held constant by a Zero-Order Hold (ZOH) for one sampling period T.\n\nThe sampler and ZOH are placed before the continuous controller G_c(s).\nThe continuous controller sees a staircase input e(t) = e(kT) for kT \\le t &lt; kT+T.\nThe result is z-transformed to get the discrete controller G_d(z).\n\nZOH Transfer Function: G_{zoh}(s) = \\frac{1}{s} (1 - e^{-sT}). Positive step at kT minus negative step at kT+T.\n\n\n\n\n\n\nTransformation Rule\n\n\nThe pulse transfer function G_d(z) of the discretised controller is given by:\n\n\n\nG_d(z) = \\mathcal{Z}\\left\\{ \\mathcal{L}^{-1}\\left[ \\frac{1-e^{-sT}}{s} G_c(s) \\right] \\right\\} = (1-z^{-1}) \\mathcal{Z}\\left\\{\\frac{G_c(s)}{s}\\right\\}"
  },
  {
    "objectID": "lectures/lecture11.html#zoh-equivalence-properties",
    "href": "lectures/lecture11.html#zoh-equivalence-properties",
    "title": "EGH445 Modern Control",
    "section": "ZOH Equivalence Properties",
    "text": "ZOH Equivalence Properties\n\nThe ZOH method exactly matches the response of the continuous and discrete systems at the sampling instants if the input to G_c(s) were indeed staircase (due to the ZOH).\nStability is preserved if G_c(s) is stable.\n\nMATLAB: c2d(Gc, T, 'zoh').\nPython: scipy.signal.cont2discrete(Gc, dt=T, method='zoh')."
  },
  {
    "objectID": "lectures/lecture11.html#method-3-matched-pole-zero-mpz",
    "href": "lectures/lecture11.html#method-3-matched-pole-zero-mpz",
    "title": "EGH445 Modern Control",
    "section": "Method 3: Matched Pole-Zero (MPZ)",
    "text": "Method 3: Matched Pole-Zero (MPZ)\n\nConcept: This method aims to preserve the locations of the poles and zeros of the continuous controller G_c(s) by mapping them directly to the z-plane using the fundamental relationship z = e^{sT}.\nThree-Step Process:\n\nMap Poles and Zeros:\n\nFor each pole p_i of G_c(s), the corresponding pole in G_d(z) is z_{p_i} = e^{p_i T}.\nFor each zero z_j of G_c(s), the corresponding zero in G_d(z) is z_{z_j} = e^{z_j T}.\nForm the basic structure: \n  G_d(z) = K_d \\frac{\\prod (z - z_{z_j})}{\\prod (z - z_{p_i})}\n  \n\nAdjust Numerator Order (Add Zeros at z=-1):\n\nIf G_c(s) is strictly proper (more poles than zeros), G_d(z) obtained from step 1 will also be strictly proper.\nTo match the phase characteristics or improve response, zeros at z=-1 (corresponding to the Nyquist frequency) are often added to the numerator until its order is equal to or one less than the denominator’s order. This means multiplying the numerator by terms like (z+1) or (1+z^{-1}).\nThis helps average/smooth the input, similar to Tustin’s trapezoidal rule effect.\n\nMatch DC Gain:\n\nAdjust the discrete gain K_d so that the DC gain of G_d(z) matches the DC gain of G_c(s).\nDC gain of G_c(s) is \\lim_{s \\to 0} G_c(s).\nDC gain of G_d(z) is \\lim_{z \\to 1} G_d(z).\nSet these equal and solve for K_d: \n  \\lim_{z \\to 1} K_d \\frac{\\prod (z - z_{z_j})}{\\prod (z - z_{p_i})} (\\text{with any added } (z+1) \\text{ terms}) = \\lim_{s \\to 0} G_c(s)\n  \n\n\nMATLAB function: c2d(Gc, T, 'matched') or c2d(Gc, T, 'mpz')."
  },
  {
    "objectID": "lectures/lecture11.html#matched-pole-zero-mpz-method-1",
    "href": "lectures/lecture11.html#matched-pole-zero-mpz-method-1",
    "title": "EGH445 Modern Control",
    "section": "Matched Pole-Zero (MPZ) Method",
    "text": "Matched Pole-Zero (MPZ) Method\nPreserves the locations of poles/zeros by mapping them directly using the fundamental relationship z = e^{sT}.\n1. Map Poles and Zeros:\n\nMap poles and zeros of G_c(s) to poles and zeros of G_d(z) using the relationship z = e^{s T}.\nForm the basic structure: G_d(z) = K_d \\frac{\\prod (z - z_{z_j})}{\\prod (z - z_{p_i})}.\n\n2. Adjust Numerator Order (Add Zeros at z=-1):\n\nTo match phase characteristics, zeros at z=-1 (Nyquist frequency) are added to the numerator until its order is equal to or one less than the denominator’s order.\nThis helps average/smooth the input, similar to Tustin’s trapezoidal rule effect.\n\n3. Match DC Gain:\n\nAdjust K_d so that the DC gain of G_d(z) (at z=1) matches the DC gain of G_c(s) (at s=0)."
  },
  {
    "objectID": "lectures/lecture11.html#matched-pole-zero-method-properties",
    "href": "lectures/lecture11.html#matched-pole-zero-method-properties",
    "title": "EGH445 Modern Control",
    "section": "Matched Pole-Zero Method: Properties",
    "text": "Matched Pole-Zero Method: Properties\nStability Preservation:\n\nThe poles are mapped such that |z| = |e^{sT}| = e^{Re(s)T} &lt; 1, preserving the stability of G_c(s).\n\nFrequency Response:\n\nThe matching of DC gain ensures that the low-frequency response of G_d(z) is similar to G_c(s).\n\n\n\n\n\nComparison to Other Methods:\n\nTustin: Generally better overall frequency response approximation. \nZOH: Exact at sampling instants if the input is staircase. Can introduce phase lag.\nMPZ: Good for preserving specific pole/zero locations and DC gain. Often used when the locations of dominant poles/zeros are critical. \n\n\nMATLAB function: c2d(Gc, T, 'matched') or c2d(Gc, T, 'mpz').\nPython function: control.TransferFunction(num, den).sample(T, 'matched')."
  },
  {
    "objectID": "lectures/lecture11.html#example-discretising-a-continuous-lead-controller",
    "href": "lectures/lecture11.html#example-discretising-a-continuous-lead-controller",
    "title": "EGH445 Modern Control",
    "section": "Example: Discretising a Continuous Lead Controller",
    "text": "Example: Discretising a Continuous Lead Controller\n\nWe will use the example from the reference PDF to illustrate the discretisation methods.\nContinuous-Time System (Mass-Spring-Damper):\n\nState-space model: \n  \\begin{align*} \\dot{x} &= \\begin{bmatrix} 0 & 1 \\\\ -1 & -2 \\end{bmatrix} x(t) + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u(t) \\\\ y(t) &= \\begin{bmatrix} 1 & 0 \\end{bmatrix} x(t) \\end{align*}\n   (Parameters: m=1, b=2, c=1, output is displacement x_1).\n\nContinuous-Time Lead Controller G_c(s): \n  G_c(s) = K \\frac{s+a}{s+b} = 2.8 \\frac{s+2}{s+4}\n   This controller was pre-designed for the continuous system.\nTask: Find the equivalent discrete-time controller G_d(z) using:\n\nTustin (Bilinear) Transform\nZero-Order Hold (ZOH) Equivalence\nMatched Pole-Zero (MPZ) Method\n\nWe will consider two sampling times:\n\nPart A: T = 1.0 second\nPart B: T = 0.1 second And compare their performance when controlling the continuous plant.\n\n(Optional: Show a Simulink block diagram sketch of the closed-loop system with the digital controller and continuous plant, indicating Sampler and ZOH around the plant if using a discrete controller with a continuous plant model in Simulink, or around the digital controller if using a fully discrete simulation)."
  },
  {
    "objectID": "lectures/lecture11.html#example-discretising-a-continuous-controller",
    "href": "lectures/lecture11.html#example-discretising-a-continuous-controller",
    "title": "EGH445 Modern Control",
    "section": "Example: Discretising a Continuous Controller",
    "text": "Example: Discretising a Continuous Controller\n\n\nContinuous-Time System (Mass-Spring-Damper):\n\nState-space model: \n  \\begin{align*} \\dot{x} &= \\begin{bmatrix} 0 & 1 \\\\ -1 & -2 \\end{bmatrix} x(t) + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u(t) \\\\ y(t) &= \\begin{bmatrix} 1 & 0 \\end{bmatrix} x(t) \\end{align*}\n  \n\n\nContinuous-Time Lead Controller G_c(s): \n    G_c(s) = K \\frac{s+a}{s+b} = 2.8 \\frac{s+2}{s+4}\n     Pre-designed controller for the continuous system.\nTask: Find the discrete-time controller G_d(z)"
  },
  {
    "objectID": "lectures/lecture11.html#section-3",
    "href": "lectures/lecture11.html#section-3",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Task: Find the equivalent discrete-time controller G_d(z) using:\n\nTustin (Bilinear) Transform\nZero-Order Hold (ZOH) Equivalence\nMatched Pole-Zero (MPZ) Method\n\nWe will consider two sampling times:\n\nPart A: T = 1.0 second\nPart B: T = 0.1 second\n\n\nAnd compare their performance when controlling the continuous plant."
  },
  {
    "objectID": "lectures/lecture11.html#example-part-a-tustin-method-t1.0s",
    "href": "lectures/lecture11.html#example-part-a-tustin-method-t1.0s",
    "title": "EGH445 Modern Control",
    "section": "Example Part A: Tustin Method (T=1.0s)",
    "text": "Example Part A: Tustin Method (T=1.0s)\n\nContinuous Lead Controller: G_c(s) = 2.8 \\frac{s+2}{s+4}\nSampling Time: T=1.0 second.\nTustin Transformation Rule: Substitute s \\leftarrow \\frac{2}{T}\\frac{z-1}{z+1} = \\frac{2(z-1)}{z+1} (since T=1) into G_c(s).\n\n  \\begin{align*} G_d(z) &= 2.8 \\left( \\frac{ \\frac{2(z-1)}{z+1} + 2 }{ \\frac{2(z-1)}{z+1} + 4 } \\right) \\\\ &= 2.8 \\left( \\frac{ 2(z-1) + 2(z+1) }{ 2(z-1) + 4(z+1) } \\right) \\quad \\text{(Multiply num/den by }(z+1)\\text{)} \\\\ &= 2.8 \\left( \\frac{ 2z - 2 + 2z + 2 }{ 2z - 2 + 4z + 4 } \\right) \\\\ &= 2.8 \\left( \\frac{ 4z }{ 6z + 2 } \\right) = 2.8 \\left( \\frac{ 2z }{ 3z + 1 } \\right)= \\frac{5.6z}{3z+1} = \\frac{1.8667z}{z+0.3333} \\end{align*}"
  },
  {
    "objectID": "lectures/lecture11.html#slide-title",
    "href": "lectures/lecture11.html#slide-title",
    "title": "EGH445 Modern Control",
    "section": "Slide Title",
    "text": "Slide Title\n\nMATLAB Implementation:\n\n\n\n% MATLAB Code Snippet\nK_c = 2.8;\nGc_s = tf(K_c*[1 2], [1 4]);\nT = 1.0;\nGd_tustin = c2d(Gc_s, T, 'tustin');\ndisp('Gd_tustin (T=1.0s):');\ndisp(Gd_tustin);\n\n\n\n\nAnalysis (from PDF):\n\nThe PDF (page 17) shows a step response plot where this controller (F_C(Z)G(S) curve) is applied to the continuous plant. It notes “Poor/Undesirable Performance” for T=1.0s with Tustin.\n(Optional: Include a simplified version of the plot from PDF page 17 or your own generated plot showing this specific case.)"
  },
  {
    "objectID": "lectures/lecture11.html#section-4",
    "href": "lectures/lecture11.html#section-4",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "MATLAB Implementation\n\n\n% MATLAB Code Snippet\nK_c = 2.8;\nGc_s = tf(K_c*[1 2], [1 4]);\nT = 1.0;\nGd_tustin = c2d(Gc_s, T, 'tustin')\n\n   1.867 z\n  ----------\n  z + 0.3333\n\n\n\n\n\n\n\n\n\n\nStable\nPoor performance"
  },
  {
    "objectID": "lectures/lecture11.html#example-part-a-zoh-method-t1.0s",
    "href": "lectures/lecture11.html#example-part-a-zoh-method-t1.0s",
    "title": "EGH445 Modern Control",
    "section": "Example Part A: ZOH Method (T=1.0s)",
    "text": "Example Part A: ZOH Method (T=1.0s)\n\nContinuous Lead Controller: G_c(s) = 2.8 \\frac{s+2}{s+4}\nSampling Time: T=1.0 second.\nZOH Transformation Rule: G_d(z) = (1-z^{-1}) \\mathcal{Z}\\left\\{\\frac{G_c(s)}{s}\\right\\}\nStep 1: Find \\frac{G_c(s)}{s} \n  \\frac{G_c(s)}{s} = 2.8 \\frac{s+2}{s(s+4)}\n   Using partial fraction expansion: \n  \\frac{s+2}{s(s+4)} = \\frac{A}{s} + \\frac{B}{s+4} = \\frac{0.5}{s} + \\frac{0.5}{s+4}\n   So, \\frac{G_c(s)}{s} = 2.8 \\left( \\frac{0.5}{s} + \\frac{0.5}{s+4} \\right) = \\frac{1.4}{s} + \\frac{1.4}{s+4}."
  },
  {
    "objectID": "lectures/lecture11.html#section-5",
    "href": "lectures/lecture11.html#section-5",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Step 2: Find \\mathcal{Z}\\left\\{\\frac{G_c(s)}{s}\\right\\}\n\n\\mathcal{Z}\\left\\{\\frac{1.4}{s}\\right\\} = 1.4 \\frac{z}{z-1}\n\\mathcal{Z}\\left\\{\\frac{1.4}{s+4}\\right\\} = 1.4 \\frac{z}{z-e^{-4T}} = 1.4 \\frac{z}{z-e^{-4}} (since T=1)\nSo, \\mathcal{Z}\\left\\{\\frac{G_c(s)}{s}\\right\\} = 1.4 \\left( \\frac{z}{z-1} + \\frac{z}{z-e^{-4}} \\right).\n\nStep 3: Calculate G_d(z) \n  \\begin{align*} G_d(z) &= \\frac{z-1}{z} \\cdot 1.4 \\left( \\frac{z}{z-1} + \\frac{z}{z-e^{-4}} \\right) \\\\ &= 1.4 \\left( 1 + \\frac{z-1}{z-e^{-4}} \\right) = 1.4 \\left( \\frac{z-e^{-4} + z-1}{z-e^{-4}} \\right) \\\\ &= 1.4 \\frac{2z - (1+e^{-4})}{z-e^{-4}}\n  = \\frac{2.8z - 1.4256}{z - 0.0183}\n  \\end{align*}"
  },
  {
    "objectID": "lectures/lecture11.html#section-6",
    "href": "lectures/lecture11.html#section-6",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Marginally stable\nOscillatory behaviour"
  },
  {
    "objectID": "lectures/lecture11.html#example-part-a-mpz-method-t1.0s",
    "href": "lectures/lecture11.html#example-part-a-mpz-method-t1.0s",
    "title": "EGH445 Modern Control",
    "section": "Example Part A: MPZ Method (T=1.0s)",
    "text": "Example Part A: MPZ Method (T=1.0s)\n\nContinuous Lead Controller: G_c(s) = 2.8 \\frac{s+2}{s+4}\nSampling Time: T=1.0 second.\n\n\nMap Poles and Zeros:\n\nZero of G_c(s): s_z = -2 \\implies z_z = e^{-2T} = e^{-2} \\approx 0.1353.\nPole of G_c(s): s_p = -4 \\implies z_p = e^{-4T} = e^{-4} \\approx 0.0183.\nBasic structure: G_d(z) = K_d' \\frac{z - e^{-2}}{z - e^{-4}}.\n\nAdjust Numerator Order:\n\nThe numerator and denominator are already of the same order (both first order). No additional zeros at z=-1 are needed in this case."
  },
  {
    "objectID": "lectures/lecture11.html#section-7",
    "href": "lectures/lecture11.html#section-7",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Match DC Gain:\n\nDC gain of G_c(s): \\lim_{s \\to 0} 2.8 \\frac{s+2}{s+4} = 2.8 \\frac{2}{4} = 1.4.\nDC gain of G_d(z): \\lim_{z \\to 1} K_d' \\frac{z - e^{-2}}{z - e^{-4}} = K_d' \\frac{1 - e^{-2}}{1 - e^{-4}}.\nEquating: K_d' \\frac{1 - e^{-2}}{1 - e^{-4}} = 1.4.\nK_d' = 1.4 \\frac{1 - e^{-4}}{1 - e^{-2}} = 1.4 \\frac{1 - 0.018316}{1 - 0.135335} \\approx 1.5895.\nSo, G_d(z) = 1.5895 \\frac{z - 0.1353}{z - 0.0183} = \\frac{1.5895z - 0.2151}{z - 0.0183}."
  },
  {
    "objectID": "lectures/lecture11.html#section-8",
    "href": "lectures/lecture11.html#section-8",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "Stable\nPoor performance"
  },
  {
    "objectID": "lectures/lecture11.html#comparison-t1.0s-and-t0.1s",
    "href": "lectures/lecture11.html#comparison-t1.0s-and-t0.1s",
    "title": "EGH445 Modern Control",
    "section": "Comparison T=1.0s and T=0.1s",
    "text": "Comparison T=1.0s and T=0.1s\n\n\n\n\n\n\n\nThank you."
  },
  {
    "objectID": "2025/week11.html#student-voice-surveys",
    "href": "2025/week11.html#student-voice-surveys",
    "title": "EGH445 Modern Control",
    "section": "Student Voice Surveys",
    "text": "Student Voice Surveys"
  },
  {
    "objectID": "2025/week11.html#student-support",
    "href": "2025/week11.html#student-support",
    "title": "EGH445 Modern Control",
    "section": "Student Support",
    "text": "Student Support"
  },
  {
    "objectID": "2025/week11.html#student-voice-surveys-1",
    "href": "2025/week11.html#student-voice-surveys-1",
    "title": "EGH445 Modern Control",
    "section": "Student Voice Surveys",
    "text": "Student Voice Surveys"
  },
  {
    "objectID": "2025/week11.html#student-support-1",
    "href": "2025/week11.html#student-support-1",
    "title": "EGH445 Modern Control",
    "section": "Student Support",
    "text": "Student Support"
  },
  {
    "objectID": "2025/week11.html#student-support-2",
    "href": "2025/week11.html#student-support-2",
    "title": "EGH445 Modern Control",
    "section": "Student Support",
    "text": "Student Support"
  }
]