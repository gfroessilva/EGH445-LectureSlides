[
  {
    "objectID": "lectures/lecture9.html#title-slide",
    "href": "lectures/lecture9.html#title-slide",
    "title": "EGH445 Modern Control",
    "section": "Title Slide",
    "text": "Title Slide\n\n\n\n\n\nDiscrete-Time Control Design 3\n\n\nOptimal Control\n\n\n\nDr Guilherme Froes Silva School of Electrical Engineering & Robotics Queensland University of Technology\n\n\nEGH445 - Modern Control\n\n\n\nConsultation: GP-S1111 Email: g.froessilva@qut.edu.au"
  },
  {
    "objectID": "lectures/lecture9.html#quick-review-of-some-of-the-content-so-far",
    "href": "lectures/lecture9.html#quick-review-of-some-of-the-content-so-far",
    "title": "EGH445 Modern Control",
    "section": "Quick review of (some of) the content so far",
    "text": "Quick review of (some of) the content so far\n\nContinuous-time system: \n\\begin{align*}\\dot{x}(t) &= f(x, u), \\\\ y(t) &= g(x, u) \\end{align*}\n\n\n\nLinearised system: \n\\begin{align*}\n\\delta\\dot{x}(t) &= A\\delta x(t) + B \\delta u(t),  \\\\\n\\delta y(t) &= C\\delta x(t) + D\\delta u(t),\n\\end{align*}\n\nwhere \\delta x = x - \\bar x, \\delta u = u - \\bar u, \\delta y = y - \\bar y, and the A, B, C, D are the matrices \n\\begin{align*}\nA = \\frac{\\partial f}{\\partial x} \\bigg|_{\\bar x, \\bar u}\nB = \\frac{\\partial f}{\\partial u} \\bigg|_{\\bar x, \\bar u}\nC = \\frac{\\partial g}{\\partial x} \\bigg|_{\\bar x, \\bar u}\nD = \\frac{\\partial g}{\\partial u} \\bigg|_{\\bar x, \\bar u}\n\\end{align*}\n\n\n\nDiscrete-time system: \n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\\\ y(kT) &= Cx(kT) + Du(kT), \\end{align*}\n\nwhere \n\\begin{align*}\nG = e^{AT}, \\quad H = \\left[\\int_0^T e^{A\\tau} d\\tau\\right] B \\quad \\left(\\text{or, if $A$ is invertible, } H = A^{-1} (G-I)B\\right)\n\\end{align*}\n\n\n\nState-feedback controller: \n\\begin{align*}\nu(k) = -Kx(kT)\n\\end{align*}\n\nwhere K is the feedback gain matrix that is designed to arbitrarily move the poles of the closed-loop system \n\\begin{align*}\nx(kT+T) = (G-HK)x(kT)\n\\end{align*}\n\nfor example, by equating the characteristic polynomial of the closed-loop system to a desired polynomial, \n\\begin{align*}\n\\det(zI - (G-HK)) = (z - z_1)(z - z_2) \\cdots (z - z_n)\n\\end{align*}\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThe solution exists if the system is controllable, i.e. if the controllability matrix \\;\\mathcal{C} = \\left[H,\\; GH,\\; G^2H,\\; \\ldots,\\; G^{n-1}H\\right] has full rank (\\text{rank}(\\mathcal{C})=n).\n\n\n\n\n\nHow do you choose the poles?\n\n\n\nt_s - settling time\nt_r - rise time\n\\%OS - percent overshoot\n\\omega_n - natural frequency\n\\zeta - damping ratio (or through percent overshoot)\n\n\n\n\\zeta = \\dfrac{\\ln(\\%OS/100)}{\\sqrt{\\pi^2 + \\ln^2(\\%OS/100)}}\n\\omega_n = \\frac{4}{\\zeta t_s}\ns_{1,2} = -\\zeta \\omega_n \\pm j \\omega_n \\sqrt{1-\\zeta^2}\nz_{1,2} = e^{s_{1,2}T}\n\n\n\n\nWe also saw the Internal Model Principle (e.g. Integral Action) to reject disturbances (or follow references) with a known model (e.g. a step input, a ramp input, a sinusoidal input, etc.). Those still relied on the pole placement approach."
  },
  {
    "objectID": "lectures/lecture9.html#higher-order-systems",
    "href": "lectures/lecture9.html#higher-order-systems",
    "title": "EGH445 Modern Control",
    "section": "Higher-Order Systems",
    "text": "Higher-Order Systems\n\n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\\\ y(kT) &= Cx(kT) + Du(kT), \\end{align*}\n\nwhere \\quad x(kT) \\in \\mathbb{R}^n, \\quad u(kT) \\in \\mathbb{R}, \\quad y(kT) \\in \\mathbb{R}, \\quad n &gt; 3\n\n\n\n\n\n\n\nImportant\n\n\nThe link between pole locations and desired time-domain response becomes less clear. Arbitrary choices can lead to poor performance or excessive control effort.\nHow do you choose the best pole locations for a 5th, 10th, or higher-order system?"
  },
  {
    "objectID": "lectures/lecture9.html#mimo-systems",
    "href": "lectures/lecture9.html#mimo-systems",
    "title": "EGH445 Modern Control",
    "section": "MIMO Systems",
    "text": "MIMO Systems\nLet’s start with an example.\nConsider a simple MIMO system with two states and two inputs: \n\\begin{align*} x(kT+T) &= \\begin{bmatrix} 1 & 0.1 \\\\ 0 & 0.8 \\end{bmatrix} x(kT) + \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} u(kT)\n\\end{align*}\n\nWe want to place the poles at 0.4 and 0.6, by using u(kT) = -Kx(kT), where K = \\begin{bmatrix} k_1 & k_2 \\\\ k_3 & k_4\\end{bmatrix}.\n\nThe characteristic polynomial is given by: \n\\begin{align*}\n&\\det(zI - (G-HK)) = \\det\\left(\\begin{bmatrix} z-1+k_1 & k_2-0.1 \\\\ k_3 & z-0.8+k_4\\end{bmatrix}\\right) \\\\\n&= z^2 + (k_1 + k_4 - 1.8)z + (0.1k_3 - k_4 - 0.8k_1 + k_1k_4 - k_2k_3 + 0.8)\n\\end{align*}\n\n\n\nThe desired characteristic polynomial is given by: \n\\begin{align*}\n(z-0.4)(z-0.6) = z^2 - 1z + 0.24\n\\end{align*}\n\n\n\nWhen we equate the coefficients, we get: \n\\begin{align*}\n\\begin{cases}\n(k_1 + k_4 - 1.8) = -1 \\\\\n0.1k_3 - k_4 - 0.8k_1 + k_1k_4 - k_2k_3 + 0.8 = 0.24\n\\end{cases}\n\\end{align*}\n\n\n\n\n\n\n\n\n\nImportant\n\n\nNote that we have two equations and four unknowns. This means that we have degrees of freedom in the design. We can choose two of the four variables arbitrarily, and then solve for the other two.\n\n\n\n\n\nThis seems like a good idea, but it is not. The problem is that different choices of K, even if they lead to the same eigenvalues, can lead to different eigenvectors. The closed-loop system’s eigenvectors directly affect the response of the system.\n\n\nLet k_2 = k_3 = 0 and consider the following two cases of gains matrices K_1 and K_2, both of which lead assign the closed-loop eigenvalues to the desired values: \nK_1 = \\begin{bmatrix} 0.6 & 0 \\\\ 0 & 0.2 \\end{bmatrix}, \\quad K_2 = \\begin{bmatrix} 0.4 & 0 \\\\ 0 & 0.4 \\end{bmatrix}\n\nBoth of which lead to the same eigenvalues we wanted. But let’s simulate the system with both K_1 and K_2.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport control as ctrl\n# Create the system matrices\nG = np.array([[1, 0.1], [0, 0.8]])\nH = np.array([[1, 0], [0, 1]])\nC = np.array([[1, 0], [0, 1]]) # Outputs are the states\nD = np.array([[0, 0], [0, 0]])\nK1 = np.array([[0.6, 0], [0, 0.2]]) \nK2 = np.array([[0.4, 0], [0, 0.4]]) \nGcl1 = G - H @ K1 \nGcl2 = G - H @ K2\n\nTs = 0.1 # Sampling time (seconds)\n# Create the closed-loop LTI systems\nsys1 = ctrl.ss(Gcl1, H, C, D, Ts) \nsys2 = ctrl.ss(Gcl2, H, C, D, Ts) \n\n# Simulate for initial condition x0 = [1, 0]\nx0 = np.array([1, 0])\nt_end = 1.5 \nt = np.arange(0, t_end, Ts) \nt1, y1 = ctrl.initial_response(sys1, T=t, X0=x0)\nt2, y2 = ctrl.initial_response(sys2, T=t, X0=x0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(t1, y1[0, :], label='K1 - x1', linewidth=3)\nplt.plot(t1, y1[1, :], label='K1 - x2', linewidth=3)\nplt.plot(t2, y2[0, :], label='K2 - x1', linestyle='--', linewidth=3) \nplt.plot(t2, y2[1, :], label='K2 - x2', linestyle='--', linewidth=3) \nplt.title('Initial Condition Response ($x_0=[1, 0]^T$) with Different K Matrices', fontsize=16) \nplt.xlabel('Time (steps * dt)', fontsize=14) \nplt.ylabel('State Values', fontsize=14) \nplt.legend(fontsize=12)\nplt.grid(True) \nplt.show()"
  },
  {
    "objectID": "lectures/lecture9.html#mimo-systems-1",
    "href": "lectures/lecture9.html#mimo-systems-1",
    "title": "EGH445 Modern Control",
    "section": "MIMO Systems",
    "text": "MIMO Systems\n\n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\\\ y(kT) &= Cx(kT) + Du(kT), \\end{align*}\n\nx(kT) \\in \\mathbb{R}^n, \\quad u(kT) \\in \\mathbb{R}^m, \\quad y(kT) \\in \\mathbb{R}^p, \\quad n, m, p &gt; 1\n\n\n\n\n\n\n\nImportant\n\n\nFor MIMO systems, pole placement is significantly more complex. Specifying only the eigenvalues leaves degrees of freedom in the eigenvectors, which also affect the response. The design process becomes non-unique and less intuitive.\nHow do you systematically handle interactions between different inputs and outputs?"
  },
  {
    "objectID": "lectures/lecture9.html#performance-trade-off",
    "href": "lectures/lecture9.html#performance-trade-off",
    "title": "EGH445 Modern Control",
    "section": "Performance trade-off",
    "text": "Performance trade-off\nFinally, pole placement does not consider the control effort, not addressing the trade-off between:\n\nregulating the state, by making x(kT), or \\delta x(kT)1, small\n\n1 For linearised systems, we defined \\delta x = x - \\bar x.\n\nregulating required control effort u(kT).\n\n\n\n\n\n\n\n\nImportant\n\n\nYou might achieve desired poles but with impractically large control signals.\nWhich, like in the following example, can lead to instability."
  },
  {
    "objectID": "lectures/lecture9.html#linear-controller-failure-for-a-nonlinear-system",
    "href": "lectures/lecture9.html#linear-controller-failure-for-a-nonlinear-system",
    "title": "EGH445 Modern Control",
    "section": "Linear Controller Failure for a Nonlinear System",
    "text": "Linear Controller Failure for a Nonlinear System\nConsider a scalar system with cubic nonlinearity, \\; \\dot{x}(t) = -x(t) + x(t)^3 + u(t).\n\n\n-x represents a stabilizing linear dynamic.\n\n\n\n\nx^3 is a destabilizing nonlinearity that becomes significant for larger values of |x|.\n\n\n\nWe want to design a controller u(kT) to stabilize the system at \\bar x = 0 (requiring \\bar u = 0).\n\n\n1. Linearise the system around the equilibrium point (\\bar x=0, \\bar u=0):\n\n\n\nA = \\frac{\\partial}{\\partial x}(-x + x^3 + u) \\Big|_{x=0, u=0} = (-1 + 3x^2)\\Big|_{x=0} = -1\n\n\n\n\nB = \\frac{\\partial}{\\partial u}(-x + x^3 + u) \\Big|_{x=0, u=0} = 1\n\n\n\nThe linearised continuous-time system is \\;\\boxed{\\delta\\dot{x}(t) = -\\delta x(t) + \\delta u(t)}.\n\n\n2. Discretise the system with a sampling time T=0.1s: \n\\begin{align*} x(kT+T) &= Gx(kT) + Hu(kT), \\quad y(kT) = x(kT) \\end{align*}\n\nwhere G = e^{-AT} = 0.905, and H = A^{-1}(G-I)B = 0.095.\nThe discrete-time (linearised) system is \\boxed{x(kT+T) = 0.905x(kT) + 0.095u(kT)}.\n\n\n3. Design u(kT) = -K x(kT) to place the closed-loop pole of this linearised system at p = 0.5.\n3.1 Design through pole placement. The closed-loop is x(kT+T) = (G - H K) x(kT).\nWe want the closed-loop pole to be equal to p=0.5:\n\n\n\nTry to do this through equating the characteristic polynomials: \\det(zI - (G-HK)) = (z-p).\n\n\n\n\n\\begin{align*} 0.905 - 0.095 K &= 0.5 \\\\ 0.095 K &= 0.905 - 0.5 = 0.405 \\\\ K &= \\frac{0.405}{0.095} = 4.263 \\end{align*}\n\nSo, the linear controller designed for the linearized system is \\boxed{u(kT) = -4.263 x(kT)}.\n\n\n4. Test the controller on the original nonlinear system.\nSimulate the continuous nonlinear dynamics, but apply the control input u(kT) = -K x(kT) constant over the period [kT, kT + T) (Zero-Order Hold).\n\n\n\n\n\n\nImportant\n\n\nNote that the controller was derived for the linearised system, which ignores the x^3 term. The linear model is only accurate for small x (close to the equilibrium x=0). When |x| becomes large, the x^3 term can dominate, and the linear controller may fail.\n\n\n\n\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\n# Set default font size for plots\nplt.rcParams['font.size'] = 14 \n\n# 1. Nonlinear System Dynamics\ndef nonlinear_system(x, u):\n    # Scalar system dynamics: dx/dt = -x + x^3 + u\n    return -x + x**3 + u\n\n# 2. Linear Controller Parameters (derived above)\nAd = 0.905\nBd = 0.095\nK = 4.263\nTs = 0.1\nx_eq = 0.0 # Equilibrium state\n\n# 3. Simulation Setup\nt_start = 0\nt_end = 1.0 # Shorter simulation time might be enough\nn_steps = int(t_end / Ts)\n\n# Initial Conditions to compare\nx0_small = 2\nx0_large = 2.4\n\n# Function to run the simulation for a given x0\ndef run_simulation_smooth(x0_val, points_per_interval=10):\n    # Store results for smooth plotting\n    t_history_smooth = [t_start]\n    x_history_smooth = [x0_val]\n    # Store results at discrete intervals for markers and control calc\n    t_history_discrete = [t_start]\n    x_history_discrete = [x0_val]\n    u_history = [] # Control applied over the interval starting at t_history_discrete\n\n    current_t = t_start\n    current_x_start_of_interval = np.array([x0_val]) # State at beginning of interval\n\n    # print(f\"\\nRunning smooth simulation for x(0) = {x0_val}...\")\n    for k in range(n_steps):\n        # Calculate discrete control based on state at START of interval\n        x_deviation = current_x_start_of_interval[0] - x_eq\n        u_k = -K * x_deviation\n        # Optional: Limit control effort\n        # u_k = np.clip(u_k, -u_max, u_max)\n        u_history.append(u_k)\n\n        # Define ODE function for this interval (constant u_k)\n        def ode_interval(t, y): # y is a 1-element array\n            return np.array([nonlinear_system(y[0], u_k)])\n\n        # Simulate one interval Ts, evaluating at multiple points\n        t_eval_interval = np.linspace(current_t, current_t + Ts, points_per_interval, endpoint=True)\n        sol_interval = solve_ivp(\n            ode_interval,\n            (current_t, current_t + Ts), # t_span for the interval\n            current_x_start_of_interval, # Initial state for interval\n            method='RK45',\n            t_eval=t_eval_interval # Evaluate at these points\n        )\n\n        if sol_interval.status != 0:\n            print(f\"Simulation failed at step {k} (t={current_t:.2f}) for x(0)={x0_val}\")\n            # Pad history if needed\n            # ... (padding logic can be added if needed) ...\n            break\n\n        # Update state for next interval START\n        current_x_start_of_interval = sol_interval.y[:, -1] # State at the end\n        current_t += Ts\n\n        # Store history\n        # Append points *after* the first one to avoid duplication\n        t_history_smooth.extend(sol_interval.t[1:])\n        x_history_smooth.extend(sol_interval.y[0, 1:])\n        # Store discrete points\n        t_history_discrete.append(current_t)\n        x_history_discrete.append(current_x_start_of_interval[0])\n\n\n        # Stop if state diverges excessively\n        if abs(current_x_start_of_interval[0]) &gt; 10:\n            print(f\"State diverging at step {k} (t={current_t:.2f}) for x(0)={x0_val}. Stopping.\")\n            break\n\n    return (np.array(t_history_smooth), np.array(x_history_smooth), # Smooth results\n            np.array(t_history_discrete), np.array(x_history_discrete), # Discrete results\n            np.array(u_history)) # Control history\n\n# --- Run both simulations ---\n(t_small_smooth, x_small_smooth,\n t_small_discrete, x_small_discrete, u_small) = run_simulation_smooth(x0_small)\n\n(t_large_smooth, x_large_smooth,\n t_large_discrete, x_large_discrete, u_large) = run_simulation_smooth(x0_large)\n\n\n# --- MODIFIED Plotting Section ---\nplt.figure(figsize=(10, 8))\n\n# --- Plot states (Top Plot) ---\nplt.subplot(2, 1, 1)\n# Plot smooth trajectories\nplt.plot(t_small_smooth, x_small_smooth,\n         label=f'State x(t) (x0={x0_small})', linewidth=2)\nplt.plot(t_large_smooth, x_large_smooth,\n         label=f'State x(t) (x0={x0_large})', linewidth=2, linestyle='--')\n\nplt.axhline(0, color='black', linewidth=0.5, linestyle=':')\nplt.ylabel('State x')\nplt.title('Scalar Nonlinear System with Linear Controller (Pole @ 0.5)')\nplt.grid(True)\nplt.legend()\n# Adjust ylim automatically or set manually if needed\nylim_min = min(np.nanmin(x_small_smooth), np.nanmin(x_large_smooth), -1) - 0.5\nylim_max = max(np.nanmax(x_small_smooth), np.nanmax(x_large_smooth), 1) + 0.5\nplt.ylim(ylim_min, ylim_max)\n\n# --- Plot control inputs (Bottom Plot - using stairs) ---\nplt.subplot(2, 1, 2)\n# Ensure simulation ran successfully and generated history\nfinal_idx_small = len(u_small)\nif final_idx_small &gt; 0:\n    plt.stairs(u_small, t_small_discrete, label=f'Control u[k] (x0={x0_small})',\n               baseline=None, linewidth=2, linestyle='--')\n\nfinal_idx_large = len(u_large)\nif final_idx_large &gt; 0:\n    plt.stairs(u_large[:final_idx_large-1], t_large_discrete, label=f'Control u[k] (x0={x0_large})',\n               baseline=None, linewidth=2, linestyle='--')\n\n\nplt.ylabel('Control Input u')\nplt.xlabel('Time (s)')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nSimulation failed at step 2 (t=0.20) for x(0)=2.4"
  },
  {
    "objectID": "lectures/lecture9.html#the-discrete-time-optimal-control-problem",
    "href": "lectures/lecture9.html#the-discrete-time-optimal-control-problem",
    "title": "EGH445 Modern Control",
    "section": "The Discrete-Time Optimal Control Problem",
    "text": "The Discrete-Time Optimal Control Problem\nConsider the System Dynamics1, \\; x(kT+T) = Gx(kT) + Hu(kT), \\quad x(0) = x_0.\n\n1The general concept extends to nonlinear systems, but through methods that are beyond the scope of this course.\n\n\nWe want to find u(kT) = -Kx(kT) that minimizes the Cost Function / Performance Index2:\n\nJ = \\sum_{k=0}^{\\infty} \\left( x(kT)^T Q x(kT) + u(kT)^T R u(kT) \\right), \\quad Q\\succeq 0, \\quad R\\succ 0\n\n\n2 This is an infinite horizon cost function, where the state penalty matrix Q and the control penalty matrix R define the cost.\n\n\n\n\nThe solution to this problem is the Linear Quadratic Regulator (LQR).\n\n\n\n\n\n\n\nAdditional Reading\n\n\nFor more information about the general optimal control problem, see the additional readings on Canvas.\n\n\n\n\n\n\nA\\succeq 0 means that A is positive semi-definite, i.e. x^T A x \\geq 0 for all x \\in \\mathbb{R}^n.\nA\\succ 0 means that A is positive definite, i.e. x^T A x &gt; 0 for all x \\in \\mathbb{R}^n and x \\neq 0."
  },
  {
    "objectID": "lectures/lecture9.html#the-discrete-linear-quadratic-regulator-dlqr",
    "href": "lectures/lecture9.html#the-discrete-linear-quadratic-regulator-dlqr",
    "title": "EGH445 Modern Control",
    "section": "The Discrete Linear Quadratic Regulator (DLQR)",
    "text": "The Discrete Linear Quadratic Regulator (DLQR)\nThe gain K that minimizes the cost function1, subject to the system dynamics2 is given by:\n\nK = (R + H^\\intercal P H)^{-1} H^\\intercal P G,\n\n\nwhich depends on the unique, symmetric, positive semi-definite solution S of the Discrete Algebraic Riccati Equation (DARE): \nP = G^\\intercal PG - (G^\\intercal PH)(R+H^\\intercal PH)^{-1}(H^\\intercal PG) + Q,\n\n\nRemember that P must be unique, symmetric, positive semi-definite, that is P=P^\\intercal\\succeq 0.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe full derivation of the optimal control gain K formula is beyond the scope of the course. If you’re curious, start by investigating the Bellman’s Principle of Optimality (Dynamic Programming).\n\n\n\n\nJ = \\sum_{k=0}^{\\infty} \\left( x(kT)^T Q x(kT) + u(kT)^T R u(kT) \\right), \\quad Q\\succeq 0, \\quad R\\succ 0x(kT+T) = Gx(kT) + Hu(kT), \\quad x(0) = x_0"
  },
  {
    "objectID": "lectures/lecture9.html#design-of-dlqr-controllers",
    "href": "lectures/lecture9.html#design-of-dlqr-controllers",
    "title": "EGH445 Modern Control",
    "section": "Design of DLQR Controllers",
    "text": "Design of DLQR Controllers\nThe design of DLQR controllers is based on the following steps:\n\n\nCheck controllability of x(KT+T) = Gx(kT) + Hu(kT).\nSelect the cost matrices Q\\succeq 0 and R\\succ 0.\n\nJ = \\sum_{k=0}^{\\infty} \\left( x(kT)^T Q x(kT) + u(kT)^T R u(kT) \\right)\n\nSolve the DARE to find the positive semi-definite symmetric P.\nCompute the gain matrix K = (R + H^\\intercal P H)^{-1} H^\\intercal P G.\nImplement the controller u(kT) = -Kx(kT).\nTest the controller performance and adjust Q and R as necessary."
  },
  {
    "objectID": "lectures/lecture9.html#existence-and-stability-of-the-solution",
    "href": "lectures/lecture9.html#existence-and-stability-of-the-solution",
    "title": "EGH445 Modern Control",
    "section": "Existence and Stability of the Solution",
    "text": "Existence and Stability of the Solution\nWe need to check if the solution to the DARE exists and is stable.\n\n\n\n\n\n\n\nQuestions\n\n\n\nDoes a unique solution to the DARE always exist?\nDoes the resulting controller guarantee stability?\n\n\n\n\n\n\nTurns out that we need the following conditions.\nConditions for Stabilizing DLQR Solution:\n\nControllability: Can the controller move all unstable poles of the system?\nObservability: Can all unstable models be observed by the cost function?"
  },
  {
    "objectID": "lectures/lecture9.html#the-dlqr-stability-theorem",
    "href": "lectures/lecture9.html#the-dlqr-stability-theorem",
    "title": "EGH445 Modern Control",
    "section": "The DLQR Stability Theorem",
    "text": "The DLQR Stability Theorem\n\n\n\n\n\n\nTheorem: DLQR Stability Theorem\n\n\nAssume R \\succ 0 and Q \\succeq 0. Let Q=V^TV (where V might not be unique).\nIf the following conditions hold:\n\n\nThe pair (G,H) is controllable.\nThe pair (G,V) is observable1.\n\n1Note that this is not the same as the observability of the system, as what’s important is observability of states by the cost function.\n\n\n\nThen:\n\nA unique solution P=P^\\intercal \\succeq 0 to the DARE exists.\nThe resulting controller closed-loop system is stable."
  },
  {
    "objectID": "lectures/lecture9.html#simple-example-of-dlqr-design",
    "href": "lectures/lecture9.html#simple-example-of-dlqr-design",
    "title": "EGH445 Modern Control",
    "section": "Simple Example of DLQR Design",
    "text": "Simple Example of DLQR Design\nLet’s consider a simple example of a DLQR design and solve it “by hand”. Consider again the scalar system with cubic nonlinearity and its discrete linearised version,\n\n\\dot{x}(t) = -x(t) + x(t)^3 + u(t) \\quad \\rightarrow \\quad x(kT+T) = 0.905x(kT) + 0.095u(kT)\n\n\nControllability: \\mathcal{C} = [H,\\; GH] = [0.095,\\; 0.086475] has full rank (\\text{rank}(\\mathcal{C})=1).\nCost Matrices: Select Q = 10.0 and R = 0.1.\nObservability: \\mathcal{O} = [V,\\; VG]^\\intercal, where V = \\sqrt{Q}. Then \\mathcal{O} = [3.162,\\; 2.857]^\\intercal, which has full rank.\nSolution P=P^\\intercal\\succeq 0 of the DARE: P - G^\\intercal P G + (G^\\intercal P H)(R + H^\\intercal P H)^{-1}(H^\\intercal P G) = Q \n\\begin{align*}\nP - G^2 P + \\frac{(G H P)^2}{R + H^2 P} &= Q \\\\\n(P - G^2 P)(R + H^2 P) + G^2 H^2 P^2 &= Q (R + H^2 P) \\\\\nP R + H^2 P^2 - G^2 P R - Q R - Q H^2 P &= 0 \\\\\n(H^2) P^2 + (R - G^2 R - Q H^2) P + (- Q R) &= 0 \\\\\n0.009 P^2 -0.0722 P - 1 &= 0 \\\\\nP = 15.2571 \\text{ or } P = -7.2624\n\\end{align*}\n\nPick the positive semi-definite solution: P = 15.2571.\nCompute the gain K = (R + H^\\intercal P H)^{-1} H^\\intercal P G = 5.519.\n\n\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are # Import DARE solver\nimport matplotlib.pyplot as plt\n\n# Set default font size for plots\nplt.rcParams['font.size'] = 14 \n\n# 1. Nonlinear System Dynamics\ndef nonlinear_system(x, u):\n    # Scalar system dynamics: dx/dt = -x + x^3 + u\n    return -x + x**3 + u\n\n# 2. Linear Controller Parameters (derived above)\nAd = 0.905\nBd = 0.095\nK = 4.263\nTs = 0.1\nx_eq = 0.0 # Equilibrium state\n\n# 3. Simulation Setup\nt_start = 0\nt_end = 1.0 # Shorter simulation time might be enough\nn_steps = int(t_end / Ts)\n\n# Initial Conditions to compare\nx0_small = 2\nx0_large = 2.4\n\n# --- DLQR Controller Design ---\nQd = 10.0  # State weight\nRd = 0.1  # Control weight\n\n# Solve Discrete Algebraic Riccati Equation (DARE)\n# Need to pass arguments as 2D arrays for the solver\nA_dare = np.array([[Ad]])\nB_dare = np.array([[Bd]])\nQ_dare = np.array([[Qd]])\nR_dare = np.array([[Rd]])\n\nS = solve_discrete_are(A_dare, B_dare, Q_dare, R_dare)\nS_scalar = S[0, 0] # Extract scalar value\n\n# Calculate DLQR gain Kd\n# Kd = (R + B'SB)^-1 B'SA\nKd_term1 = R_dare + B_dare.T @ S @ B_dare\nKd_term2 = B_dare.T @ S @ A_dare\nKd = np.linalg.inv(Kd_term1) @ Kd_term2\nK_lqr = Kd[0, 0] # Extract scalar gain\n\n# Check closed-loop stability for the linear system\nAcl_lqr = Ad - Bd * K_lqr\nprint(f\"DARE Solution S = {S_scalar:.3f} (for Q={Qd}, R={Rd})\")\nprint(f\"DLQR Gain K = {K_lqr:.3f}\")\nif abs(Acl_lqr) &lt; 1:\n    print(f\"Linear Closed-Loop Pole = {Acl_lqr:.3f}. (STABLE)\")\nelse:\n    print(f\"Linear Closed-Loop Pole = {Acl_lqr:.3f}. (UNSTABLE)\")\n# --- End DLQR Design ---\n\n# Function to run the simulation (same as before, just uses K_lqr)\ndef run_simulation(x0_val, K_gain):\n    t_history = [t_start]\n    x_history = [x0_val]\n    u_history = []\n    current_t = t_start\n    current_x = np.array([x0_val]) # State needs to be array\n\n    for k in range(n_steps):\n        x_deviation = current_x[0] - x_eq\n        u_k = -K_gain * x_deviation\n        u_history.append(u_k)\n\n        def ode_interval(t, y):\n            return np.array([nonlinear_system(y[0], u_k)])\n\n        t_interval = (current_t, current_t + Ts)\n        sol_interval = solve_ivp(\n            ode_interval, t_interval, current_x, method='RK45',\n            t_eval=[current_t + Ts]\n        )\n\n        if sol_interval.status != 0:\n            print(f\"Simulation failed at step {k} (t={current_t:.2f}) for x(0)={x0_val}\")\n            # Pad history...\n            failed_steps = n_steps - k\n            t_history.extend(np.linspace(current_t + Ts, t_end, failed_steps))\n            x_history.extend([np.nan] * failed_steps)\n            u_history.extend([np.nan] * failed_steps)\n            break\n\n        current_t += Ts\n        current_x = sol_interval.y[:, -1]\n        t_history.append(current_t)\n        x_history.append(current_x[0])\n\n        if abs(current_x[0]) &gt; 10: # Stop if diverging\n            print(f\"State diverging at step {k} (t={current_t:.2f}) for x(0)={x0_val}. Stopping.\")\n            failed_steps = n_steps - k -1\n            if failed_steps &gt; 0:\n                 t_history.extend(np.linspace(current_t + Ts, t_end, failed_steps))\n                 x_history.extend([np.nan] * failed_steps)\n            break\n\n    return np.array(t_history), np.array(x_history), np.array(u_history)\n\n# Run both simulations with DLQR gain\nt_small, x_small, u_small = run_simulation(x0_small, K_lqr)\nt_large, x_large, u_large = run_simulation(x0_large, K_lqr)\n\n# 4. Plotting Results\nplt.figure(figsize=(10, 8))\nplt.subplot(2, 1, 1)\nplt.plot(t_small, x_small, label=f'State x(t) (x0={x0_small})', linewidth=2)\nplt.plot(t_large, x_large, label=f'State x(t) (x0={x0_large})', linewidth=2, linestyle='--')\nplt.axhline(0, color='black', linewidth=0.5, linestyle=':')\nplt.ylabel('State x')\nplt.title(f'Scalar Nonlinear System with DLQR Controller (Q={Qd}, R={Rd})')\nplt.grid(True)\nplt.legend()\nplt.ylim(min(np.nanmin(x_small), np.nanmin(x_large), -1) - 0.5, max(np.nanmax(x_small), np.nanmax(x_large), 1) + 0.5)\n\nplt.subplot(2, 1, 2)\nfinal_idx_small = len(u_small)\nif final_idx_small &gt; 0:\n    # Use step plot for control signal visualization\n    plt.step(t_small[:final_idx_small], u_small[:final_idx_small], where='post',\n             label=f'Control u[k] (x0={x0_small})', linewidth=2)\n\nfinal_idx_large = len(u_large)\nif final_idx_large &gt; 0:\n    plt.step(t_large[:final_idx_large], u_large[:final_idx_large], where='post',\n             label=f'Control u[k] (x0={x0_large})', linewidth=2, linestyle='--')\n\nplt.ylabel('Control Input u')\nplt.xlabel('Time (s)')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nDARE Solution S = 15.257 (for Q=10.0, R=0.1)\nDLQR Gain K = 5.519\nLinear Closed-Loop Pole = 0.381. (STABLE)"
  },
  {
    "objectID": "lectures/lecture9.html#mimo-example-of-dlqr-design",
    "href": "lectures/lecture9.html#mimo-example-of-dlqr-design",
    "title": "EGH445 Modern Control",
    "section": "MIMO Example of DLQR Design",
    "text": "MIMO Example of DLQR Design\nConsider the MIMO system with two states and two inputs: \n\\begin{align*} x(kT+T) &= \\begin{bmatrix} 1 & 0.1 \\\\ 0 & 0.8 \\end{bmatrix} x(kT) + \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} u(kT) \\\\ y(kT) &= Cx(kT) + Du(kT) \\end{align*}\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.linalg as linalg\nimport control as ctrl\n# Create the system matrices\nG = np.array([[1, 0.1], [0, 0.8]])\nH = np.array([[1, 0], [0, 1]])\nC = np.array([[1, 0], [0, 1]]) # Outputs are the states\nD = np.array([[0, 0], [0, 0]])\n\n# Pole placement for MIMO system\nK1 = np.array([[0.6, 0], [0, 0.2]]) \nK2 = np.array([[0.4, 0], [0, 0.4]]) \n\n# QLQR design for MIMO system\nQ = np.array([[10, 0], [0, 10]]) # State weight matrix\nR = np.array([[0.1, 0], [0, 0.1]]) # Control weight matrix\n\n# Solve the Discrete Algebraic Riccati Equation (DARE)\nS = linalg.solve_discrete_are(G, H, Q, R) # S is the solution to the DARE\n# Calculate DLQR gain K\nK3 = np.linalg.inv(R + H.T @ S @ H) @ (H.T @ S @ G)\nprint(f\"K3 = [{K3[0, 0]:.3f}, {K3[0, 1]:.3f}; {K3[1, 0]:.3f}, {K3[1, 1]:.3f}]\")\nprint(f\"eig(G - H @ K3) = {np.linalg.eigvals(G - H @ K3)}\")\n\n# Compute the closed-loop system matrices\nGcl1 = G - H @ K1 \nGcl2 = G - H @ K2\nGcl3 = G - H @ K3\n\nTs = 0.1 # Sampling time (seconds)\n# Create the closed-loop LTI systems\nsys1 = ctrl.ss(Gcl1, H, C, D, Ts) \nsys2 = ctrl.ss(Gcl2, H, C, D, Ts) \nsys3 = ctrl.ss(Gcl3, H, C, D, Ts)\n\n# Simulate for initial condition x0 = [1, 0]\nx0 = np.array([1.5, 0.5])\nt_end = 1.5 \nt = np.arange(0, t_end, Ts) \nt1, y1 = ctrl.initial_response(sys1, T=t, X0=x0)\nt2, y2 = ctrl.initial_response(sys2, T=t, X0=x0)\nt3, y3 = ctrl.initial_response(sys3, T=t, X0=x0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(t1, y1[0, :], label='K1 - x1', linewidth=3)\nplt.plot(t1, y1[1, :], label='K1 - x2', linewidth=3)\nplt.plot(t2, y2[0, :], label='K2 - x1', linestyle='--', linewidth=3) \nplt.plot(t2, y2[1, :], label='K2 - x2', linestyle='--', linewidth=3) \nplt.plot(t3, y3[0, :], label='K3 - x1', linestyle=':', linewidth=3) \nplt.plot(t3, y3[1, :], label='K3 - x2', linestyle=':', linewidth=3) \nplt.title('Initial Condition Response ($x_0=[1, 0]^T$) with Different K Matrices', fontsize=16) \nplt.xlabel('Time (steps * dt)', fontsize=14) \nplt.ylabel('State Values', fontsize=14) \nplt.legend(fontsize=12)\nplt.grid(True) \nplt.show()\n\n\nK3 = [0.990, 0.099; 0.000, 0.792]\neig(G - H @ K3) = [0.00980006 0.0078745 ]"
  },
  {
    "objectID": "lectures/lecture9.html#tuning-q-and-r-the-trade-off",
    "href": "lectures/lecture9.html#tuning-q-and-r-the-trade-off",
    "title": "EGH445 Modern Control",
    "section": "Tuning Q and R: The Trade-off",
    "text": "Tuning Q and R: The Trade-off\n\nThe relative size (Q/R ratio) affects the trade-off between state regulation and control effort.\nIncreasing Q relative to R \\rightarrow increases the penalty on state deviation\n\nfaster state convergence,\nhigher control effort.\n\nIncreasing R relative to Q \\rightarrow increases the penalty on control effort\n\nslower state convergence,\nlower control effort."
  },
  {
    "objectID": "lectures/lecture9.html#quick-example-of-dlqr-tuning",
    "href": "lectures/lecture9.html#quick-example-of-dlqr-tuning",
    "title": "EGH445 Modern Control",
    "section": "Quick Example of DLQR Tuning",
    "text": "Quick Example of DLQR Tuning\n\n\nCode\n# Iterate over different Q values and simulate the system\nQ_values = [1, 10, 50, 100]  # Different state penalty values\nR = 0.1  # Fixed control penalty\nAd = 0.905\nBd = 0.095\nTs = 0.1\nx0 = 2  # Initial condition\n\n# Store results for plotting\nresults = []\n\nfor Q in Q_values:\n  # Solve DARE for each Q\n  S = solve_discrete_are(np.array([[Ad]]), np.array([[Bd]]), np.array([[Q]]), np.array([[R]]))\n  K = np.linalg.inv(R + Bd**2 * S) @ (Bd * S * Ad)\n  K = K[0, 0]  # Extract scalar gain\n\n  # Simulate the system\n  t_history = [0]\n  x_history = [x0]\n  current_x = x0\n  for k in range(50):  # Simulate for 50 steps\n    u = -K * current_x\n    current_x = Ad * current_x + Bd * u\n    t_history.append((k + 1) * Ts)\n    x_history.append(current_x)\n\n  results.append((Q, t_history, x_history))\n\n# Plot results\nplt.figure(figsize=(10, 6))\nfor Q, t_history, x_history in results:\n  plt.plot(t_history, x_history, label=f\"Q={Q}\", linewidth=2)\n\nplt.title(\"DLQR State Response for Different Q Values\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"State x\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/lecture9.html#limitations-of-lqr-control-saturation",
    "href": "lectures/lecture9.html#limitations-of-lqr-control-saturation",
    "title": "EGH445 Modern Control",
    "section": "Limitations of LQR: Control Saturation",
    "text": "Limitations of LQR: Control Saturation\n\nConsider the system \\; \\ddot x = x - c\\dot x + u, where u_{\\min} \\le u(kT) \\le u_{\\max}.\n\nUnsaturated LQRSaturated LQRMPC\n\n\n\n\n# Scenario 1: Unconstrained LQR for Nonzero Regulation (Self-Contained)\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are\nimport matplotlib.pyplot as plt\nimport time\n\n# print(\"--- Running Scenario 1: Unconstrained LQR (Nonzero Setpoint) ---\")\n\n# --- 1. System Definition ---\n# Stable continuous system: dx/dt = -x + u\na = -1.0\nb = 1.0\ndef system_ode(t, x, u, a_par, b_par):\n    # x is a 1-element array\n    return a_par * x[0] + b_par * u\n\n# --- 2. Discretization ---\nTs = 0.1 # Sampling time\nAc = np.array([[a]])\nBc = np.array([[b]])\nAd = np.array([[np.exp(a * Ts)]])\nBd = np.array([[(np.exp(a * Ts) - 1) / a * b]])\nn_states = 1\nm_inputs = 1\n# print(f\"Discrete System: Ad = {Ad[0,0]:.3f}, Bd = {Bd[0,0]:.3f}\")\n\n# --- 3. Setpoint ---\nx_ref = 2.0\nu_ref = (1 - Ad[0,0]) / Bd[0,0] * x_ref # Calculate for discrete system\n# print(f\"Setpoint: x_ref = {x_ref:.2f}, requires u_ref = {u_ref:.2f}\")\nx_eq_ref = np.array([x_ref]) # Target state for reference\n\n# --- 4. DLQR Design (for error regulation) ---\n# Weights for error cost: sum(Qd*xtilde^2 + Rd*utilde^2)\nQd = 10.0\nRd = 1.0 # Using less aggressive weights\nQ_dare = np.array([[Qd]])\nR_dare = np.array([[Rd]])\ntry:\n    S = solve_discrete_are(Ad, Bd, Q_dare, R_dare)\n    Kd_term1 = R_dare + Bd.T @ S @ Bd\n    Kd_term2 = Bd.T @ S @ Ad\n    Kd = np.linalg.inv(Kd_term1) @ Kd_term2\n    K_lqr = Kd[0, 0] # Scalar gain for error feedback\n    print(f\"DLQR Gain for error K = {K_lqr:.3f}\")\nexcept Exception as e:\n    print(f\"DLQR Design failed: {e}\"); exit()\n\n\nprint(f\"Control Limits: [-inf, inf]\")\n\n# --- 5. Simulation Setup ---\nt_start = 0\nt_end = 5.0\nn_steps = int(t_end / Ts)\nx0 = 0.0 # Initial condition (start from origin)\n\n# --- Helper function for interval simulation ---\ndef simulate_interval(ode_func, t_start, x_start, duration, control_val):\n    t_eval_interval = [t_start + duration]\n    sol_interval = solve_ivp(\n        ode_func, (t_start, t_start + duration), x_start, method='RK45',\n        t_eval=t_eval_interval, args=(control_val,)\n    )\n    if sol_interval.status != 0: return None, False\n    return sol_interval.y[:, -1], True\n\n# --- 6. Simulation Loop ---\nt_history = [t_start]\nx_history = [np.array([x0])] # Store as list of arrays\nu_history = []\ncurrent_t = t_start\ncurrent_x = np.array([x0])\nsuccess_run = True\n\nstart_sim_time = time.time()\n# print(\"Running simulation...\")\nfor k in range(n_steps):\n    x_error = current_x[0] - x_ref\n    # Control Law: u[k] = u_ref - K*(x[k]-x_ref)\n    u_k = u_ref - K_lqr * x_error\n    # NO SATURATION APPLIED HERE\n    u_history.append(u_k)\n\n    def ode_lqr_interval(t, y, u_val):\n        return system_ode(t, y, u_val, a, b)\n\n    next_x, success = simulate_interval(ode_lqr_interval, current_t, current_x, Ts, u_k)\n\n    if not success:\n        print(f\"ODE solver failed at t={current_t:.2f}\")\n        success_run = False; break\n\n    current_t += Ts\n    current_x = next_x\n    t_history.append(current_t)\n    x_history.append(current_x.copy()) # Append copy\n\n    if np.max(np.abs(current_x)) &gt; 100: # Divergence check\n        print(f\"State diverging excessively at t={current_t:.2f}. Stopping.\")\n        success_run = False; break\n\nprint(f\"Simulation loop time: {time.time() - start_sim_time:.2f} s\")\n\n# --- 7. Plotting ---\nif success_run:\n    t_history = np.array(t_history)\n    x_history = np.array(x_history).T[0,:] # Extract scalar state history\n    u_history = np.array(u_history)\n    n_plot = len(u_history) # Successful steps\n\n    # Define consistent plot limits\n    xlims = (t_start, t_end)\n    ylims_x = (-0.5, 2.5)\n    ylims_u = (-3.5, 5.5) # Wider range to potentially show large unconstrained values\n\n    plt.figure(\"Unconstrained LQR (Nonzero Setpoint)\", figsize=(10, 8))\n    plt.rcParams.update({'font.size': 12})\n\n    plt.subplot(2, 1, 1)\n    plt.plot(t_history[:n_plot+1], x_history[:n_plot+1], label='State x(t)', lw=2, marker='.')\n    plt.axhline(x_ref, color='k', linestyle=':', label=f'Setpoint x={x_ref}')\n    plt.ylabel('State x')\n    plt.title(f'Unconstrained LQR (Setpoint x={x_ref})')\n    plt.ylim(ylims_x); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(2, 1, 2)\n    plt.step(t_history[:n_plot], u_history[:n_plot], where='post', label='Control u[k]', lw=2)\n    plt.axhline(u_ref, color='k', linestyle=':', label=f'Steady-State u={u_ref:.2f}')\n    plt.ylabel('Control Input u')\n    plt.xlabel('Time (s)')\n    plt.ylim(ylims_u); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Simulation failed, not plotting.\")\n\n\nDLQR Gain for error K = 1.956\nControl Limits: [-inf, inf]\nSimulation loop time: 0.02 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Scenario 2: Saturated LQR for Nonzero Regulation (Damped 2nd Order System)\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are, expm\nfrom scipy.signal import cont2discrete\nimport matplotlib.pyplot as plt\nimport time\n\n# print(\"--- Running Scenario 2: Saturated LQR (Nonzero Setpoint, Damped System) ---\")\n\n# --- 1. System Definition ---\nc_damping = 0.5\nAc = np.array([[0., 1.], [1., -c_damping]])\nBc = np.array([[0.], [1.]])\nn_states = Ac.shape[0]\nm_inputs = Bc.shape[1]\ndef system_ode(t, x, u, Ac_mat, Bc_mat):\n    u_val = u if np.isscalar(u) else u[0]\n    dxdt = Ac_mat @ x + Bc_mat @ np.array([u_val])\n    return dxdt.flatten()\n\n# --- 2. Discretization ---\nTs = 0.1\nAd, Bd, _, _, _ = cont2discrete((Ac, Bc, np.eye(n_states), np.zeros((n_states, m_inputs))), Ts, method='zoh')\n\n# --- 3. Setpoint ---\nx_ref = np.array([1.0, 0.0])\ntry:\n    u_ref_vec = np.linalg.pinv(Bd) @ (np.eye(n_states) - Ad) @ x_ref\n    u_ref = u_ref_vec[0]\n    # print(f\"Setpoint: x_ref = {x_ref.T}, requires discrete u_ref = {u_ref:.3f} (cont. u_ref=-1.0)\")\nexcept Exception as e:\n    print(f\"Could not calculate discrete u_ref: {e}. Using continuous u_ref.\")\n    u_ref = -x_ref[0]\n\n# --- 4. DLQR Design (for error regulation) ---\nQd = np.diag([1.0, 0.1])\nRd = np.array([[0.01]])\ntry:\n    S = solve_discrete_are(Ad, Bd, Qd, Rd)\n    Kd_term1 = Rd + Bd.T @ S @ Bd\n    Kd_term2 = Bd.T @ S @ Ad\n    Kd = np.linalg.inv(Kd_term1) @ Kd_term2\n    print(f\"DLQR Gain for error K = {Kd}\")\nexcept Exception as e:\n    print(f\"DLQR Design failed: {e}\"); exit()\n\n# --- 5. Simulation Setup ---\nt_start = 0\nt_end = 6.0\nn_steps = int(t_end / Ts)\nx0 = np.array([0.0, 0.0])\nx_eq_ref = np.array([x_ref]) # Use as reference, not equilibrium for deviation calc\nu_min = -2.5 # Saturation Limits (Allow u_ref=-1)\nu_max = 0.5\nprint(f\"Control Limits: [{u_min}, {u_max}]\")\n\n# --- Helper function for interval simulation ---\ndef simulate_interval(ode_func, t_start, x_start, duration, control_val):\n    t_eval_interval = [t_start + duration]\n    sol_interval = solve_ivp(\n        ode_func, (t_start, t_start + duration), x_start, method='RK45',\n        t_eval=t_eval_interval, args=(control_val,)\n    )\n    if sol_interval.status != 0: return None, False\n    return sol_interval.y[:, -1], True\n\n# --- 6. Simulation Loop ---\nt_history = [t_start]\nx_history = [x0]\nu_lqr_desired_hist = []\nu_sat_applied_hist = []\ncurrent_t = t_start\ncurrent_x = x0.copy()\nsuccess_run = True\n\nstart_sim_time = time.time()\n# print(\"Running simulation...\")\nfor k in range(n_steps):\n    x_error = current_x - x_ref\n    # Calculate desired LQR control\n    u_lqr_k_vec = u_ref - Kd @ x_error\n    u_lqr_k = u_lqr_k_vec[0]\n    u_lqr_desired_hist.append(u_lqr_k)\n\n    # Apply saturation\n    u_sat_k = np.clip(u_lqr_k, u_min, u_max)\n    u_sat_applied_hist.append(u_sat_k)\n\n    def ode_satlqr_interval(t, y, u_val):\n        return system_ode(t, y, u_val, Ac, Bc)\n\n    next_x, success = simulate_interval(ode_satlqr_interval, current_t, current_x, Ts, u_sat_k)\n\n    if not success:\n        print(f\"ODE solver failed at t={current_t:.2f}\")\n        success_run = False; break\n\n    current_t += Ts\n    current_x = next_x\n    t_history.append(current_t)\n    x_history.append(current_x.copy())\n\n    if np.max(np.abs(current_x)) &gt; 100: # Divergence check\n        print(f\"State diverging excessively at t={current_t:.2f}. Stopping.\")\n        success_run = False; break\n\nprint(f\"Simulation loop time: {time.time() - start_sim_time:.2f} s\")\n\n# --- 7. Plotting ---\nif success_run:\n    t_history = np.array(t_history)\n    x_history = np.array(x_history).T\n    u_lqr_desired_hist = np.array(u_lqr_desired_hist)\n    u_sat_applied_hist = np.array(u_sat_applied_hist)\n    n_plot = len(u_sat_applied_hist)\n\n    # Use consistent plot limits from Block 1 (or adjust if needed)\n    xlims = (t_start, t_end)\n    ylims_x1 = (-0.5, 1.5)\n    ylims_x2 = (-0.5, 1.5)\n    ylims_u = (-4, 1) # Keep wide limits to see desired vs saturated\n\n    plt.figure(\"Saturated LQR (Nonzero Setpoint, Damped)\", figsize=(10, 9))\n    plt.rcParams.update({'font.size': 12})\n\n    plt.subplot(3, 1, 1)\n    plt.plot(t_history[:n_plot+1], x_history[0,:n_plot+1], label='$x_1$ (y)', lw=2)\n    plt.axhline(x_ref[0], color='k', linestyle=':', label=f'Setpoint $x_1={x_ref[0]}$')\n    plt.title(f'Saturated LQR (Setpoint x=[{x_ref[0]},{x_ref[1]}]^T, u limits=[{u_min},{u_max}])')\n    plt.ylabel('Position $x_1$')\n    plt.ylim(ylims_x1); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 2)\n    plt.plot(t_history[:n_plot+1], x_history[1,:n_plot+1], label='$x_2$ (dy/dt)', lw=2)\n    plt.axhline(x_ref[1], color='k', linestyle=':', label=f'Setpoint $x_2={x_ref[1]}$')\n    plt.ylabel('Velocity $x_2$')\n    plt.ylim(ylims_x2); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 3)\n    plt.step(t_history[:n_plot], u_lqr_desired_hist[:n_plot], where='post', label='Desired LQR u[k]', lw=1.5, ls=':')\n    plt.step(t_history[:n_plot], u_sat_applied_hist[:n_plot], where='post', label='Applied Saturated u[k]', lw=2)\n    plt.axhline(u_ref, color='k', linestyle=':', label=f'Steady-State u={u_ref:.2f}')\n    plt.axhline(u_max, color='r', linestyle=':', label='Limit')\n    plt.axhline(u_min, color='r', linestyle=':')\n    plt.ylabel('Control Input u')\n    plt.xlabel('Time (s)')\n    plt.ylim(ylims_u); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Simulation failed, not plotting.\")\n\n\nDLQR Gain for error K = [[8.76224795 4.36193866]]\nControl Limits: [-2.5, 0.5]\nSimulation loop time: 0.03 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Scenario 3: MPC for Nonzero Regulation (Damped 2nd Order System)\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve_discrete_are, expm\nfrom scipy.signal import cont2discrete\nimport cvxpy as cp # Requires cvxpy and a solver like OSQP\nimport matplotlib.pyplot as plt\nimport time\n\n# print(\"--- Running Scenario 3: MPC (Nonzero Setpoint, Damped System) ---\")\n\n# --- 1. System Definition ---\nc_damping = 0.5\nAc = np.array([[0., 1.], [1., -c_damping]])\nBc = np.array([[0.], [1.]])\nn_states = Ac.shape[0]\nm_inputs = Bc.shape[1]\ndef system_ode(t, x, u, Ac_mat, Bc_mat):\n    u_val = u if np.isscalar(u) else u[0]\n    dxdt = Ac_mat @ x + Bc_mat @ np.array([u_val])\n    return dxdt.flatten()\n\n# --- 2. Discretization ---\nTs = 0.1\nAd, Bd, _, _, _ = cont2discrete((Ac, Bc, np.eye(n_states), np.zeros((n_states, m_inputs))), Ts, method='zoh')\n# print(\"--- Discrete Model ---\")\n# print(f\"Ad = \\n{Ad}\")\n# print(f\"Bd = \\n{Bd}\")\n\n# --- 3. Setpoint ---\nx_ref = np.array([[1.0], [0.0]]) # Target state y=1, y_dot=0 (use column vector shape)\ntry:\n    u_ref_vec = np.linalg.pinv(Bd) @ (np.eye(n_states) - Ad) @ x_ref\n    u_ref = u_ref_vec[0,0] # Extract scalar\n    # print(f\"Setpoint: x_ref = {x_ref.T}, requires discrete u_ref = {u_ref:.3f} (cont. u_ref=-1.0)\")\nexcept Exception as e:\n    print(f\"Could not calculate discrete u_ref: {e}. Using continuous u_ref.\")\n    u_ref = -x_ref[0,0] # Use continuous calculation\n\n# --- 4. MPC Setup ---\nP = 30 # Prediction Horizon\n# Use same base weights as LQR example, but consider tuning R_mpc\nQ_mpc = np.diag([1.0, 0.1])\n# R_mpc = np.array([[0.01]]) # Original aggressive LQR R\nR_mpc = np.array([[0.1]]) # Use less aggressive R for MPC? Try this.\n# Calculate terminal weight Qf from DARE solution S using MPC weights\ntry:\n    S = solve_discrete_are(Ad, Bd, Q_mpc, R_mpc)\n    Qf_mpc = S\n    # print(f\"Using terminal weight Qf=S based on MPC Q/R: \\n{Qf_mpc}\")\nexcept Exception as e:\n    print(f\"DARE solve failed ({e}), using Qf=Q\"); Qf_mpc = Q_mpc\n\n# Constraints\nu_min = -2.5\nu_max = 0.5\nprint(f\"Control Limits: [{u_min}, {u_max}]\")\n\n# --- 5. CVXPY Optimization Problem Setup ---\nU = cp.Variable((m_inputs, P), name='U')\nx_k_param = cp.Parameter(n_states, name='x_k_param') # Current state parameter\ncost = 0.0\nconstraints = []\nx_pred = x_k_param # Use parameter for current state\n\nfor t in range(P):\n    # Penalize deviation from reference state & absolute control input\n    cost += cp.quad_form(x_pred - x_ref.flatten(), Q_mpc) + cp.quad_form(U[:, t], R_mpc)\n    x_next = Ad @ x_pred + Bd @ U[:, t]\n    constraints += [U[:, t] &gt;= u_min, U[:, t] &lt;= u_max]\n    x_pred = x_next\n# Terminal cost penalizes deviation from x_ref\ncost += cp.quad_form(x_pred - x_ref.flatten(), Qf_mpc)\n\nobjective = cp.Minimize(cost)\nproblem = cp.Problem(objective, constraints)\nprint(\"MPC CVXPY Problem defined.\")\n\n# --- 6. Simulation Setup ---\nt_start = 0\nt_end = 6.0\nn_steps = int(t_end / Ts)\nx0 = np.array([0.0, 0.0]) # Initial condition\n\n# --- Helper function for interval simulation ---\ndef simulate_interval(ode_func, t_start, x_start, duration, control_val):\n    t_eval_interval = [t_start + duration]\n    sol_interval = solve_ivp(\n        ode_func, (t_start, t_start + duration), x_start, method='RK45',\n        t_eval=t_eval_interval, args=(control_val,)\n    )\n    if sol_interval.status != 0: return None, False\n    return sol_interval.y[:, -1], True\n\n# --- 7. Simulation Loop ---\nt_history = [t_start]\nx_history = [x0]\nu_history = [] # Applied MPC control\ncurrent_t = t_start\ncurrent_x = x0.copy()\nsuccess_run = True\ninfeasibility_count = 0\n\nstart_sim_time = time.time()\n# print(\"Running simulation...\")\nfor k in range(n_steps):\n    x_k_param.value = current_x # Update parameter\n    u_k = u_ref # Default if solver fails\n    try:\n        problem.solve(solver=cp.OSQP, warm_start=True, verbose=False)\n        if problem.status == cp.OPTIMAL or problem.status == cp.OPTIMAL_INACCURATE:\n            u_k = U.value[0, 0]\n        else:\n            print(f\"MPC Warning: Solver status {problem.status} at k={k}\")\n            infeasibility_count += 1\n            if U.value is not None: u_k = U.value[0, 0]\n            if problem.status == cp.INFEASIBLE:\n                 print(\"MPC Problem is infeasible. Stopping.\"); success_run = False; break\n    except Exception as e:\n        print(f\"MPC Solver failed at k={k}: {e}\"); infeasibility_count += 1\n\n    # Apply saturation as safety/clip numerical tolerances\n    u_k_sat = np.clip(u_k, u_min, u_max)\n    u_history.append(u_k_sat)\n\n    def ode_mpc_interval(t, y, u_val):\n        return system_ode(t, y, u_val, Ac, Bc)\n\n    next_x, success = simulate_interval(ode_mpc_interval, current_t, current_x, Ts, u_k_sat)\n\n    if not success:\n        print(f\"ODE solver failed at t={current_t:.2f}\")\n        success_run = False; break\n\n    current_t += Ts\n    current_x = next_x\n    t_history.append(current_t)\n    x_history.append(current_x.copy())\n\n    if np.max(np.abs(current_x)) &gt; 100: # Divergence check\n        print(f\"State diverging excessively at t={current_t:.2f}. Stopping.\")\n        success_run = False; break\n\nprint(f\"MPC Simulation loop time: {time.time() - start_sim_time:.2f} s\")\nif infeasibility_count &gt; 0: print(f\"Warning: MPC Solver non-optimal status {infeasibility_count} times.\")\n\n# --- 8. Plotting ---\nif success_run:\n    t_history = np.array(t_history)\n    x_history = np.array(x_history).T\n    u_history = np.array(u_history)\n    n_plot = len(u_history)\n\n    # Use consistent plot limits from Block 1 & 2 (or adjust)\n    xlims = (t_start, t_end)\n    ylims_x1 = (-0.5, 1.5)\n    ylims_x2 = (-0.5, 1.5)\n    ylims_u = (-4, 1) # Use same limits\n\n    plt.figure(\"MPC (Nonzero Setpoint, Damped)\", figsize=(10, 9))\n    plt.rcParams.update({'font.size': 12})\n\n    plt.subplot(3, 1, 1)\n    plt.plot(t_history[:n_plot+1], x_history[0,:n_plot+1], label='$x_1$ (y)', lw=2)\n    plt.axhline(x_ref[0], color='k', linestyle=':', label=f'Setpoint $x_1={x_ref[0,0]}$')\n    plt.title(f'MPC Response (Setpoint x=[{x_ref[0,0]},{x_ref[1,0]}]^T, u limits=[{u_min},{u_max}], P={P})')\n    plt.ylabel('Position $x_1$')\n    plt.ylim(ylims_x1); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 2)\n    plt.plot(t_history[:n_plot+1], x_history[1,:n_plot+1], label='$x_2$ (dy/dt)', lw=2)\n    plt.axhline(x_ref[1], color='k', linestyle=':', label=f'Setpoint $x_2={x_ref[1,0]}$')\n    plt.ylabel('Velocity $x_2$')\n    plt.ylim(ylims_x2); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.subplot(3, 1, 3)\n    plt.step(t_history[:n_plot], u_history[:n_plot], where='post', label='Applied MPC u[k]', lw=2)\n    plt.axhline(u_ref, color='k', linestyle=':', label=f'Steady-State u={u_ref:.2f}')\n    plt.axhline(u_max, color='r', linestyle=':', label='Limit')\n    plt.axhline(u_min, color='r', linestyle=':')\n    plt.ylabel('Control Input u')\n    plt.xlabel('Time (s)')\n    plt.ylim(ylims_u); plt.xlim(xlims); plt.grid(True); plt.legend()\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Simulation failed, not plotting.\")\n\n\nControl Limits: [-2.5, 0.5]\nMPC CVXPY Problem defined.\n\n\nMPC Simulation loop time: 18.55 s"
  },
  {
    "objectID": "lectures/lecture9.html#how-mpc-works",
    "href": "lectures/lecture9.html#how-mpc-works",
    "title": "EGH445 Modern Control",
    "section": "How MPC Works",
    "text": "How MPC Works\n\nMPC solves an optimization problem at each time step to find the optimal control sequence over a finite horizon.\nThe first control input of the optimal sequence is applied to the system.\nThe process is repeated at the next time step, using the current state as the new initial condition.\n\n\nMPC vs LQR:\n\nMPC handles constraints (e.g. input saturation, state constraints, rate of change limits, etc).\nMPC is more complex and computationally intensive than LQR.\nMPC’s performance depends on the accuracy of the system model."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "2025/week9.html#diversity-in-stem",
    "href": "2025/week9.html#diversity-in-stem",
    "title": "EGH445 Modern Control",
    "section": "Diversity in STEM",
    "text": "Diversity in STEM"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EGH445 Modern Control",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]